{
  
    
        "post0": {
            "title": "COVID-19 Analysis & Visualization",
            "content": "&#49436;&#47200; . &#48516;&#49437; &#48176;&#44221; &#48143; &#47785;&#51201; . &#48516;&#49437; &#48176;&#44221; . 분석 배경 - 2022년 6월 현재 코로나의 상황은 매일 10000명의 확진자가 나오고 있는 상황이지만 코로나가 처음 발병하고 나서와는 조금은 다른 반응이다. 최근 정부에서는 집단 면역이 90% 이상 형성이 되어있으며 확진자의 추세 또한 감소세를 보이고 있는 상황에서 2020년 01월부터 2020년 06월까지 수집된 해당 데이터를 기반으로 과연 과거와 현재의 차이는 얼마나 있고 당시 정부와 뉴스에서 주장하던 코로나에 대한 정보는 과연 타당하였고 올바른 정보였는지 궁금하여 해당 주제를 선정하여 분석을 진행하게 되었습니다. . &#48516;&#49437; &#47785;&#51201; . 분석 목적 - 자료를 제공한 데이콘에서는 해당 자료들은 이용해서 다음과 같은 인공지능 AI를 활용 코로나 확산 방지와 예방을 위한 인사이트 / 시각화 발굴. 이라는 목적을 가지고 진행을 하였습니다. 해서 저는 당시 기간동안 가장 많이 확진된 연령층과 주된 감염 원인과 그 이유에 대해서 알아보고, 어떤 연령층에게 가장 치명적인 질병이었느지와 당시 정부의 방역 대책은 타당하였는지 에 대해서 목적을 가지고 해당 분석을 진행하였습니다. . &#45936;&#51060;&#53552; &#49548;&#44060; . &#45936;&#51060;&#53552; &#52852;&#53580;&#44256;&#47532; . 1) Case Data . Case: 한국의 COVID-19 감염 사례 데이터 | . 2) Patient Data . PatientInfo: 한국의 코로나19 환자 역학 데이터 | PatientRoute: 국내 코로나19 환자 경로 데이터 | . 3) Time Series Data . Time: 한국의 코로나19 상태의 시계열 데이터 | TimeAge: 한국의 연령별 코로나19 현황 시계열 데이터 | TimeGender: 한국의 성별에 따른 코로나19 현황의 시계열 데이터 | TimeProvince: 한국의 지역별 코로나19 현황 시계열 데이터 | . 4) Additional Data . Region: 대한민국 내 지역의 위치 및 통계 자료 | Weather: 한국 지역의 날씨 데이터 | SearchTrend: 국내 최대 포털사이트 네이버에서 검색된 키워드의 트렌드 데이터 | SeoulFloating: 대한민국 서울 유동인구 데이터(SK텔레콤 빅데이터 허브에서) | Policy: 한국의 코로나19에 대한 정부 정책 데이터 | . &#45936;&#51060;&#53552; &#54805;&#53468; . 색상이 의미하는 것은 비슷한 속성을 가지고 있다는 것입니다. | 열 사이에 선이 연결되어 있다는 것은 열의 값이 부분적으로 공유됨을 의미합니다. | 점선은 약한 관련성을 의미합니다. | . https://user-images.githubusercontent.com/50820635/86225695-8dca0580-bbc5-11ea-9e9b-b0ca33414d8a.PNG . &#45936;&#51060;&#53552; &#49464;&#48512; &#49444;&#47749; . import numpy as np import pandas as pd import os for dirname, _, filenames in os.walk(&#39;../notebook/coronavirusdataset&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . ../notebook/coronavirusdataset Case.csv ../notebook/coronavirusdataset PatientInfo.csv ../notebook/coronavirusdataset PatientRoute.csv ../notebook/coronavirusdataset Policy.csv ../notebook/coronavirusdataset Region.csv ../notebook/coronavirusdataset SearchTrend.csv ../notebook/coronavirusdataset SeoulFloating.csv ../notebook/coronavirusdataset Time.csv ../notebook/coronavirusdataset TimeAge.csv ../notebook/coronavirusdataset TimeGender.csv ../notebook/coronavirusdataset TimeProvince.csv ../notebook/coronavirusdataset Weather.csv . path = &#39;../notebook/coronavirusdataset/&#39; case = p_info = pd.read_csv(path+&#39;Case.csv&#39;) patientinfo = pd.read_csv(path+&#39;PatientInfo.csv&#39;) patientroute = pd.read_csv(path+&#39;PatientRoute.csv&#39;) time = pd.read_csv(path+&#39;Time.csv&#39;) timeage = pd.read_csv(path+&#39;TimeAge.csv&#39;) timegender = pd.read_csv(path+&#39;TimeGender.csv&#39;) timeprovince = pd.read_csv(path+&#39;TimeProvince.csv&#39;) region = pd.read_csv(path+&#39;Region.csv&#39;) weather = pd.read_csv(path+&#39;Weather.csv&#39;) searchtrend = pd.read_csv(path+&#39;SearchTrend.csv&#39;) seoulfloating = pd.read_csv(path+&#39;SeoulFloating.csv&#39;) policy = pd.read_csv(path+&#39;Policy.csv&#39;) . 1) Case . 한국의 COVID-19 감염 사례 데이터 . case_id: the ID of the infection case case_id(7) = region_code(5) + case_number(2) | You can check the region_code in &#39;Region.csv&#39; | . | province: Special City / Metropolitan City / Province(-do) | city: City(-si) / Country (-gun) / District (-gu) The value &#39;from other city&#39; means that where the group infection started is other city. | . | group: TRUE: group infection / FALSE: not group If the value is &#39;TRUE&#39; in this column, the value of &#39;infection_cases&#39; means the name of group. | The values named &#39;contact with patient&#39;, &#39;overseas inflow&#39; and &#39;etc&#39; are not group infection. | . | infection_case: the infection case (the name of group or other cases) The value &#39;overseas inflow&#39; means that the infection is from other country. | The value &#39;etc&#39; includes individual cases, cases where relevance classification is ongoing after investigation, and cases under investigation. | . | confirmed: the accumulated number of the confirmed | latitude: the latitude of the group (WGS84) | longitude: the longitude of the group (WGS84) | . case.head() . case_id province city group infection_case confirmed latitude longitude . 0 1000001 | Seoul | Yongsan-gu | True | Itaewon Clubs | 139 | 37.538621 | 126.992652 | . 1 1000002 | Seoul | Gwanak-gu | True | Richway | 119 | 37.48208 | 126.901384 | . 2 1000003 | Seoul | Guro-gu | True | Guro-gu Call Center | 95 | 37.508163 | 126.884387 | . 3 1000004 | Seoul | Yangcheon-gu | True | Yangcheon Table Tennis Club | 43 | 37.546061 | 126.874209 | . 4 1000005 | Seoul | Dobong-gu | True | Day Care Center | 43 | 37.679422 | 127.044374 | . 2) PatientInfo . 한국의 코로나19 환자 역학 데이터 . patient_id: the ID of the patient patient_id(10) = region_code(5) + patient_number(5) | You can check the region_code in &#39;Region.csv&#39; | There are two types of the patient_number 1) local_num: The number given by the local government. 2) global_num: The number given by the KCDC | . | sex: the sex of the patient | age: the age of the patient 0s: 0 ~ 9 | 10s: 10 ~ 19 ... | 90s: 90 ~ 99 | 100s: 100 ~ 109 | . | country: the country of the patient | province: the province of the patient | city: the city of the patient | infection_case: the case of infection | infected_by: the ID of who infected the patient This column refers to the &#39;patient_id&#39; column. | . | contact_number: the number of contacts with people | symptom_onset_date: the date of symptom onset | confirmed_date: the date of being confirmed | released_date: the date of being released | deceased_date: the date of being deceased | state: isolated / released / deceased isolated: being isolated in the hospital | released: being released from the hospital | deceased: being deceased | . | . patientinfo.head() . patient_id sex age country province city infection_case infected_by contact_number symptom_onset_date confirmed_date released_date deceased_date state . 0 1000000001 | male | 50s | Korea | Seoul | Gangseo-gu | overseas inflow | NaN | 75 | 2020-01-22 | 2020-01-23 | 2020-02-05 | NaN | released | . 1 1000000002 | male | 30s | Korea | Seoul | Jungnang-gu | overseas inflow | NaN | 31 | NaN | 2020-01-30 | 2020-03-02 | NaN | released | . 2 1000000003 | male | 50s | Korea | Seoul | Jongno-gu | contact with patient | 2002000001 | 17 | NaN | 2020-01-30 | 2020-02-19 | NaN | released | . 3 1000000004 | male | 20s | Korea | Seoul | Mapo-gu | overseas inflow | NaN | 9 | 2020-01-26 | 2020-01-30 | 2020-02-15 | NaN | released | . 4 1000000005 | female | 20s | Korea | Seoul | Seongbuk-gu | contact with patient | 1000000002 | 2 | NaN | 2020-01-31 | 2020-02-24 | NaN | released | . 3) PatientRoute . 한국의 코로나19 환자 경로 데이터 . patient_id: the ID of the patient | date: YYYY-MM-DD | province: Special City / Metropolitan City / Province(-do) | city: City(-si) / Country (-gun) / District (-gu) | latitude: the latitude of the visit (WGS84) | longitude: the longitude of the visit (WGS84) | . patientroute.head() . patient_id global_num date province city type latitude longitude . 0 1000000001 | 2.0 | 2020-01-22 | Gyeonggi-do | Gimpo-si | airport | 37.615246 | 126.715632 | . 1 1000000001 | 2.0 | 2020-01-24 | Seoul | Jung-gu | hospital | 37.567241 | 127.005659 | . 2 1000000002 | 5.0 | 2020-01-25 | Seoul | Seongbuk-gu | etc | 37.592560 | 127.017048 | . 3 1000000002 | 5.0 | 2020-01-26 | Seoul | Seongbuk-gu | store | 37.591810 | 127.016822 | . 4 1000000002 | 5.0 | 2020-01-26 | Seoul | Seongdong-gu | public_transportation | 37.563992 | 127.029534 | . 4) Time . 한국의 COVID-19 상태의 시계열 데이터 . date: YYYY-MM-DD | time: Time (0 = AM 12:00 / 16 = PM 04:00) The time for KCDC to open the information has been changed from PM 04:00 to AM 12:00 since March 2nd. | . | test: the accumulated number of tests A test is a diagnosis of an infection. | . | negative: the accumulated number of negative results | confirmed: the accumulated number of positive results | released: the accumulated number of releases | deceased: the accumulated number of deceases | . time.head() . date time test negative confirmed released deceased . 0 2020-01-20 | 16 | 1 | 0 | 1 | 0 | 0 | . 1 2020-01-21 | 16 | 1 | 0 | 1 | 0 | 0 | . 2 2020-01-22 | 16 | 4 | 3 | 1 | 0 | 0 | . 3 2020-01-23 | 16 | 22 | 21 | 1 | 0 | 0 | . 4 2020-01-24 | 16 | 27 | 25 | 2 | 0 | 0 | . 5) TimeAge . 한국의 연령별 코로나19 현황 시계열 데이터 . date: YYYY-MM-DD The status in terms of the age has been presented since March 2nd. | . | time: Time | age: the age of patients | confirmed: the accumulated number of the confirmed | deceased: the accumulated number of the deceased | . timeage.head() . date time age confirmed deceased . 0 2020-03-02 | 0 | 0s | 32 | 0 | . 1 2020-03-02 | 0 | 10s | 169 | 0 | . 2 2020-03-02 | 0 | 20s | 1235 | 0 | . 3 2020-03-02 | 0 | 30s | 506 | 1 | . 4 2020-03-02 | 0 | 40s | 633 | 1 | . 6) TimeGender . 한국의 성별에 따른 COVID-19 현황의 시계열 데이터 . date: YYYY-MM-DD The status in terms of the gender has been presented since March 2nd. | . | time: Time | sex: the gender of patients | confirmed: the accumulated number of the confirmed | deceased: the accumulated number of the deceased | . timegender.head() . date time sex confirmed deceased . 0 2020-03-02 | 0 | male | 1591 | 13 | . 1 2020-03-02 | 0 | female | 2621 | 9 | . 2 2020-03-03 | 0 | male | 1810 | 16 | . 3 2020-03-03 | 0 | female | 3002 | 12 | . 4 2020-03-04 | 0 | male | 1996 | 20 | . 7) TimeProvince . 한국의 지역별 코로나19 현황 시계열 데이터 . date: YYYY-MM-DD | time: Time | province: the province of South Korea | confirmed: the accumulated number of the confirmed in the province The confirmed status in terms of the provinces has been presented since Feburary 21th. | The value before Feburary 21th can be different. | . | released: the accumulated number of the released in the province The confirmed status in terms of the provinces has been presented since March 5th. -The value before March 5th can be different. | . | deceased: the accumulated number of the deceased in the province The confirmed status in terms of the provinces has been presented since March 5th. | The value before March 5th can be different. | . | . timeprovince.head() . date time province confirmed released deceased . 0 2020-01-20 | 16 | Seoul | 0 | 0 | 0 | . 1 2020-01-20 | 16 | Busan | 0 | 0 | 0 | . 2 2020-01-20 | 16 | Daegu | 0 | 0 | 0 | . 3 2020-01-20 | 16 | Incheon | 1 | 0 | 0 | . 4 2020-01-20 | 16 | Gwangju | 0 | 0 | 0 | . 8) Region . 대한민국 내 지역의 위치 및 통계 자료 . code: the code of the region | province: Special City / Metropolitan City / Province(-do) | city: City(-si) / Country (-gun) / District (-gu) | latitude: the latitude of the visit (WGS84) | longitude: the longitude of the visit (WGS84) | elementary_school_count: the number of elementary schools | kindergarten_count: the number of kindergartens | university_count: the number of universities | academy_ratio: the ratio of academies | elderly_population_ratio: the ratio of the elderly population | elderly_alone_ratio: the ratio of elderly households living alone | nursing_home_count: the number of nursing homes | . Source of the statistic: KOSTAT (Statistics Korea) . region.head() . code province city latitude longitude elementary_school_count kindergarten_count university_count academy_ratio elderly_population_ratio elderly_alone_ratio nursing_home_count . 0 10000 | Seoul | Seoul | 37.566953 | 126.977977 | 607 | 830 | 48 | 1.44 | 15.38 | 5.8 | 22739 | . 1 10010 | Seoul | Gangnam-gu | 37.518421 | 127.047222 | 33 | 38 | 0 | 4.18 | 13.17 | 4.3 | 3088 | . 2 10020 | Seoul | Gangdong-gu | 37.530492 | 127.123837 | 27 | 32 | 0 | 1.54 | 14.55 | 5.4 | 1023 | . 3 10030 | Seoul | Gangbuk-gu | 37.639938 | 127.025508 | 14 | 21 | 0 | 0.67 | 19.49 | 8.5 | 628 | . 4 10040 | Seoul | Gangseo-gu | 37.551166 | 126.849506 | 36 | 56 | 1 | 1.17 | 14.39 | 5.7 | 1080 | . 9) Weather . 한국 지역의 날씨 데이터 . code: the code of the region | province: Special City / Metropolitan City / Province(-do) | date: YYYY-MM-DD | avg_temp: the average temperature | min_temp: the lowest temperature | max_temp: the highest temperature | precipitation: the daily precipitation | max_wind_speed: the maximum wind speed | most_wind_direction: the most frequent wind direction | avg_relative_humidity: the average relative humidity | . Source of the weather data: KMA (Korea Meteorological Administration) . weather.head() . code province date avg_temp min_temp max_temp precipitation max_wind_speed most_wind_direction avg_relative_humidity . 0 10000 | Seoul | 2016-01-01 | 1.2 | -3.3 | 4.0 | 0.0 | 3.5 | 90.0 | 73.0 | . 1 11000 | Busan | 2016-01-01 | 5.3 | 1.1 | 10.9 | 0.0 | 7.4 | 340.0 | 52.1 | . 2 12000 | Daegu | 2016-01-01 | 1.7 | -4.0 | 8.0 | 0.0 | 3.7 | 270.0 | 70.5 | . 3 13000 | Gwangju | 2016-01-01 | 3.2 | -1.5 | 8.1 | 0.0 | 2.7 | 230.0 | 73.1 | . 4 14000 | Incheon | 2016-01-01 | 3.1 | -0.4 | 5.7 | 0.0 | 5.3 | 180.0 | 83.9 | . 10) SearchTrend . 국내 최대 포털인 네이버에서 검색된 키워드의 트렌드 데이터 . date: YYYY-MM-DD | cold: the search volume of &#39;cold&#39; in Korean language The unit means relative value by setting the highest search volume in the period to 100. | . | flu: the search volume of &#39;flu&#39; in Korean language Same as above. | . | pneumonia: the search volume of &#39;pneumonia&#39; in Korean language -Same as above. | coronavirus: the search volume of &#39;coronavirus&#39; in Korean language -Same as above. | . Source of the data: NAVER DataLab . searchtrend.head() . date cold flu pneumonia coronavirus . 0 2016-01-01 | 0.11663 | 0.05590 | 0.15726 | 0.00736 | . 1 2016-01-02 | 0.13372 | 0.17135 | 0.20826 | 0.00890 | . 2 2016-01-03 | 0.14917 | 0.22317 | 0.19326 | 0.00845 | . 3 2016-01-04 | 0.17463 | 0.18626 | 0.29008 | 0.01145 | . 4 2016-01-05 | 0.17226 | 0.15072 | 0.24562 | 0.01381 | . 11) SeoulFloating . 대한민국 서울 유동인구 데이터(SK텔레콤 빅데이터 허브에서) . date: YYYY-MM-DD | hour: Hour | birth_year: the birth year of the floating population | sext: he sex of the floating population | province: Special City / Metropolitan City / Province(-do) | city: City(-si) / Country (-gun) / District (-gu) | fp_num: the number of floating population | . Source of the data: SKT Big Data Hub . seoulfloating.head() . date hour birth_year sex province city fp_num . 0 2020-01-01 | 0 | 20 | female | Seoul | Dobong-gu | 19140 | . 1 2020-01-01 | 0 | 20 | male | Seoul | Dobong-gu | 19950 | . 2 2020-01-01 | 0 | 20 | female | Seoul | Dongdaemun-gu | 25450 | . 3 2020-01-01 | 0 | 20 | male | Seoul | Dongdaemun-gu | 27050 | . 4 2020-01-01 | 0 | 20 | female | Seoul | Dongjag-gu | 28880 | . 12) Policy . 한국의 COVID-19에 대한 정부 정책 데이터 . policy_id: the ID of the policy | country: the country that implemented the policy | type: the type of the policy | gov_policy: the policy of the government | detail: the detail of the policy | start_date: the start date of the policy | end_date: the end date of the policy | . policy.head() . policy_id country type gov_policy detail start_date end_date . 0 1 | Korea | Alert | Infectious Disease Alert Level | Level 1 (Blue) | 2020-01-03 | 2020-01-19 | . 1 2 | Korea | Alert | Infectious Disease Alert Level | Level 2 (Yellow) | 2020-01-20 | 2020-01-27 | . 2 3 | Korea | Alert | Infectious Disease Alert Level | Level 3 (Orange) | 2020-01-28 | 2020-02-22 | . 3 4 | Korea | Alert | Infectious Disease Alert Level | Level 4 (Red) | 2020-02-23 | NaN | . 4 5 | Korea | Immigration | Special Immigration Procedure | from China | 2020-02-04 | NaN | . &#48376;&#47200; . Section &#51452;&#51228;1 - &#50612;&#46500; &#50672;&#47161;&#52789;&#51060; &#44032;&#51109; &#47566;&#51060; &#54869;&#51652;&#46104;&#50632;&#45716;&#44032;? . &#51452;&#51228;1 - EDA . import numpy as np import pandas as pd import seaborn as sns import plotly.express as px import plotly.graph_objects as go from matplotlib import pyplot as plt import cufflinks as cf cf.go_offline() cf.set_config_file(offline=False, world_readable=True) from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot . time.head() #time 데이터의 상위 5개를 확인 . date time test negative confirmed released deceased . 0 2020-01-20 | 16 | 1 | 0 | 1 | 0 | 0 | . 1 2020-01-21 | 16 | 1 | 0 | 1 | 0 | 0 | . 2 2020-01-22 | 16 | 4 | 3 | 1 | 0 | 0 | . 3 2020-01-23 | 16 | 22 | 21 | 1 | 0 | 0 | . 4 2020-01-24 | 16 | 27 | 25 | 2 | 0 | 0 | . fig = go.Figure() #빈 도화지를 만든다는 개념 fig.add_trace(go.Scatter(x=time[&#39;date&#39;], y=time[&#39;confirmed&#39;], mode=&#39;lines&#39;, name=&#39;확진(confirmed)&#39;)) #Scatter 형태의 플랏으로 x축은 time데이터의 date컬럼을 사용하고 y축은 time데이터의 confirmed 컬럼을 사용하고 표현 방법은 line이며 선의 이름은 확진으로 지정 fig.update_layout(title=&#39;시간의 흐름에 따른 확진자 추이&#39;, xaxis_title=&#39;Date&#39;, yaxis_title=&#39;Number&#39;) #그래프의 제목과 x축 y축의 이름을 지정 fig.show() #그래프를 출력해서 보이도록 . 해당 그래프를 보면 20년 3월부터 가파른 경사를 보이면서 우상향해서 증가하는 추세를 보이고 있습니다 . fig = go.Figure() fig.add_trace(go.Scatter(x=time[&#39;date&#39;], y=time[&#39;confirmed&#39;], mode=&#39;lines&#39;, name=&#39;확진(confirmed)&#39;)) fig.add_trace(go.Scatter(x=time[&#39;date&#39;], y=time[&#39;released&#39;], mode=&#39;lines&#39;, name=&#39;해제(released)&#39;)) fig.add_trace(go.Scatter(x=time[&#39;date&#39;], y=time[&#39;deceased&#39;], mode=&#39;lines&#39;, name=&#39;사망(deceased)&#39;)) fig.update_layout(title=&#39;시간의 흐름에 따른 코로나의 추이&#39;, xaxis_title=&#39;Date&#39;, yaxis_title=&#39;Number&#39;) fig.show() . 다음 그래프는 시간의 흐름에 따른 코로나의 추이로 확진자와 격리해제자의 추이와 사망자의 추이에 대해서 보여주고 있으며 확진자와 격리해제자의 추이는 유사하게 우상향하는 모습을 보여주고 있으며 사망자는 확진자와 격리자의 수에 비해서는 적어서 눈에 보이는 변화는 없습니다. . fig = go.Figure() fig.add_trace(go.Scatter(x=time[&#39;date&#39;], y=time[&#39;confirmed&#39;], mode=&#39;lines&#39;, name=&#39;확진(confirmed)&#39;)) fig.add_trace(go.Scatter(x=time[&#39;date&#39;],y=time[&#39;negative&#39;], mode=&#39;lines&#39;, name=&#39;음성(Negative)&#39;)) fig.add_trace(go.Scatter(x=time[&#39;date&#39;],y=time[&#39;test&#39;], mode=&#39;markers&#39;, name=&#39;검사(Test)&#39;)) fig.update_layout(title=&#39;시간의 흐름에 따른 코로나의 검사 추이&#39;, xaxis_title=&#39;Date&#39;, yaxis_title=&#39;Number&#39;) fig.show() . 해당 그래프는 시간의 흐름에 따른 코로나의 검사 추이로 검사수와 음성의 수가 거의 붙어서 우상향하는 모습이고 확진자는 이에 비해 변동이 없어 보이는 모습입니다. 이를 통해 검사를 많이 했지만 이에 비해서 확진이 된 정도는 상당히 적음을 알 수 있습니다. . fig = go.Figure() fig.add_trace(go.Bar(x=time[&#39;date&#39;],y=time[&#39;confirmed&#39;].diff(), name=&#39;confirmed&#39;, marker_color=&#39;rgba(152, 0, 0, .8)&#39;)) fig.update_layout(title=&#39;일단위 확진자 수&#39;, xaxis_title=&#39;Date&#39;, yaxis_title=&#39;Number&#39;) fig.show() . 다음은 일단위 확진자의 수를 보여주고 있습니다. 20년 3월 인근에서 800여명 까지 일일 확진되는 모습을 보이고 점차 감소하는 모습을 보이는 형태입니다 . &#51452;&#51228;1 - &#50672;&#47161;&#45824;&#48324; &#54869;&#51652; &#48708;&#50984; . display(timeage.head()) #timeage 데이터셋의 기본적인 형태를 파악하기 위해 상위 5개의 행만 추출 display(timeage.age.unique()) #timeage 데이터셋에서 age 컬럼에서 어떤 연령층이 있는지 unique 함수를 통해서 추출 . date time age confirmed deceased . 0 2020-03-02 | 0 | 0s | 32 | 0 | . 1 2020-03-02 | 0 | 10s | 169 | 0 | . 2 2020-03-02 | 0 | 20s | 1235 | 0 | . 3 2020-03-02 | 0 | 30s | 506 | 1 | . 4 2020-03-02 | 0 | 40s | 633 | 1 | . array([&#39;0s&#39;, &#39;10s&#39;, &#39;20s&#39;, &#39;30s&#39;, &#39;40s&#39;, &#39;50s&#39;, &#39;60s&#39;, &#39;70s&#39;, &#39;80s&#39;], dtype=object) . age_list = timeage.age.unique() age_list #앞에서 설명한 연령대를 따로 추출하여 age_list라는 곳에 할당을 시킴 . array([&#39;0s&#39;, &#39;10s&#39;, &#39;20s&#39;, &#39;30s&#39;, &#39;40s&#39;, &#39;50s&#39;, &#39;60s&#39;, &#39;70s&#39;, &#39;80s&#39;], dtype=object) . fig, ax = plt.subplots(figsize = (13,7)) #도화지(Figure : fig)를 깔고 그래프를 그릴 구역(Axes : ax)을 정의합니다. figsize를 통해서 도화지의 크기를 지정해준다 #이는 objection oriented API으로 그래프의 각 부분을 객체로 지정하고 그리는 유형이다 sns.barplot(age_list,timeage.confirmed[-9:]) ax.set_xlabel(&#39;age&#39;,size=13) #연령 ax.set_ylabel(&#39;number of case&#39;,size=13) #케이스의 횟수 plt.title(&#39;Confirmed Cases by Age&#39;) plt.show() . c: ProgramData Anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . 연령대별로 분석을 해본결과 20대가 압도적으로 많은 수를 차지하고 있는 형태의 플랏을 볼 수가 있다 . 2020년 연령대별 인구 . https://kosis.kr/visual/populationKorea/experienceYard/populationPyramid.do?mb=N&amp;menuId=M_3_2 | . 해당 자료를 이용해서 인구수와 확진 비율을 확인하여 과연 20대가 인구수가 많아서 이렇게 많이 확진이 되었는가에 대해서 알아본다 . age_order = pd.DataFrame() #빈 데이터 프레임을 생성 age_order[&#39;age&#39;] = age_list #앞서 생성한 age_list를 새로 만드는 데이터 프레임에 age라는 이름의 컬럼으로 할당 age_order[&#39;population&#39;] = [4054901, 4769187, 7037893, 7174782, 8257903, 8575336, 6476602, 3598811, 1657942] #population이라는 컬럼에 통계청 홈페이지에서 확인한 값을 입력 age_order[&#39;proportion&#39;] = round(age_order[&#39;population&#39;]/sum(age_order[&#39;population&#39;])*100,2) #인구 비율을 구하기 위해 모든 인구수를 더하고 각 연령별로 나누고 소수점으로 나오는것을 방지하기 위해 100을 곱하고 소수점 2번째 자리까지 표현이 되도록 설정 age_order = age_order.sort_values(&#39;age&#39;) #age를 기준으로 재정렬 age_order.set_index(np.arange(1,10),inplace=True) #인덱스의 설정을 1~10순으로 들어가도록 설정하고 본래 있던것은 대체해서 사용하도록 age_order . age population proportion . 1 0s | 4054901 | 7.86 | . 2 10s | 4769187 | 9.24 | . 3 20s | 7037893 | 13.64 | . 4 30s | 7174782 | 13.90 | . 5 40s | 8257903 | 16.00 | . 6 50s | 8575336 | 16.62 | . 7 60s | 6476602 | 12.55 | . 8 70s | 3598811 | 6.97 | . 9 80s | 1657942 | 3.21 | . fig, ax = plt.subplots(figsize=(13, 7)) plt.title(&#39;Korea Age Proportion&#39;, fontsize=17) sns.barplot(age_list, age_order.proportion[-9:]) ax.set_xlabel(&#39;Age&#39;, size=13) ax.set_ylabel(&#39;Rate (%)&#39;, size=13) plt.show() #한국의 2020년 연령별 인구의 수를 나타낸 표이다 . c: ProgramData Anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . 예상과는 다르게 20대가 가장 많은 인구수를 가지고 있는 연령대가 아닌 40,50대가 가장 인구수가 많은 연령대임을 알 수 있다. 이로 20대 인구수가 다른 연령층에 비해 많기 때문에 확진이 많이 된것은 아니다. . confirmed_by_population = age_order.sort_values(&#39;age&#39;) #&#39;age&#39;라는 컬럼으로 정렬 confirmed_by_population[&#39;confirmed&#39;] = list(timeage[-9:].confirmed) #confirmed라는 컬럼을 만들고 timeage의 해당 리스트를 할당 시킴 confirmed_by_population[&#39;confirmed_ratio&#39;] = confirmed_by_population[&#39;confirmed&#39;]/confirmed_by_population[&#39;population&#39;] *100 #인구비율에 따른 확진 비율 컬럼 추가 display(confirmed_by_population) . age population proportion confirmed confirmed_ratio . 1 0s | 4054901 | 7.86 | 193 | 0.004760 | . 2 10s | 4769187 | 9.24 | 708 | 0.014845 | . 3 20s | 7037893 | 13.64 | 3362 | 0.047770 | . 4 30s | 7174782 | 13.90 | 1496 | 0.020851 | . 5 40s | 8257903 | 16.00 | 1681 | 0.020356 | . 6 50s | 8575336 | 16.62 | 2286 | 0.026658 | . 7 60s | 6476602 | 12.55 | 1668 | 0.025754 | . 8 70s | 3598811 | 6.97 | 850 | 0.023619 | . 9 80s | 1657942 | 3.21 | 556 | 0.033536 | . fig, ax = plt.subplots(figsize=(13, 7)) plt.title(&#39;Population-adjusted Confirmed Rate by Age&#39;, fontsize=17) sns.barplot(age_list, confirmed_by_population.confirmed_ratio[-9:]) ax.set_xlabel(&#39;Age&#39;, size=13) ax.set_ylabel(&#39;Confirmed rate (%)&#39;, size=13) plt.show() #인구 비율에 따른 확진 확률 . c: ProgramData Anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . 인구 비율에 따른 확진자의 수를 보아도 20대가 인구수가 많은 연령대인 40,50대 보다도 확연하게 많은 것을 알 수 있으며 오히려 80대 이상의 연령대가 차지하는 비율이 증가하였다. . fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(23, 7)) #1행 2열의 도화지를 생성 ## 1. Confirmed Cases by Age ax[0].set_title(&#39;Confirmed Cases by Age&#39;, fontsize=15) ax[0].bar(age_list, confirmed_by_population.confirmed) ## 2. Population-adjusted Confirmed Rate ax[1].set_title(&#39;Population-adjusted Confirmed Rate&#39;, fontsize=15) ax[1].bar(age_list, confirmed_by_population.confirmed_ratio) plt.show() . 다음 두개의 플랏을 보면 앞선 그래프에서는 20대의 확진 확률이 다른 연령대에 비해서 앞도적으로 높았지만 인구의 비율에 따른 확진의 비율을 나타내는 두번째 플랏을 보면 아직도 20대가 다른 연령대에 비해서 높기는 하지만 첫번째 그래프에 비해서는 조금은 낮아진 모습과 60~80대까지 연령층의 비중이 조금은 증가 했다는 사실을 두개의 플랏을 비교하면서 알 수있습니다 따라서 저는 다른 연령에 비해 압도적으로 많은 확진 비율을 가지고 있는 20대에 대해서 집중적으로 분석을 해보도록 하겠습니다. . &#51452;&#51228;1 - &#50672;&#47161;&#45824;&#48324; &#54869;&#51652; &#44221;&#47196; . patientinfo.head() . patient_id sex age country province city infection_case infected_by contact_number symptom_onset_date confirmed_date released_date deceased_date state . 0 1000000001 | male | 50s | Korea | Seoul | Gangseo-gu | overseas inflow | NaN | 75 | 2020-01-22 | 2020-01-23 | 2020-02-05 | NaN | released | . 1 1000000002 | male | 30s | Korea | Seoul | Jungnang-gu | overseas inflow | NaN | 31 | NaN | 2020-01-30 | 2020-03-02 | NaN | released | . 2 1000000003 | male | 50s | Korea | Seoul | Jongno-gu | contact with patient | 2002000001 | 17 | NaN | 2020-01-30 | 2020-02-19 | NaN | released | . 3 1000000004 | male | 20s | Korea | Seoul | Mapo-gu | overseas inflow | NaN | 9 | 2020-01-26 | 2020-01-30 | 2020-02-15 | NaN | released | . 4 1000000005 | female | 20s | Korea | Seoul | Seongbuk-gu | contact with patient | 1000000002 | 2 | NaN | 2020-01-31 | 2020-02-24 | NaN | released | . patientinfo[&#39;infection_case&#39;] = patientinfo[&#39;infection_case&#39;].astype(str).apply(lambda x:x.split()[0]) #PatientInfo[&#39;infection_case&#39;] infectionCase = patientinfo.pivot_table(index=&#39;infection_case&#39;,columns=&#39;age&#39;, values=&#39;patient_id&#39;,aggfunc=&quot;count&quot;) #infectionCase #전체 감염 케이스 patientTotal = infectionCase.fillna(0).sum(axis=1) patientTotal = patientTotal.sort_values(ascending = False)[:5] # 20대 감염 케이스 patient20s = infectionCase[&#39;20s&#39;].dropna() patient20sTop = patient20s.sort_values(ascending=False)[:5] . display(patientTotal) display(patient20sTop) . infection_case contact 1112.0 nan 827.0 overseas 653.0 etc 638.0 Guro-gu 112.0 dtype: float64 . infection_case overseas 269.0 nan 221.0 contact 172.0 etc 127.0 Shincheonji 41.0 Name: 20s, dtype: float64 . fig = go.Figure() fig.add_trace(go.Pie(labels=patientTotal.index, values=patientTotal.values, textinfo=&#39;label+percent&#39;)) fig.update_layout(title=&#39;Confirmed infection case Total AGe&#39;) fig.show() . 전체 연령측의 감염원인에 대해서 본다면 접촉에 의한 확진이 33% 해외 입국이 19% 그외 nan과 etc가 각각 24,19%의 비율을 차지하고 있다 . fig = go.Figure() fig.add_trace(go.Pie(labels=patient20sTop.index, values=patient20sTop.values, textinfo=&#39;label+percent&#39;)) fig.update_layout(title=&#39;Confirmed infection case 20s AGe&#39;) fig.show() . 20대 연령의 그룹은 전체연령에 비해 해외입국과 nan이 각각 32 26%를 차지 하고 있다. 그러나 해당 데이터에는 원인을 알수 없는 nan데이가 전체의 1/4가량을 차지 하기 때문에 정확한 분석을 하기 어렵다 . 그렇다면 확진된 20대가 많이 돌아다닌 장소에 대해서 patientinfo 데이터를 이용해서 찾아보겠습니다 . patientroute = pd.read_csv(path+&#39;PatientRoute.csv&#39;) . patientroute.head() . patient_id global_num date province city type latitude longitude . 0 1000000001 | 2.0 | 2020-01-22 | Gyeonggi-do | Gimpo-si | airport | 37.615246 | 126.715632 | . 1 1000000001 | 2.0 | 2020-01-24 | Seoul | Jung-gu | hospital | 37.567241 | 127.005659 | . 2 1000000002 | 5.0 | 2020-01-25 | Seoul | Seongbuk-gu | etc | 37.592560 | 127.017048 | . 3 1000000002 | 5.0 | 2020-01-26 | Seoul | Seongbuk-gu | store | 37.591810 | 127.016822 | . 4 1000000002 | 5.0 | 2020-01-26 | Seoul | Seongdong-gu | public_transportation | 37.563992 | 127.029534 | . patientroute[[&#39;patient_id&#39;,&#39;date&#39;,&#39;type&#39;]] #필요한 컬럼만 선택 . patient_id date type . 0 1000000001 | 2020-01-22 | airport | . 1 1000000001 | 2020-01-24 | hospital | . 2 1000000002 | 2020-01-25 | etc | . 3 1000000002 | 2020-01-26 | store | . 4 1000000002 | 2020-01-26 | public_transportation | . ... ... | ... | ... | . 6709 6100000090 | 2020-03-24 | airport | . 6710 6100000090 | 2020-03-24 | airport | . 6711 6100000090 | 2020-03-25 | store | . 6712 6100000090 | 2020-03-25 | hospital | . 6713 6100000090 | 2020-03-25 | store | . 6714 rows × 3 columns . places = patientroute.type.unique() simproute = patientroute[[&#39;patient_id&#39;,&#39;date&#39;,&#39;type&#39;]] agedf = patientinfo[[&#39;patient_id&#39;,&#39;age&#39;]] simproutewage = pd.merge(simproute,agedf,how=&#39;left&#39;) fiplot = simproutewage.set_index(&#39;type&#39;) fiplot_count = fiplot.groupby(&#39;type&#39;).count().patient_id.sort_values() . fig = fiplot_count.iplot(asFigure = True, kind=&#39;bar&#39;) fig.show() . 해당 그래프는 전체 연령의 확진 원인에 대한 것을 카운트 시킨 결과로 etc와 hospital이 가장 많은 것을 보이나 병원은 확진자가 이상증세를 느끼고 찾아가는 당연한 경로이므로 제외를 하고 etc 또한 어느 곳에 다녀왔는지 정확하게 알 수 없어서 제외를 하고 다시 진행을 해보겠습니다. . twtfi = fiplot[fiplot.age == &#39;20s&#39;] untwtfi = fiplot[fiplot.age != &#39;20s&#39;] twt = twtfi.groupby(&#39;type&#39;).count().patient_id untwt = untwtfi.groupby(&#39;type&#39;).count().patient_id twt = twt[~twt.index.isin( [&#39;etc&#39;,&#39;hospital&#39;])] untwt = untwt[~untwt.index.isin( [&#39;etc&#39;,&#39;hospital&#39;])] fig = go.Figure() fig.add_trace(go.Bar(x = twt.index, y = twt, name = &#39;20s&#39;, marker_color=&#39;indianred&#39;)) fig.add_trace(go.Bar(x = untwt.index, y = untwt, name = &#39;except 20s&#39;, marker_color=&#39;lightsalmon&#39;)) fig.update_layout(barmode=&#39;group&#39;, xaxis_tickangle=-45) fig.show() . 다음 그래프는 20대와 그외의 연령층이 확진된 원인에 대한것으로 앞서 말한것 처럼 etc와 hospital은 가장 많은 비율을 차지 하지만 분석을 하는데 크게 도움이 되지 않는다고 판단을 하여 제외를 하고 진행을 한 결과 이다. . 공통적으로 많이 방문하는 store과 church는 비슷한 양상을 보여주고 있습니다. 하지만, 20대는 restaurant, pc방, cafe, bar 등에서 훨씬 많은 방문비율을 확인할 수 있었습니다. . &#51452;&#51228;2 - &#53076;&#47196;&#45208;&#45716; &#45572;&#44396;&#50640;&#44172; &#44032;&#51109; &#52824;&#47749;&#51201;&#51064;&#44032; . &#51452;&#51228;2 - &#50672;&#47161;&#48324;&#47196; &#54869;&#51652;&#51088;&#51032; &#52824;&#47749;&#47456; . time.head() . date time test negative confirmed released deceased . 0 2020-01-20 | 16 | 1 | 0 | 1 | 0 | 0 | . 1 2020-01-21 | 16 | 1 | 0 | 1 | 0 | 0 | . 2 2020-01-22 | 16 | 4 | 3 | 1 | 0 | 0 | . 3 2020-01-23 | 16 | 22 | 21 | 1 | 0 | 0 | . 4 2020-01-24 | 16 | 27 | 25 | 2 | 0 | 0 | . fig = go.Figure() fig.add_trace(go.Scatter(x=time[&#39;date&#39;], y=time[&#39;deceased&#39;], mode=&#39;lines&#39;, # Line plot만 그리기 name=&#39;사망(deceased)&#39;)) fig.update_layout(title=&#39;시간의 흐름에 따른 확진자 사망 추이&#39;, xaxis_title=&#39;Date&#39;, yaxis_title=&#39;Number&#39;) fig.show() . 해당 그래프는 시간의 흐름에 따른 확진자의 사망 추이로서 2020년 3월 부터 계속해서 우상향하는 모습을 볼 수 있다 . fig, ax = plt.subplots(figsize = (13,7)) #도화지(Figure : fig)를 깔고 그래프를 그릴 구역(Axes : ax)을 정의합니다. figsize를 통해서 도화지의 크기를 지정해준다 #이는 objection oriented API으로 그래프의 각 부분을 객체로 지정하고 그리는 유형이다 sns.barplot(age_list,timeage.deceased[-9:]) ax.set_xlabel(&#39;age&#39;,size=13) #연령 ax.set_ylabel(&#39;number of case&#39;,size=13) #케이스의 횟수 plt.title(&#39;Deceased Cases by Age&#39;) plt.show() . c: ProgramData Anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . 나이가 많아질수록 사망자의 비율이 높다 과연 나이가 많아질수록 인구수가 많아져서 이러한 현상이 나오는 지 인구 비율에 따른 사망자에 대해서 다시 한번 살펴보자 . confirmed_by_population = age_order.sort_values(&#39;age&#39;) confirmed_by_population[&#39;deceased&#39;] = list(timeage[-9:].deceased) # 2. Get confirmed ratio regarding population confirmed_by_population[&#39;deceased_ratio&#39;] = confirmed_by_population[&#39;deceased&#39;]/confirmed_by_population[&#39;population&#39;] *100 display(confirmed_by_population) . age population proportion deceased deceased_ratio . 1 0s | 4054901 | 7.86 | 0 | 0.000000 | . 2 10s | 4769187 | 9.24 | 0 | 0.000000 | . 3 20s | 7037893 | 13.64 | 0 | 0.000000 | . 4 30s | 7174782 | 13.90 | 2 | 0.000028 | . 5 40s | 8257903 | 16.00 | 3 | 0.000036 | . 6 50s | 8575336 | 16.62 | 15 | 0.000175 | . 7 60s | 6476602 | 12.55 | 41 | 0.000633 | . 8 70s | 3598811 | 6.97 | 82 | 0.002279 | . 9 80s | 1657942 | 3.21 | 139 | 0.008384 | . fig, ax = plt.subplots(figsize=(13, 7)) plt.title(&#39;Population-adjusted Deceased Rate by Age&#39;, fontsize=17) sns.barplot(age_list, confirmed_by_population.deceased_ratio[-9:]) ax.set_xlabel(&#39;Age&#39;, size=13) ax.set_ylabel(&#39;Deceased rate (%)&#39;, size=13) plt.show() #인구 비율에 따른 확진 확률 . c: ProgramData Anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . 인구 비율에 따른 사망자의 비율을 살펴본 결과이다 0~20대 까지는 사망자는 없으며 30대부터 확진으로 인한 사망자가 존재한다. 그러나 80대 이상의 연령층의 경우는 가장 많은 인구수를 가진 연령층도 아니지만 사망자의 비중이 가장 높은 것을 볼 수 있다. 이로 코로나 바이러스는 고연령층 일수록 가장 치명적인 질병임을 예측할 수 있다. . 그렇다면 고연령층의 확진 원인에 대해서 알아보자 . &#51452;&#51228;2 - &#50672;&#47161;&#45824;&#51032; &#54869;&#51652; &#50896;&#51064; . aged_pat = patientinfo[(patientinfo[&#39;age&#39;] == &#39;60s&#39;)|(patientinfo[&#39;age&#39;] == &#39;70s&#39;)| (patientinfo[&#39;age&#39;] == &#39;80s&#39;)][[&#39;province&#39;,&#39;age&#39;,&#39;infection_case&#39;]] aged_inf = pd.DataFrame(aged_pat[&#39;infection_case&#39;].value_counts()) #고연령측의 확진 원인 . patientinfo[&#39;infection_case&#39;] = patientinfo[&#39;infection_case&#39;].astype(str).apply(lambda x:x.split()[0]) #PatientInfo[&#39;infection_case&#39;] infectionCase = patientinfo.pivot_table(index=&#39;infection_case&#39;,columns=&#39;age&#39;, values=&#39;patient_id&#39;,aggfunc=&quot;count&quot;) #infectionCase #전체 감염 케이스 patientTotal = infectionCase.fillna(0).sum(axis=1) patientTotal = patientTotal.sort_values(ascending = False)[:5] # 60대 감염 케이스 patient60s = infectionCase[&#39;60s&#39;].dropna() patient60sTop = patient60s.sort_values(ascending=False)[:5] # 70대 감염 케이스 patient70s = infectionCase[&#39;70s&#39;].dropna() patient70sTop = patient70s.sort_values(ascending=False)[:5] # 80대 감염 케이스 patient80s = infectionCase[&#39;80s&#39;].dropna() patient80sTop = patient80s.sort_values(ascending=False)[:5] . fig = go.Figure() fig.add_trace(go.Pie(labels=patient60sTop.index, values=patient60sTop.values, textinfo=&#39;label+percent&#39;)) fig.update_layout(title=&#39;Confirmed infection case 60s AGe&#39;) fig.show() . fig = go.Figure() fig.add_trace(go.Pie(labels=patient70sTop.index, values=patient70sTop.values, textinfo=&#39;label+percent&#39;)) fig.update_layout(title=&#39;Confirmed infection case 70s AGe&#39;) fig.show() . fig = go.Figure() fig.add_trace(go.Pie(labels=patient80sTop.index, values=patient80sTop.values, textinfo=&#39;label+percent&#39;)) fig.update_layout(title=&#39;Confirmed infection case 80s AGe&#39;) fig.show() . 60,70,80대의 확진 원인을 보니 nan과 etc가 많기는 하지만 다른 연령대에 비해서 contact가 많다는 사실을 알수 있다. 그래도 nan과 etc가 많아 확진의 주요 원인을 접촉에 의해서 라고 단정할 수는 없다 . 그렇다면 나이가 많은 사람들은 완치 기간이 길어서 치명률이 높은 것이지 이에 대한 관계에 대해 알아보았습니다 . &#51452;&#51228;2 - &#50672;&#47161;&#45824;&#48324; &#54924;&#48373;&#44592;&#44036; . patientinfo . patient_id sex age country province city infection_case infected_by contact_number symptom_onset_date confirmed_date released_date deceased_date state . 0 1000000001 | male | 50s | Korea | Seoul | Gangseo-gu | overseas | NaN | 75 | 2020-01-22 | 2020-01-23 | 2020-02-05 | NaN | released | . 1 1000000002 | male | 30s | Korea | Seoul | Jungnang-gu | overseas | NaN | 31 | NaN | 2020-01-30 | 2020-03-02 | NaN | released | . 2 1000000003 | male | 50s | Korea | Seoul | Jongno-gu | contact | 2002000001 | 17 | NaN | 2020-01-30 | 2020-02-19 | NaN | released | . 3 1000000004 | male | 20s | Korea | Seoul | Mapo-gu | overseas | NaN | 9 | 2020-01-26 | 2020-01-30 | 2020-02-15 | NaN | released | . 4 1000000005 | female | 20s | Korea | Seoul | Seongbuk-gu | contact | 1000000002 | 2 | NaN | 2020-01-31 | 2020-02-24 | NaN | released | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5160 7000000015 | female | 30s | Korea | Jeju-do | Jeju-do | overseas | NaN | 25 | NaN | 2020-05-30 | 2020-06-13 | NaN | released | . 5161 7000000016 | NaN | NaN | Korea | Jeju-do | Jeju-do | overseas | NaN | NaN | NaN | 2020-06-16 | 2020-06-24 | NaN | released | . 5162 7000000017 | NaN | NaN | Bangladesh | Jeju-do | Jeju-do | overseas | NaN | 72 | NaN | 2020-06-18 | NaN | NaN | isolated | . 5163 7000000018 | NaN | NaN | Bangladesh | Jeju-do | Jeju-do | overseas | NaN | NaN | NaN | 2020-06-18 | NaN | NaN | isolated | . 5164 7000000019 | NaN | NaN | Bangladesh | Jeju-do | Jeju-do | overseas | NaN | NaN | NaN | 2020-06-18 | NaN | NaN | isolated | . 5165 rows × 14 columns . from datetime import datetime . pat_rel = patientinfo[[&#39;age&#39;,&#39;confirmed_date&#39;,&#39;released_date&#39;]] #pat_rel[&#39;diff&#39;] = pat_rel.released_date - pat_rel.confirmed_date str(pat_rel.confirmed_date) pat_rel.released_date = pd.to_datetime(pat_rel[&#39;released_date&#39;], format=&#39;%Y %m %d&#39;) pat_rel.confirmed_date = pd.to_datetime(pat_rel[&#39;confirmed_date&#39;], format=&#39;%Y %m %d&#39;) pat_rel[&#39;diff&#39;] = pat_rel.released_date - pat_rel.confirmed_date pat_rel = pat_rel[:5161] #격리 날짜 없는 것 삭제 display(pat_rel) . age confirmed_date released_date diff . 0 50s | 2020-01-23 | 2020-02-05 | 13 days | . 1 30s | 2020-01-30 | 2020-03-02 | 32 days | . 2 50s | 2020-01-30 | 2020-02-19 | 20 days | . 3 20s | 2020-01-30 | 2020-02-15 | 16 days | . 4 20s | 2020-01-31 | 2020-02-24 | 24 days | . ... ... | ... | ... | ... | . 5156 30s | 2020-04-03 | 2020-05-19 | 46 days | . 5157 20s | 2020-04-03 | 2020-05-05 | 32 days | . 5158 10s | 2020-04-14 | 2020-04-26 | 12 days | . 5159 30s | 2020-05-09 | 2020-06-12 | 34 days | . 5160 30s | 2020-05-30 | 2020-06-13 | 14 days | . 5161 rows × 4 columns . display(pat_rel[&#39;diff&#39;].mean()) display(pat_rel[&#39;diff&#39;].min()) display(pat_rel[&#39;diff&#39;].max()) . Timedelta(&#39;24 days 17:48:39.041614123&#39;) . Timedelta(&#39;0 days 00:00:00&#39;) . Timedelta(&#39;114 days 00:00:00&#39;) . 평균 완치일은 24일 . 최대 완치일은 114일 입니다. . pat_rel[&#39;over_avg&#39;] = np.where(pat_rel[&#39;diff&#39;]&gt;&#39;24 days 17:48:39.041614123&#39;,1,0) over_av_released = pat_rel[pat_rel[&#39;over_avg&#39;]==1] under_av_released = pat_rel[pat_rel[&#39;over_avg&#39;]==0] over_av=pd.DataFrame(over_av_released[&#39;age&#39;].value_counts().sort_index()).reset_index() under_av=pd.DataFrame(under_av_released[&#39;age&#39;].value_counts().sort_index()).reset_index() #연령대층별로 감염자수가 확연히 다르기때문에 각 연령층별의 비율로 계산 under_av[&#39;per&#39;]=under_av[&#39;age&#39;]/(under_av[&#39;age&#39;]+over_av[&#39;age&#39;]) over_av[&#39;per&#39;]=over_av[&#39;age&#39;]/(under_av[&#39;age&#39;]+over_av[&#39;age&#39;]) #컬럼 재정리 under_av.columns=[&#39;age&#39;, &#39;count&#39;, &#39;under_per&#39;] over_av.columns=[&#39;age&#39;, &#39;count&#39;, &#39;over_per&#39;] . over_av = pd.DataFrame({&#39;age&#39;:[&#39;0s&#39;,&#39;10s&#39;,&#39;20s&#39;,&#39;30s&#39;,&#39;40s&#39;,&#39;50s&#39;,&#39;60s&#39;,&#39;70s&#39;,&#39;80s&#39;,&#39;90s&#39;], &#39;count&#39;:[7,20,156,90,86,119,90,46,39,8], &#39;over_per&#39;:[0.106061,0.006289,0.026212,0.264856,0.172414,0.135647,0.232877,0.326087,0.2598887,0.487500]}) . fig = go.Figure() fig.add_trace(go.Bar(x=under_av.under_per, y=under_av.age, name=&#39;빠른 완치 기간&#39;, orientation=&#39;h&#39;)) fig.add_trace(go.Bar(x=over_av.over_per, y=over_av.age, name=&#39;오랜 완치 기간&#39;, text=over_av.over_per, texttemplate=&#39;%{x:.1%}&#39;, textposition=&#39;inside&#39;, textfont=dict(color=&#39;white&#39;), orientation=&#39;h&#39;)) fig.update_layout(barmode=&#39;stack&#39;, paper_bgcolor=&#39;rgb(248, 248, 255)&#39;, plot_bgcolor=&#39;rgb(248, 248, 255)&#39;, ) fig.update_layout(title=&#39;연령대에 따른 회복 기간&#39;) fig.show() . 그래프를 본다면 0~20대의 연령츠은 10% 미만으로 평균보다 빠른 완치 기간을 가지고 있음을 알 수 있습니다. 고연령층인 70,80,90대의 경우 32,26,48%로 다른 연령층에 비해서 높기는 하지만 과반을 넘지 않기에 고연령층이라고 모두가 장기간의 회복 기간을 가진다고 판단하기 어렵습니다. . &#51452;&#51228;3 - &#51221;&#48512;&#51032; &#51221;&#52293;&#51008; &#53440;&#45817;&#54664;&#45716;&#44032;? . &#51452;&#51228;3 - &#44048;&#50684;&#48337; &#44221;&#48372; &#45800;&#44228; &#44277;&#54364; &#49884;&#51216; . policy.head() . policy_id country type gov_policy detail start_date end_date . 0 1 | Korea | Alert | Infectious Disease Alert Level | Level 1 (Blue) | 2020-01-03 | 2020-01-19 | . 1 2 | Korea | Alert | Infectious Disease Alert Level | Level 2 (Yellow) | 2020-01-20 | 2020-01-27 | . 2 3 | Korea | Alert | Infectious Disease Alert Level | Level 3 (Orange) | 2020-01-28 | 2020-02-22 | . 3 4 | Korea | Alert | Infectious Disease Alert Level | Level 4 (Red) | 2020-02-23 | NaN | . 4 5 | Korea | Immigration | Special Immigration Procedure | from China | 2020-02-04 | NaN | . policy.isna().sum() #여기서 end_date의 NA값이 너무 많은 것을 알 수 있다. 따라서 기점을 start_date로 지정하고 사용을 하겠다. . policy_id 0 country 0 type 0 gov_policy 0 detail 2 start_date 0 end_date 37 dtype: int64 . policy_alerts = policy[policy.type == &#39;Alert&#39;] display(policy_alerts) . policy_id country type gov_policy detail start_date end_date . 0 1 | Korea | Alert | Infectious Disease Alert Level | Level 1 (Blue) | 2020-01-03 | 2020-01-19 | . 1 2 | Korea | Alert | Infectious Disease Alert Level | Level 2 (Yellow) | 2020-01-20 | 2020-01-27 | . 2 3 | Korea | Alert | Infectious Disease Alert Level | Level 3 (Orange) | 2020-01-28 | 2020-02-22 | . 3 4 | Korea | Alert | Infectious Disease Alert Level | Level 4 (Red) | 2020-02-23 | NaN | . fig, ax = plt.subplots(figsize=(13,7)) plt.title(&quot;Infection Disease Alert Level&quot;,size=17) plt.plot(time.date.unique(), time.confirmed.diff(), color = &#39;gray&#39;, lw = 3) ax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)]) for day, color in zip(policy_alerts.start_date.values[1:], [&#39;yellow&#39;,&#39;orange&#39;,&#39;red&#39;]): ax.axvline(day, ls=&#39;:&#39;, color=color, lw=4) ax.legend([&#39;confirmed&#39;,&#39;alert level 2&#39;,&#39;alert level 3&#39;,&#39;alert level 4&#39;]) plt.xlabel(&#39;date&#39;) plt.ylabel(&#39;Number of Confirmed&#39;) plt.show() . 다음은 감염병의 경보 단계 별로 해당 시점과 확진자의 일일 추이를 나타낸 것으로 2,3단계는 발생을 하고 국내에 들어온 시점에 공표가 되었고 가장 강력한 단계인 4단계는 일일 확진자가 정점에 이르기 전에 공표가 된 사실을 알수 있습니다. . 그렇다면 정부의 거리두기는 어떠하였는지 알아보겠습니다. . &#51452;&#51228;3 - &#51221;&#48512;&#51032; &#44144;&#47532;&#46160;&#44592; &#44277;&#54364; &#49884;&#51216; . policy_social = policy[policy.type == &#39;Social&#39;][:-1] display(policy_social) . policy_id country type gov_policy detail start_date end_date . 28 29 | Korea | Social | Social Distancing Campaign | Strong | 2020-02-29 | 2020-03-21 | . 29 30 | Korea | Social | Social Distancing Campaign | Strong | 2020-03-22 | 2020-04-19 | . 30 31 | Korea | Social | Social Distancing Campaign | Weak | 2020-04-20 | 2020-05-05 | . 31 32 | Korea | Social | Social Distancing Campaign | Weak(1st) | 2020-05-06 | NaN | . fig, ax = plt.subplots(figsize=(13,7)) plt.title(&quot;Social Distancing Campaign&quot;,size=17) plt.plot(time.date.unique(), time.confirmed.diff(), color = &#39;gray&#39;, lw = 3) ax.set_xticks(ax.get_xticks()[::int(len(time.date.unique())/8)]) for day, color in zip(policy_social.start_date.values[:], [&#39;red&#39;,&#39;red&#39;,&#39;orange&#39;]): ax.axvline(day, ls=&#39;:&#39;, color=color, lw=4) ax.legend([&#39;confirmed&#39;,&#39;strong&#39;,&#39;strong&#39;,&#39;weak&#39;]) plt.xlabel(&#39;date&#39;) plt.ylabel(&#39;Number of Confirmed&#39;) plt.show() . 3월 23일에 감염병 경고가 4단계 발표 되고 나서 3월 29일부터 사회적 거리두기를 진행 하였습니다. 두번의 강한 거리두기를 유지하고 4월 20일부터 거리두기의 단계를 낮췄습니다. 거리두기를 확진자의 정점에 다다르는 시점에서 발표하고 그 이후 감소하는 일일 확진자의 수를 확인할 수 있다 따라서 분석을 하고 있는 해당 기간 동안은 정부의 방역 정책은 성공을 했다고 할 수 있다 . &#44208;&#47200; . 20대가 분석 기간동안 가장 많이 확진이 된 연령층이었으며, 20대의 인구수가 다른 연령층에 비해서 많아서가 아닌 다른 연령층에 비해서 활동 반경이 넓고 활발하며 불필요한 방문 지역과 장소에 자주 방문을 하였기 때문에 20대가 가장 많이 확진된 연령층이라는 결론을 내렸습니다. . | 코로나 바이러스는 고연령층이 될수록 더욱 치명적이라는 사실을 얻었습니다. 치명률이 인구 대비 비율로 살펴보아도 80대 이상이 가장 압도적으로 많았고 60,70대 또한 적지 않은 치명률을 가지고 있음을 알게 되었으며, 완치까지 걸리는 기간은 고연령층일수록 저연령층에 비해서 평균 이상으로 오래 걸리기는 하였으나 과반이상이거나 다른 연령층에 비해서 조금은 많치만 크게 상관성이 있어보지는 않았습니다. . | 정부의 거리두기는 당시 신천지로 인해서 일일 확진자가 800명 가량 나오던 시점에 공표되고 그 이후로 분석기간 동안에는 감소세를 보였습니다. 이로 정부의 거리두기는 적어도 당시에는 타당했다라는 결론을 내리게 되었습니다. . | . 코로나 종식 및 예방을 위해서는 해외 유입에 의한 확진자를 차단해야합니다. 현재 입국자에 대한 검사 및 2주 자가격리 등 많은 노력이 진행되고 있습니다. 하지만, 그럼에도 유의사항을 잘 따르지 않는 일부 인원에 의해서 신천지와 같은 큰 집단 감염이 발생될 수 있다는 사실을 잊지 말아야합니다. 따라서, 코로나에 대한 경각심과 인식을 잘 심어주어야하며, 특히나 가장 안일하게 생각하는 20대의 인식 변화를 이끌어야 할 것입니다. 또한, 20대의 행동 패턴 및 방문 경로를 바탕으로 감염 위험이 있는 업종은 특히나 더욱 신경써서 사회적 거리두기, 마스크 착용, 손세정제와 손씻기 등을 더욱 권장하도록 해야합니다. . 분석을 하면서 느낀 한계점은 자료에 etc나 NaN으로 표시된 자료의 형태가 많아서 정확한 원인들을 찾기에 어려움이 있었습니다. . 자료 출처 및 참고 출처 . https://dacon.io/competitions/official/235590/overview/description (데이콘 - 코로나 데이터 시각화 AI 경진대회) | https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset (kaggle - Data Science for COVID-19 in South Korea) | https://chancoding.tistory.com/119 (plotly line plot) | https://plotly.com/python/pie-charts/(about pie plot) | .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/06/07/last.html",
            "relUrl": "/jupyter/python/2022/06/07/last.html",
            "date": " • Jun 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Proximity Analysis",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . import math import geopandas as gpd import pandas as pd from shapely.geometry import MultiPolygon import folium from folium import Choropleth, Marker from folium.plugins import HeatMap, MarkerCluster from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex5 import * . You&#39;ll use the embed_map() function to visualize your maps. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Visualize the collision data. . Run the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018. . collisions = gpd.read_file(&quot;../input/geospatial-learn-course-data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp&quot;) collisions.head() . Use the &quot;LATITUDE&quot; and &quot;LONGITUDE&quot; columns to create an interactive map to visualize the collision data. What type of map do you think is most effective? . &quot;LATITUDE&quot; 및 &quot;LONGITUDE&quot; 열을 사용하여 충돌 데이터를 시각화하는 대화형 맵을 만듭니다. 어떤 유형의 지도가 가장 효과적이라고 생각하십니까? . m_1 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the collision data HeatMap(data=collisions[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m_1) # Uncomment to see a hint #q_1.hint() # Show the map embed_map(m_1, &quot;q_1.html&quot;) . #q_1.check() # Uncomment to see our solution (your code may look different!) #q_1.solution() . 2) Understand hospital coverage. . Run the next code cell to load the hospital data. . hospitals = gpd.read_file(&quot;../input/geospatial-learn-course-data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp&quot;) hospitals.head() . Use the &quot;latitude&quot; and &quot;longitude&quot; columns to visualize the hospital locations. . &quot;위도&quot; 및 &quot;경도&quot; 열을 사용하여 병원 위치를 시각화합니다. . m_2 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the hospital locations for idx, row in hospitals.iterrows(): Marker([row[&#39;latitude&#39;], row[&#39;longitude&#39;]], popup=row[&#39;name&#39;]).add_to(m_2) # Uncomment to see a hint #q_2.hint() # Show the map embed_map(m_2, &quot;q_2.html&quot;) . q_2.check() # Uncomment to see our solution (your code may look different!) #q_2.solution() . 3) When was the closest hospital more than 10 kilometers away? . Create a DataFrame outside_range containing all rows from collisions with crashes that occurred more than 10 kilometers from the closest hospital. . 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌이 있는 &#39;충돌&#39;의 모든 행을 포함하는 DataFrame &#39;outside_range&#39;를 만듭니다. . Note that both hospitals and collisions have EPSG 2263 as the coordinate reference system, and EPSG 2263 has units of meters. . &#39;병원&#39;과 &#39;충돌&#39; 모두 좌표 참조 시스템으로 EPSG 2263을 사용하고 EPSG 2263은 미터 단위를 사용합니다. . coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) my_union = coverage.geometry.unary_union outside_range = collisions.loc[~collisions[&quot;geometry&quot;].apply(lambda x: my_union.contains(x))] # Check your answer q_3.check() . #q_3.hint() #q_3.solution() . The next code cell calculates the percentage of collisions that occurred more than 10 kilometers away from the closest hospital. . percentage = round(100*len(outside_range)/len(collisions), 2) print(&quot;Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(percentage)) . 4) Make a recommender. . When collisions occur in distant locations, it becomes even more vital that injured persons are transported to the nearest available hospital. . 멀리 떨어진 곳에서 충돌이 발생하면 부상자를 가장 가까운 병원으로 이송하는 것이 더욱 중요해집니다. . With this in mind, you decide to create a recommender that: . takes the location of the crash (in EPSG 2263) as input, | finds the closest hospital (where distance calculations are done in EPSG 2263), and | returns the name of the closest hospital. | . def best_hospital(collision_location): idx_min = hospitals.geometry.distance(collision_location).idxmin() my_hospital = hospitals.iloc[idx_min] # Your code here name = my_hospital[&quot;name&quot;] return name # Test your function: this should suggest CALVARY HOSPITAL INC print(best_hospital(outside_range.geometry.iloc[0])) # Check your answer q_4.check() . #q_4.hint() #q_4.solution() . 5) Which hospital is under the highest demand? . Considering only collisions in the outside_range DataFrame, which hospital is most recommended? . outside_range DataFrame에서 충돌만 고려한다면 어느 병원을 가장 추천하는가? . Your answer should be a Python string that exactly matches the name of the hospital returned by the function you created in 4). . highest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax() # Check your answer q_5.check() . #q_5.hint() #q_5.solution() . 6) Where should the city construct new hospitals? . Run the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital. . 다음 코드 셀(변경 없이)을 실행하여 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 외에도 병원 위치를 시각화합니다. . m_6 = folium.Map(location=[40.7, -74], zoom_start=11) coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6) HeatMap(data=outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m_6) folium.LatLngPopup().add_to(m_6) embed_map(m_6, &#39;m_6.html&#39;) . Click anywhere on the map to see a pop-up with the corresponding location in latitude and longitude. . The city of New York reaches out to you for help with deciding locations for two brand new hospitals. They specifically want your help with identifying locations to bring the calculated percentage from step 3) to less than ten percent. Using the map (and without worrying about zoning laws or what potential buildings would have to be removed in order to build the hospitals), can you identify two locations that would help the city accomplish this goal? . Put the proposed latitude and longitude for hospital 1 in lat_1 and long_1, respectively. (Likewise for hospital 2.) . 병원 1에 대해 제안된 위도와 경도를 각각 lat_1과 long_1에 넣습니다. (병원2도 마찬가지) . Then, run the rest of the cell as-is to see the effect of the new hospitals. Your answer will be marked correct, if the two new hospitals bring the percentage to less than ten percent. . 그런 다음 나머지 셀을 그대로 실행하여 새 병원의 효과를 확인하십시오. 두 개의 새로운 병원에서 백분율을 10% 미만으로 낮추면 답이 정답으로 표시됩니다. . lat_1 = 40.6714 long_1 = -73.8492 # Your answer here: proposed location of hospital 2 lat_2 = 40.6702 long_2 = -73.7612 # Do not modify the code below this line try: new_df = pd.DataFrame( {&#39;Latitude&#39;: [lat_1, lat_2], &#39;Longitude&#39;: [long_1, long_2]}) new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude)) new_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} new_gdf = new_gdf.to_crs(epsg=2263) # get new percentage new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000) new_my_union = new_coverage.geometry.unary_union new_outside_range = outside_range.loc[~outside_range[&quot;geometry&quot;].apply(lambda x: new_my_union.contains(x))] new_percentage = round(100*len(new_outside_range)/len(collisions), 2) print(&quot;(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(new_percentage)) # Did you help the city to meet its goal? q_6.check() # make the map m = folium.Map(location=[40.7, -74], zoom_start=11) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m) folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m) for idx, row in new_gdf.iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]]).add_to(m) HeatMap(data=new_outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m) folium.LatLngPopup().add_to(m) display(embed_map(m, &#39;q_6.html&#39;)) except: q_6.hint() . #q_6.solution() . Congratulations! . You have just completed the Geospatial Analysis micro-course! Great job! . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/05/26/exercise-proximity-analysis.html",
            "relUrl": "/jupyter/python/2022/05/26/exercise-proximity-analysis.html",
            "date": " • May 26, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Manipulating Geospatial Data",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . import math import pandas as pd import geopandas as gpd #from geopy.geocoders import Nominatim # What you&#39;d normally run from learntools.geospatial.tools import Nominatim # Just for this exercise import folium from folium import Marker from folium.plugins import MarkerCluster from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex4 import * . You&#39;ll use the embed_map() function from the previous exercise to visualize your maps. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Geocode the missing locations. . Run the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California. . starbucks = pd.read_csv(&quot;../input/geospatial-learn-course-data/starbucks_locations.csv&quot;) starbucks.head() . Most of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing. . print(starbucks.isnull().sum()) # View rows with missing locations rows_with_missing = starbucks[starbucks[&quot;City&quot;]==&quot;Berkeley&quot;] rows_with_missing . Use the code cell below to fill in these values with the Nominatim geocoder. . Note that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course. . 튜토리얼에서 우리는 값을 지오코딩하기 위해 Nominatim()(geopy.geocoders에서)을 사용했으며 이것은 이 과정 이외의 자신의 프로젝트에서 사용할 수 있는 것입니다. . In this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas. . 이 연습에서는 약간 다른 함수 Nominatim()(learntools.geospatial.tools에서)을 사용합니다. 이 기능은 노트북 상단에서 가져온 것으로 GeoPandas의 기능과 동일하게 작동합니다. . So, in other words, as long as: . you don&#39;t change the import statements at the top of the notebook, and | you call the geocoding function as geocode() in the code cell below, | . your code will work as intended! . geolocator = Nominatim(user_agent=&quot;kaggle_learn&quot;) # Your code here def my_geocoder(row): point = geolocator.geocode(row).point return pd.Series({&#39;Latitude&#39;: point.latitude, &#39;Longitude&#39;: point.longitude}) berkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x[&#39;Address&#39;]), axis=1) starbucks.update(berkeley_locations) # Check your answer q_1.check() . #q_1.solution() . 2) View Berkeley locations. . Let&#39;s take a look at the locations you just found. Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style. . 방금 찾은 위치를 살펴보겠습니다. OpenStreetMap 스타일로 버클리의 (위도, 경도) 위치를 시각화합니다. . m_2 = folium.Map(location=[37.88,-122.26], zoom_start=13) # Your code here: Add a marker for each Berkeley location for idx, row in starbucks[starbucks[&quot;City&quot;]==&#39;Berkeley&#39;].iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]]).add_to(m_2) # Uncomment to see a hint #q_2.a.hint() # Show the map embed_map(m_2, &#39;q_2.html&#39;) . q_2.a.check() # Uncomment to see our solution (your code may look different!) #q_2.a.solution() . Considering only the five locations in Berkeley, how many of the (latitude, longitude) locations seem potentially correct (are located in the correct city)? . q_2.b.solution() . 3) Consolidate your data. . Run the code below to load a GeoDataFrame CA_counties containing the name, area (in square kilometers), and a unique id (in the &quot;GEOID&quot; column) for each county in the state of California. The &quot;geometry&quot; column contains a polygon with county boundaries. . CA_counties = gpd.read_file(&quot;../input/geospatial-learn-course-data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp&quot;) CA_counties.head() . Next, we create three DataFrames: . CA_pop contains an estimate of the population of each county. | CA_high_earners contains the number of households with an income of at least $150,000 per year. | CA_median_age contains the median age for each county. | . CA_pop = pd.read_csv(&quot;../input/geospatial-learn-course-data/CA_county_population.csv&quot;, index_col=&quot;GEOID&quot;) CA_high_earners = pd.read_csv(&quot;../input/geospatial-learn-course-data/CA_county_high_earners.csv&quot;, index_col=&quot;GEOID&quot;) CA_median_age = pd.read_csv(&quot;../input/geospatial-learn-course-data/CA_county_median_age.csv&quot;, index_col=&quot;GEOID&quot;) . Use the next code cell to join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age. . Name the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: &quot;GEOID&quot;, &quot;name&quot;, &quot;area_sqkm&quot;, &quot;geometry&quot;, &quot;population&quot;, &quot;high_earners&quot;, and &quot;median_age&quot;. Also, make sure the CRS is set to {&#39;init&#39;: &#39;epsg:4326&#39;}. . 결과 GeoDataFrame의 이름을 CA_stats로 지정하고 &quot;GEOID&quot;, &quot;name&quot;, &quot;area_sqkm&quot;, &quot;geometry&quot;, &quot;population&quot;, &quot;high_earners&quot; 및 &quot;median_age&quot;의 8개 열이 있는지 확인합니다. 또한 CRS가 {&#39;init&#39;: &#39;epsg:4326&#39;}으로 설정되어 있는지 확인하십시오. . cols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index() CA_stats = CA_counties.merge(cols_to_add, on=&quot;GEOID&quot;) # Check your answer q_3.check() . #q_3.hint() #q_3.solution() . Now that we have all of the data in one place, it&#39;s much easier to calculate statistics that use a combination of columns. Run the next code cell to create a &quot;density&quot; column with the population density. . CA_stats[&quot;density&quot;] = CA_stats[&quot;population&quot;] / CA_stats[&quot;area_sqkm&quot;] . 4) Which counties look promising? . Collapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria. . Use the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where: . 다음 코드 셀을 사용하여 CA_stats GeoDataFrame에서 행(및 모든 열)의 하위 집합을 포함하는 GeoDataFrame sel_counties를 만듭니다. 특히 다음과 같은 카운티를 선택해야 합니다. . there are at least 100,000 households making $150,000 per year, | the median age is less than 38.5, and | the density of inhabitants is at least 285 (per square kilometer). | . Additionally, selected counties should satisfy at least one of the following criteria: . there are at least 500,000 households making $150,000 per year, | the median age is less than 35.5, or | the density of inhabitants is at least 1400 (per square kilometer). | . sel_counties = CA_stats[((CA_stats.high_earners &gt; 100000) &amp; (CA_stats.median_age &lt; 38.5) &amp; (CA_stats.density &gt; 285) &amp; ((CA_stats.median_age &lt; 35.5) | (CA_stats.density &gt; 1400) | (CA_stats.high_earners &gt; 500000)))] # Check your answer q_4.check() . #q_4.hint() #q_4.solution() . 5) How many stores did you identify? . When looking for the next Starbucks Reserve Roastery location, you&#39;d like to consider all of the stores within the counties that you selected. So, how many stores are within the selected counties? . To prepare to answer this question, run the next code cell to create a GeoDataFrame starbucks_gdf with all of the starbucks locations. . starbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude)) starbucks_gdf.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} . So, how many stores are in the counties you selected? . 그렇다면 선택한 카운티에는 몇 개의 매장이 있습니까? . locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties) num_stores = len(locations_of_interest) # Check your answer q_5.check() . #q_5.hint() #q_5.solution() . 6) Visualize the store locations. . Create a map that shows the locations of the stores that you identified in the previous question. . 이전 질문에서 식별한 상점의 위치를 ​​보여주는 지도를 만드십시오. . m_6 = folium.Map(location=[37,-120], zoom_start=6) # Your code here: show selected store locations mc = MarkerCluster() locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties) for idx, row in locations_of_interest.iterrows(): if not math.isnan(row[&#39;Longitude&#39;]) and not math.isnan(row[&#39;Latitude&#39;]): mc.add_child(folium.Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]])) m_6.add_child(mc) # Uncomment to see a hint #q_6.hint() # Show the map embed_map(m_6, &#39;q_6.html&#39;) . #q_6.check() # Uncomment to see our solution (your code may look different!) #q_6.solution() . Keep going . Learn about how proximity analysis can help you to understand the relationships between points on a map. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/05/26/exercise-manipulating-geospatial-data.html",
            "relUrl": "/jupyter/python/2022/05/26/exercise-manipulating-geospatial-data.html",
            "date": " • May 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "NYC taxi",
            "content": "Visual Analytics with Python . 강의자료 출처 - kaggle.com . In this script we will explore the spatial and temporal behavior of the people of New York as can be inferred by examining their cab usage. . The main fields of this dataset are taxi pickup time and location, as well as dropoff location and trip duration. There is a total of around 1.4 Million trips in the dataset that took place during the first half of 2016. . We will study how the patterns of cab usage change throughout the year, throughout the week and throughout the day, and we will focus on difference between weekdays and weekends. . %matplotlib inline from sklearn import decomposition from scipy import stats from sklearn import cluster import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(&#39;fivethirtyeight&#39;) matplotlib.rcParams[&#39;font.size&#39;] = 12 matplotlib.rcParams[&#39;figure.figsize&#39;] = (10,10) . Load data and preprocess measurements to sensible units . data_frame = pd.read_csv(&#39;train.csv&#39;) . data_frame.describe() . vendor_id passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude trip_duration . count 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | . mean 1.534950e+00 | 1.664530e+00 | -7.397349e+01 | 4.075092e+01 | -7.397342e+01 | 4.075180e+01 | 9.594923e+02 | . std 4.987772e-01 | 1.314242e+00 | 7.090186e-02 | 3.288119e-02 | 7.064327e-02 | 3.589056e-02 | 5.237432e+03 | . min 1.000000e+00 | 0.000000e+00 | -1.219333e+02 | 3.435970e+01 | -1.219333e+02 | 3.218114e+01 | 1.000000e+00 | . 25% 1.000000e+00 | 1.000000e+00 | -7.399187e+01 | 4.073735e+01 | -7.399133e+01 | 4.073588e+01 | 3.970000e+02 | . 50% 2.000000e+00 | 1.000000e+00 | -7.398174e+01 | 4.075410e+01 | -7.397975e+01 | 4.075452e+01 | 6.620000e+02 | . 75% 2.000000e+00 | 2.000000e+00 | -7.396733e+01 | 4.076836e+01 | -7.396301e+01 | 4.076981e+01 | 1.075000e+03 | . max 2.000000e+00 | 9.000000e+00 | -6.133553e+01 | 5.188108e+01 | -6.133553e+01 | 4.392103e+01 | 3.526282e+06 | . data_frame.shape . (1458644, 11) . np.max(data_frame[&#39;trip_duration&#39;]) #이렇게 보면서 이상치가 있는 것을 눈으로 봄 3526282(전처리전) --&gt; 4764(전처리후) . 3526282 . allLat = np.array(list(data_frame[&#39;pickup_latitude&#39;]) + list(data_frame[&#39;dropoff_latitude&#39;])) #두개의 컬럼을 리스트로 합쳐줌 allLong = np.array(list(data_frame[&#39;pickup_longitude&#39;]) + list(data_frame[&#39;dropoff_longitude&#39;])) longLimits = [np.percentile(allLong, 0.3), np.percentile(allLong, 99.7)] latLimits = [np.percentile(allLat , 0.3), np.percentile(allLat , 99.7)] durLimits = [np.percentile(data_frame[&#39;trip_duration&#39;], 0.4), np.percentile(data_frame[&#39;trip_duration&#39;], 99.7)] data_frame = data_frame[(data_frame[&#39;pickup_latitude&#39;] &gt;= latLimits[0] ) &amp; (data_frame[&#39;pickup_latitude&#39;] &lt;= latLimits[1]) ] #이상치 제거 data_frame = data_frame[(data_frame[&#39;dropoff_latitude&#39;] &gt;= latLimits[0] ) &amp; (data_frame[&#39;dropoff_latitude&#39;] &lt;= latLimits[1]) ] data_frame = data_frame[(data_frame[&#39;pickup_longitude&#39;] &gt;= longLimits[0]) &amp; (data_frame[&#39;pickup_longitude&#39;] &lt;= longLimits[1])] data_frame = data_frame[(data_frame[&#39;dropoff_longitude&#39;] &gt;= longLimits[0]) &amp; (data_frame[&#39;dropoff_longitude&#39;] &lt;= longLimits[1])] data_frame = data_frame[(data_frame[&#39;trip_duration&#39;] &gt;= durLimits[0] ) &amp; (data_frame[&#39;trip_duration&#39;] &lt;= durLimits[1]) ] data_frame = data_frame.reset_index(drop=True) . medianLat = np.percentile(allLat,50) medianLong = np.percentile(allLong,50) latMultiplier = 111.32 longMultiplier = np.cos(medianLat*(np.pi/180.0)) * 111.32 data_frame[&#39;duration [min]&#39;] = data_frame[&#39;trip_duration&#39;]/60.0 data_frame[&#39;src lat [km]&#39;] = latMultiplier * (data_frame[&#39;pickup_latitude&#39;] - medianLat) data_frame[&#39;src long [km]&#39;] = longMultiplier * (data_frame[&#39;pickup_longitude&#39;] - medianLong) data_frame[&#39;dst lat [km]&#39;] = latMultiplier * (data_frame[&#39;dropoff_latitude&#39;] - medianLat) data_frame[&#39;dst long [km]&#39;] = longMultiplier * (data_frame[&#39;dropoff_longitude&#39;] - medianLong) allLat = np.array(list(data_frame[&#39;src lat [km]&#39;]) + list(data_frame[&#39;dst lat [km]&#39;])) allLong = np.array(list(data_frame[&#39;src long [km]&#39;]) + list(data_frame[&#39;dst long [km]&#39;])) . Plot the histograms of trip duration, latitude and longitude . fig, axArray = plt.subplots(nrows=1,ncols=3,figsize=(13,4)) axArray[0].hist(data_frame[&#39;duration [min]&#39;],80); axArray[0].set_xlabel(&#39;trip duration [min]&#39;); axArray[0].set_ylabel(&#39;counts&#39;) axArray[1].hist(allLat ,80); axArray[1].set_xlabel(&#39;latitude [km]&#39;) axArray[2].hist(allLong,80); axArray[2].set_xlabel(&#39;longitude [km]&#39;) #위경도는 큰의미는 없지만 플랏으로 이상치가 있는지 시각적으로 확인 . Text(0.5, 0, &#39;longitude [km]&#39;) . Plot the trip Duration vs. the Aerial Distance between pickup and dropoff . data_frame[&#39;log duration&#39;] = np.log1p(data_frame[&#39;duration [min]&#39;]) data_frame[&#39;euclidian distance&#39;] = np.sqrt((data_frame[&#39;src lat [km]&#39;] - data_frame[&#39;dst lat [km]&#39;] )**2 + (data_frame[&#39;src long [km]&#39;] - data_frame[&#39;dst long [km]&#39;])**2) fig, axArray = plt.subplots(nrows=1,ncols=2,figsize=(13,6)) axArray[0].scatter(data_frame[&#39;euclidian distance&#39;], data_frame[&#39;duration [min]&#39;],c=&#39;b&#39;,s=5,alpha=0.01); axArray[0].set_xlabel(&#39;Aerial Euclidian Distance [km]&#39;); axArray[0].set_ylabel(&#39;Duration [min]&#39;) axArray[0].set_xlim(data_frame[&#39;euclidian distance&#39;].min(),data_frame[&#39;euclidian distance&#39;].max()) axArray[0].set_ylim(data_frame[&#39;duration [min]&#39;].min(),data_frame[&#39;duration [min]&#39;].max()) axArray[0].set_title(&#39;trip Duration vs Aerial trip Distance&#39;) axArray[1].scatter(data_frame[&#39;euclidian distance&#39;], data_frame[&#39;log duration&#39;],c=&#39;b&#39;,s=5,alpha=0.01); axArray[1].set_xlabel(&#39;Aerial Euclidian Distance [km]&#39;); axArray[1].set_ylabel(&#39;log(1+Duration) [log(min)]&#39;) axArray[1].set_xlim(data_frame[&#39;euclidian distance&#39;].min(),data_frame[&#39;euclidian distance&#39;].max()) axArray[1].set_ylim(data_frame[&#39;log duration&#39;].min(),data_frame[&#39;log duration&#39;].max()) axArray[1].set_title(&#39;log of trip Duration vs Aerial trip Distance&#39;) . Text(0.5, 1.0, &#39;log of trip Duration vs Aerial trip Distance&#39;) . Exercise 1 . 위의 Scatter plot은 Point가 너무 많이 존재하여 좋은 시각화가 아닐 수 있습니다. 보다 효율적인 시각화 방안을 제시해 보세요. . matplotlib, seaborn, plotly 등 다양한 패키지를 활용할 수 있습니다 . import seaborn as sns sns.jointplot(x=data_frame[&#39;euclidian distance&#39;], y=data_frame[&#39;duration [min]&#39;], kind=&quot;hex&quot;, color=&quot;#4CB391&quot;, xlim = (0,30), ylim = (0,70)) . &lt;seaborn.axisgrid.JointGrid at 0x1d6899db160&gt; . p = sns.jointplot(x=np.log1p(data_frame[&#39;euclidian distance&#39;]), y=data_frame[&#39;log duration&#39;], kind=&quot;hex&quot;, color=&quot;#4CB391&quot;) #log를 취해 주어서 히트맵을 변환시켜 보여줌 p.ax_joint.set_xlabel(&#39;log(1+Aerial Euclidian Distance) [log(km)]&#39;) p.ax_joint.set_ylabel(&#39;log(1+Duration) [log(min)]&#39;) . Text(8.060000000000002, 0.5, &#39;log(1+Duration) [log(min)]&#39;) . We can see that the trip distance defines the lower bound on trip duration, as one would expect. . Plot spatial density plot of the pickup and dropoff locations . imageSize = (700,700) longRange = [-5,19] latRange = [-13,11] allLatInds = imageSize[0] - (imageSize[0] * (allLat - latRange[0]) / (latRange[1] - latRange[0]) ).astype(int) allLongInds = (imageSize[1] * (allLong - longRange[0]) / (longRange[1] - longRange[0])).astype(int) locationDensityImage = np.zeros(imageSize) for latInd, longInd in zip(allLatInds,allLongInds): locationDensityImage[latInd,longInd] += 1 fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,12)) ax.imshow(np.log(locationDensityImage+1),cmap=&#39;inferno&#39;) ax.set_axis_off() . Exercise 2 . folium이나 pydeck을 사용하여 택시의 승차지점, 하차지점을 시각화 해봅시다. 디서 승객이 많이 타고 내리는지를 확인할 수 있습니까? . data_frame.head() . id vendor_id pickup_datetime dropoff_datetime passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude store_and_fwd_flag trip_duration duration [min] src lat [km] src long [km] dst lat [km] dst long [km] log duration euclidian distance . 0 id2875421 | 2 | 2016-03-14 17:24:55 | 2016-03-14 17:32:30 | 1 | -73.982155 | 40.767937 | -73.964630 | 40.765602 | N | 455 | 7.583333 | 1.516008 | -0.110015 | 1.256121 | 1.367786 | 2.149822 | 1.500479 | . 1 id2377394 | 1 | 2016-06-12 00:43:35 | 2016-06-12 00:54:38 | 1 | -73.980415 | 40.738564 | -73.999481 | 40.731152 | N | 663 | 11.050000 | -1.753813 | 0.036672 | -2.578912 | -1.571088 | 2.489065 | 1.807119 | . 2 id3858529 | 2 | 2016-01-19 11:35:24 | 2016-01-19 12:10:48 | 1 | -73.979027 | 40.763939 | -74.005333 | 40.710087 | N | 2124 | 35.400000 | 1.070973 | 0.153763 | -4.923841 | -2.064547 | 3.594569 | 6.392080 | . 3 id3504673 | 2 | 2016-04-06 19:32:31 | 2016-04-06 19:39:40 | 1 | -74.010040 | 40.719971 | -74.012268 | 40.706718 | N | 429 | 7.150000 | -3.823568 | -2.461500 | -5.298809 | -2.649362 | 2.098018 | 1.487155 | . 4 id2181028 | 2 | 2016-03-26 13:30:55 | 2016-03-26 13:38:10 | 1 | -73.973053 | 40.793209 | -73.972923 | 40.782520 | N | 435 | 7.250000 | 4.329328 | 0.657515 | 3.139453 | 0.668452 | 2.110213 | 1.189925 | . data_frame.columns . Index([&#39;id&#39;, &#39;vendor_id&#39;, &#39;pickup_datetime&#39;, &#39;dropoff_datetime&#39;, &#39;passenger_count&#39;, &#39;pickup_longitude&#39;, &#39;pickup_latitude&#39;, &#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;, &#39;store_and_fwd_flag&#39;, &#39;trip_duration&#39;, &#39;duration [min]&#39;, &#39;src lat [km]&#39;, &#39;src long [km]&#39;, &#39;dst lat [km]&#39;, &#39;dst long [km]&#39;, &#39;log duration&#39;, &#39;euclidian distance&#39;], dtype=&#39;object&#39;) . import pydeck as pdk . layer = pdk.Layer( &quot;HexagonLayer&quot;, data_frame[:1000], get_position=[&quot;pickup_longitude&quot;, &quot;pickup_latitude&quot;], auto_highlight=True, elevation_scale=50, pickable=True, elevation_range=[0, 50], extruded=True, coverage=1, ) # Set the viewport location view_lon = np.mean(data_frame[&quot;pickup_longitude&quot;]) view_lat = np.mean(data_frame[&quot;pickup_latitude&quot;]) view_state = pdk.ViewState( longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36, ) # Render r = pdk.Deck(layers=[layer], initial_view_state=view_state) r.show() #r.to_html(&quot;hexagon_ny_taxi.html&quot;) . layer = pdk.Layer( &quot;HeatmapLayer&quot;, data_frame[:1000], get_position=[&quot;pickup_longitude&quot;, &quot;pickup_latitude&quot;], auto_highlight=True, elevation_scale=50, pickable=True, elevation_range=[0, 50], extruded=True, coverage=1, ) # Set the viewport location view_lon = np.mean(data_frame[&quot;pickup_longitude&quot;]) view_lat = np.mean(data_frame[&quot;pickup_latitude&quot;]) view_state = pdk.ViewState( longitude=view_lon, latitude=view_lat, zoom=10, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36, ) # Render r = pdk.Deck(layers=[layer], initial_view_state=view_state) r.show() . . Closing in on Manhattan . imageSizeMan = (720,480) latRangeMan = [-8,10] longRangeMan = [-5,7] indToKeep = np.logical_and(allLat &gt; latRangeMan[0], allLat &lt; latRangeMan[1]) indToKeep = np.logical_and(indToKeep, np.logical_and(allLong &gt; longRangeMan[0], allLong &lt; longRangeMan[1])) allLatMan = allLat[indToKeep] allLongMan = allLong[indToKeep] allLatIndsMan = (imageSizeMan[0]-1) - (imageSizeMan[0] * (allLatMan - latRangeMan[0]) / (latRangeMan[1] - latRangeMan[0])).astype(int) allLongIndsMan = (imageSizeMan[1] * (allLongMan - longRangeMan[0]) / (longRangeMan[1] - longRangeMan[0])).astype(int) locationDensityImageMan = np.zeros(imageSizeMan) for latInd, longInd in zip(allLatIndsMan,allLongIndsMan): locationDensityImageMan[latInd,longInd] += 1 fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,18)) ax.imshow(np.log(locationDensityImageMan+1),cmap=&#39;inferno&#39;) ax.set_axis_off() . Cluster the Trips and Look at their distribution . pickupTime = pd.to_datetime(data_frame[&#39;pickup_datetime&#39;]) data_frame[&#39;src hourOfDay&#39;] = (pickupTime.dt.hour*60.0 + pickupTime.dt.minute) / 60.0 data_frame[&#39;dst hourOfDay&#39;] = data_frame[&#39;src hourOfDay&#39;] + data_frame[&#39;duration [min]&#39;] / 60.0 data_frame[&#39;dayOfWeek&#39;] = pickupTime.dt.weekday data_frame[&#39;hourOfWeek&#39;] = data_frame[&#39;dayOfWeek&#39;]*24.0 + data_frame[&#39;src hourOfDay&#39;] data_frame[&#39;monthOfYear&#39;] = pickupTime.dt.month data_frame[&#39;dayOfYear&#39;] = pickupTime.dt.dayofyear data_frame[&#39;weekOfYear&#39;] = pickupTime.dt.weekofyear data_frame[&#39;hourOfYear&#39;] = data_frame[&#39;dayOfYear&#39;]*24.0 + data_frame[&#39;src hourOfDay&#39;] . &lt;ipython-input-20-b78d8432b14c&gt;:11: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead. data_frame[&#39;weekOfYear&#39;] = pickupTime.dt.weekofyear . tripAttributes = np.array(data_frame.loc[:,[&#39;src lat [km]&#39;,&#39;src long [km]&#39;,&#39;dst lat [km]&#39;,&#39;dst long [km]&#39;,&#39;duration [min]&#39;]]) meanTripAttr = tripAttributes.mean(axis=0) stdTripAttr = tripAttributes.std(axis=0) tripAttributes = stats.zscore(tripAttributes, axis=0) numClusters = 40 TripKmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=120000, n_init=100, random_state=1) clusterInds = TripKmeansModel.fit_predict(tripAttributes) clusterTotalCounts, _ = np.histogram(clusterInds, bins=numClusters) sortedClusterInds = np.flipud(np.argsort(clusterTotalCounts)) plt.figure(figsize=(12,4)); plt.title(&#39;Cluster Histogram of all trip&#39;) plt.bar(range(1,numClusters+1),clusterTotalCounts[sortedClusterInds]) plt.ylabel(&#39;Frequency [counts]&#39;); plt.xlabel(&#39;Cluster index (sorted by cluster frequency)&#39;) plt.xlim(0,numClusters+1) . (0.0, 41.0) . Plot typical Trips on the Map . def ConvertToImageCoords(latCoord, longCoord, latRange, longRange, imageSize): latInds = imageSize[0] - (imageSize[0] * (latCoord - latRange[0]) / (latRange[1] - latRange[0]) ).astype(int) longInds = (imageSize[1] * (longCoord - longRange[0]) / (longRange[1] - longRange[0])).astype(int) return latInds, longInds templateTrips = TripKmeansModel.cluster_centers_ * np.tile(stdTripAttr,(numClusters,1)) + np.tile(meanTripAttr,(numClusters,1)) srcCoords = templateTrips[:,:2] dstCoords = templateTrips[:,2:4] srcImCoords = ConvertToImageCoords(srcCoords[:,0],srcCoords[:,1], latRange, longRange, imageSize) dstImCoords = ConvertToImageCoords(dstCoords[:,0],dstCoords[:,1], latRange, longRange, imageSize) plt.figure(figsize=(12,12)) plt.imshow(np.log(locationDensityImage+1),cmap=&#39;inferno&#39;); plt.grid(&#39;off&#39;) plt.scatter(srcImCoords[1],srcImCoords[0],c=&#39;m&#39;,s=200,alpha=0.8) plt.scatter(dstImCoords[1],dstImCoords[0],c=&#39;g&#39;,s=200,alpha=0.8) for i in range(len(srcImCoords[0])): plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i], edgecolor=&#39;c&#39;, facecolor=&#39;c&#39;, width=0.8,alpha=0.4,head_width=10.0,head_length=10.0,length . File &#34;&lt;ipython-input-22-39197d099a2f&gt;&#34;, line 22 edgecolor=&#39;c&#39;, facecolor=&#39;c&#39;, width=0.8,alpha=0.4,head_width=10.0,head_length=10.0,length ^ SyntaxError: unexpected EOF while parsing . Calculate the trip distribution for different hours of the weekday . hoursOfDay = np.sort(data_frame[&#39;src hourOfDay&#39;].astype(int).unique()) clusterDistributionHourOfDay_weekday = np.zeros((len(hoursOfDay),numClusters)) for k, hour in enumerate(hoursOfDay): slectedInds = (data_frame[&#39;src hourOfDay&#39;].astype(int) == hour) &amp; (data_frame[&#39;dayOfWeek&#39;] &lt;= 4) currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters) clusterDistributionHourOfDay_weekday[k,:] = currDistribution[sortedClusterInds] fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6)) ax.set_title(&#39;Trip Distribution during Weekdays&#39;, fontsize=12) ax.imshow(clusterDistributionHourOfDay_weekday); ax.grid(&#39;off&#39;) ax.set_xlabel(&#39;Trip Cluster&#39;); ax.set_ylabel(&#39;Hour of Day&#39;) . Calculate the trip distribution for different hours of the weekend . hoursOfDay = np.sort(data_frame[&#39;src hourOfDay&#39;].astype(int).unique()) clusterDistributionHourOfDay_weekend = np.zeros((len(hoursOfDay),numClusters)) for k, hour in enumerate(hoursOfDay): slectedInds = (data_frame[&#39;src hourOfDay&#39;].astype(int) == hour) &amp; (data_frame[&#39;dayOfWeek&#39;] &gt;= 5) currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters) clusterDistributionHourOfDay_weekend[k,:] = currDistribution[sortedClusterInds] fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6)) ax.set_title(&#39;Trip Distribution during Weekends&#39;, fontsize=12) ax.imshow(clusterDistributionHourOfDay_weekend); ax.grid(&#39;off&#39;) ax.set_xlabel(&#39;Trip Cluster&#39;); ax.set_ylabel(&#39;Hour of Day&#39;) . Calculate the trip distribution for day of week . daysOfWeek = np.sort(data_frame[&#39;dayOfWeek&#39;].unique()) clusterDistributionDayOfWeek = np.zeros((len(daysOfWeek),numClusters)) for k, day in enumerate(daysOfWeek): slectedInds = data_frame[&#39;dayOfWeek&#39;] == day currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters) clusterDistributionDayOfWeek[k,:] = currDistribution[sortedClusterInds] plt.figure(figsize=(12,5)); plt.title(&#39;Trip Distribution throughout the Week&#39;) plt.imshow(clusterDistributionDayOfWeek); plt.grid(&#39;off&#39;) plt.xlabel(&#39;Trip Cluster&#39;); plt.ylabel(&#39;Day of Week&#39;) . Calculate the trip distribution for day of year . daysOfYear = data_frame[&#39;dayOfYear&#39;].unique() daysOfYear = np.sort(daysOfYear) clusterDistributionDayOfYear = np.zeros((len(daysOfYear),numClusters)) for k, day in enumerate(daysOfYear): slectedInds = data_frame[&#39;dayOfYear&#39;] == day currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters) clusterDistributionDayOfYear[k,:] = currDistribution[sortedClusterInds] fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(10,16)) ax.set_title(&#39;Trip Distribution throughout the Year&#39;, fontsize=12) ax.imshow(clusterDistributionDayOfYear); ax.grid(&#39;off&#39;) ax.set_xlabel(&#39;Trip Cluster&#39;); ax.set_ylabel(&#39;Day of Year&#39;) ax.annotate(&#39;Large Snowstorm&#39;, color=&#39;r&#39;, fontsize=15 ,xy=(5, 21), xytext=(20, 17), arrowprops=dict(facecolor=&#39;red&#39;, shrink=0.03)) ax.annotate(&#39;Memorial Day&#39;, color=&#39;r&#39;, fontsize=15, xy=(5, 151), xytext=(20, 157), arrowprops=dict(facecolor=&#39;red&#39;, shrink=0.03)) . Computing PCA coefficients . hoursOfYear = np.sort(data_frame[&#39;hourOfYear&#39;].astype(int).unique()) clusterDistributionHourOfYear = np.zeros((len(range(hoursOfYear[0],hoursOfYear[-1])),numClusters)) dayOfYearVec = np.zeros(clusterDistributionHourOfYear.shape[0]) weekdayVec = np.zeros(clusterDistributionHourOfYear.shape[0]) weekOfYearVec = np.zeros(clusterDistributionHourOfYear.shape[0]) for k, hour in enumerate(hoursOfYear): slectedInds = data_frame[&#39;hourOfYear&#39;].astype(int) == hour currDistribution, _ = np.histogram(clusterInds[slectedInds], bins=numClusters) clusterDistributionHourOfYear[k,:] = currDistribution[sortedClusterInds] dayOfYearVec[k] = data_frame[slectedInds][&#39;dayOfYear&#39;].mean() weekdayVec[k] = data_frame[slectedInds][&#39;dayOfWeek&#39;].mean() weekOfYearVec[k] = data_frame[slectedInds][&#39;weekOfYear&#39;].mean() numComponents = 3 TripDistributionPCAModel = decomposition.PCA(n_components=numComponents,whiten=True, random_state=1) compactClusterDistributionHourOfYear = TripDistributionPCAModel.fit_transform(clusterDistributionHourOfYear) . Collect traces for all weeks of year . listOfFullWeeks = [] for uniqueVal in np.unique(weekOfYearVec): if (weekOfYearVec == uniqueVal).sum() == 24*7: listOfFullWeeks.append(uniqueVal) weeklyTraces = np.zeros((24*7,numComponents,len(listOfFullWeeks))) for k, weekInd in enumerate(listOfFullWeeks): weeklyTraces[:,:,k] = compactClusterDistributionHourOfYear[weekOfYearVec == weekInd,:] fig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(10,10)) fig.suptitle(&#39;PCA coefficients during the Week&#39;, fontsize=25) for PC_coeff in range(numComponents): meanTrace = weeklyTraces[:,PC_coeff,:].mean(axis=1) axArray[PC_coeff].plot(weeklyTraces[:,PC_coeff,:],&#39;red&#39;,linewidth=1.5) axArray[PC_coeff].plot(meanTrace,&#39;k&#39;,linewidth=2.5) axArray[PC_coeff].set_ylabel(&#39;PC %d coeff&#39; %(PC_coeff+1)) axArray[PC_coeff].vlines([0,23,47,71,95,119,143,167], weeklyTraces[:,PC_coeff,:].min(), weeklyTraces[:,PC_coeff,:].max(), colors=&#39;black&#39;, lw=2) axArray[PC_coeff].set_xlabel(&#39;hours since start of week&#39;) axArray[PC_coeff].set_xlim(-0.9,24*7-0.1) . Examine what different PC coefficients mean by looking at their trip template distributions . fig, axArray = plt.subplots(nrows=numComponents,ncols=1,sharex=True, figsize=(12,11)) fig.suptitle(&#39;Trip Distribution PCA Components&#39;, fontsize=25) for PC_coeff in range(numComponents): tripTemplateDistributionDifference = TripDistributionPCAModel.components_[PC_coeff,:] * TripDistributionPCAModel.explained_variance_[PC_coeff] axArray[PC_coeff].bar(range(1,numClusters+1),tripTemplateDistributionDifference) axArray[PC_coeff].set_title(&#39;PCA %d component&#39; %(PC_coeff+1)) axArray[PC_coeff].set_ylabel(&#39;delta frequency [counts]&#39;) axArray[PC_coeff].set_xlabel(&#39;cluster index (sorted by cluster frequency)&#39;) axArray[PC_coeff].set_xlim(0,numClusters+0.5) axArray[1].hlines([-25,25], 0, numClusters+0.5, colors=&#39;r&#39;, lw=0.7) axArray[2].hlines([-11,11], 0, numClusters+0.5, colors=&#39;r&#39;, lw=0.7) . We can see that the first PCA component looks very similar to the overall trip distribution, suggesting that it&#39;s mainly a &quot;gain&quot; component that controls just the number of total trips in that period of time. .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/05/25/newyorktaxi.html",
            "relUrl": "/jupyter/python/2022/05/25/newyorktaxi.html",
            "date": " • May 25, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Your First Map",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . import geopandas as gpd from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex1 import * . 1) Get the data. . Use the next cell to load the shapefile located at loans_filepath to create a GeoDataFrame world_loans. . 다음 셀을 사용하여 loans_filepath에 있는 shapefile을 로드하여 GeoDataFrame world_loans를 생성합니다. . loans_filepath = &quot;../input/geospatial-learn-course-data/kiva_loans/kiva_loans/kiva_loans.shp&quot; # Your code here: Load the data world_loans = gpd.read_file(loans_filepath) # Check your answer q_1.check() # Uncomment to view the first five rows of the data #world_loans.head() . 2) Plot the data. . Run the next code cell without changes to load a GeoDataFrame world containing country boundaries. . 변경 없이 다음 코드 셀을 실행하여 국가 경계가 포함된 GeoDataFrame &#39;world&#39;를 로드합니다. . #q_1.hint() #q_1.solution() . world_filepath = gpd.datasets.get_path(&#39;naturalearth_lowres&#39;) world = gpd.read_file(world_filepath) world.head() . Use the world and world_loans GeoDataFrames to visualize Kiva loan locations across the world. . world 및 world_loans GeoDataFrames를 사용하여 전 세계의 Kiva loan locations를 시각화합니다. . ax = world.plot(figsize=(20,20), color=&#39;none&#39;, edgecolor=&#39;gainsboro&#39;,zorder=3) world_loans.plot(color=&#39;skyblue&#39;, markersize=2,ax=ax) # Uncomment to see a hint #q_2.hint() . q_2.check() # Uncomment to see our solution (your code may look different!) #q_2.solution() . 3) Select loans based in the Philippines. . Next, you&#39;ll focus on loans that are based in the Philippines. Use the next code cell to create a GeoDataFrame PHL_loans which contains all rows from world_loans with loans that are based in the Philippines. . 다음으로 필리핀에 기반을 둔 대출에 중점을 둘 것입니다. 다음 코드 셀을 사용하여 필리핀에 기반을 둔 대출이 있는 world_loans의 모든 행을 포함하는 GeoDataFrame PHL_loans를 만듭니다. . PHL_loans = world_loans.loc[world_loans.country==&quot;Philippines&quot;].copy() # Check your answer q_3.check() . #q_3.hint() #q_3.solution() . 4) Understand loans in the Philippines. . Run the next code cell without changes to load a GeoDataFrame PHL containing boundaries for all islands in the Philippines. . 필리핀의 모든 섬에 대한 경계를 포함하는 GeoDataFrame PHL을 로드하려면 변경 없이 다음 코드 셀을 실행하십시오. . gpd.io.file.fiona.drvsupport.supported_drivers[&#39;KML&#39;] = &#39;rw&#39; PHL = gpd.read_file(&quot;../input/geospatial-learn-course-data/Philippines_AL258.kml&quot;, driver=&#39;KML&#39;) PHL.head() . Use the PHL and PHL_loans GeoDataFrames to visualize loans in the Philippines. . &#39;PHL&#39; 및 &#39;PHL_loans&#39; GeoDataFrames를 사용하여 필리핀의 대출을 시각화합니다. . ax = PHL.plot(figsize=(20,20), color=&#39;none&#39;, edgecolor=&#39;gainsboro&#39;, zorder=3) PHL_loans.plot(color=&#39;skyblue&#39;, markersize=2, ax=ax) # Uncomment to see a hint #q_4.a.hint() . q_4.a.check() # Uncomment to see our solution (your code may look different!) #q_4.a.solution() . Can you identify any islands where it might be useful to recruit new Field Partners? Do any islands currently look outside of Kiva&#39;s reach? . You might find this map useful to answer the question. . q_4.b.solution() . Keep going . Continue to learn about coordinate reference systems. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/05/24/exercise-your-first-map.html",
            "relUrl": "/jupyter/python/2022/05/24/exercise-your-first-map.html",
            "date": " • May 24, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Interactive Maps",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . import pandas as pd import geopandas as gpd import folium from folium import Choropleth from folium.plugins import HeatMap from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex3 import * . We define a function embed_map() for displaying interactive maps. It accepts two arguments: the variable containing the map, and the name of the HTML file where the map will be saved. . This function ensures that the maps are visible in all web browsers. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Do earthquakes coincide with plate boundaries? . Run the code cell below to create a DataFrame plate_boundaries that shows global plate boundaries. The &quot;coordinates&quot; column is a list of (latitude, longitude) locations along the boundaries. . 아래 코드 셀을 실행하여 전역 플레이트 경계를 표시하는 DataFrame plate_boundaries를 만듭니다. &quot;좌표&quot; 열은 경계를 따라 (위도, 경도) 위치의 목록입니다. . plate_boundaries = gpd.read_file(&quot;../input/geospatial-learn-course-data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp&quot;) plate_boundaries[&#39;coordinates&#39;] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis=&#39;columns&#39;) plate_boundaries.drop(&#39;geometry&#39;, axis=1, inplace=True) plate_boundaries.head() . Next, run the code cell below without changes to load the historical earthquake data into a DataFrame earthquakes. . earthquakes = pd.read_csv(&quot;../input/geospatial-learn-course-data/earthquakes1970-2014.csv&quot;, parse_dates=[&quot;DateTime&quot;]) earthquakes.head() . The code cell below visualizes the plate boundaries on a map. Use all of the earthquake data to add a heatmap to the same map, to determine whether earthquakes coincide with plate boundaries. . 아래 코드 셀은 지도에서 판 경계를 시각화합니다. 모든 지진 데이터를 사용하여 동일한 지도에 히트맵을 추가하여 지진이 판 경계와 일치하는지 확인합니다. . m_1 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) for i in range(len(plate_boundaries)): folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color=&#39;black&#39;).add_to(m_1) # Your code here: Add a heatmap to the map HeatMap(data=earthquakes[[&#39;Latitude&#39;, &#39;Longitude&#39;]], radius=10).add_to(m_1) # Uncomment to see a hint #q_1.a.hint() # Show the map embed_map(m_1, &#39;q_1.html&#39;) . q_1.a.check() # Uncomment to see our solution (your code may look different!) #q_1.a.solution() . So, given the map above, do earthquakes coincide with plate boundaries? . q_1.b.solution() . 2) Is there a relationship between earthquake depth and proximity to a plate boundary in Japan? . You recently read that the depth of earthquakes tells us important information about the structure of the earth. You&#39;re interested to see if there are any intereresting global patterns, and you&#39;d also like to understand how depth varies in Japan. . 최근에 지진의 깊이가 중요 정보를 알려준다는 내용을 읽었습니다. -news_science_products) 지구의 구조에 대해. 흥미로운 글로벌 패턴이 있는지 확인하고 싶고 일본의 깊이가 어떻게 다른지 이해하고 싶습니다. . m_2 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) for i in range(len(plate_boundaries)): folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color=&#39;black&#39;).add_to(m_2) # Your code here: Add a map to visualize earthquake depth def color_producer(val): if val &lt; 50: return &#39;forestgreen&#39; else: return &#39;darkred&#39; for i in range(0,len(earthquakes)): folium.Circle( location=[earthquakes.iloc[i][&#39;Latitude&#39;], earthquakes.iloc[i][&#39;Longitude&#39;]], radius=2000, color=color_producer(earthquakes.iloc[i][&#39;Depth&#39;])).add_to(m_2) # Uncomment to see a hint q_2.a.hint() # View the map embed_map(m_2, &#39;q_2.html&#39;) . q_2.a.check() # Uncomment to see our solution (your code may look different!) #q_2.a.solution() . Can you detect a relationship between proximity to a plate boundary and earthquake depth? Does this pattern hold globally? In Japan? . q_2.b.solution() . 3) Which prefectures have high population density? . Run the next code cell (without changes) to create a GeoDataFrame prefectures that contains the geographical boundaries of Japanese prefectures. . 다음 코드 셀을 변경 없이 실행하여 일본 현의 지리적 경계를 포함하는 GeoDataFrame &#39;현&#39;을 만듭니다. . prefectures = gpd.read_file(&quot;../input/geospatial-learn-course-data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp&quot;) prefectures.set_index(&#39;prefecture&#39;, inplace=True) prefectures.head() . The next code cell creates a DataFrame stats containing the population, area (in square kilometers), and population density (per square kilometer) for each Japanese prefecture. Run the code cell without changes. . 다음 코드 셀은 각 일본 현에 대한 인구, 면적(제곱 킬로미터 단위) 및 인구 밀도(제곱 킬로미터당)를 포함하는 DataFrame &#39;통계&#39;를 생성합니다. 변경 없이 코드 셀을 실행합니다. . population = pd.read_csv(&quot;../input/geospatial-learn-course-data/japan-prefecture-population.csv&quot;) population.set_index(&#39;prefecture&#39;, inplace=True) # Calculate area (in square kilometers) of each prefecture area_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name=&#39;area_sqkm&#39;) stats = population.join(area_sqkm) # Add density (per square kilometer) of each prefecture stats[&#39;density&#39;] = stats[&quot;population&quot;] / stats[&quot;area_sqkm&quot;] stats.head() . Use the next code cell to create a choropleth map to visualize population density. . 다음 코드 셀을 사용하여 등치 지도를 만들어 인구 밀도를 시각화합니다. . m_3 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) # Your code here: create a choropleth map to visualize population density Choropleth(geo_data=prefectures[&#39;geometry&#39;].__geo_interface__, data=stats[&#39;density&#39;], key_on=&quot;feature.id&quot;, fill_color=&#39;YlGnBu&#39;, legend_name=&#39;population density&#39; ).add_to(m_3) # Uncomment to see a hint #q_3.a.hint() # View the map embed_map(m_3, &#39;q_3.html&#39;) . q_3.a.check() # Uncomment to see our solution (your code may look different!) #q_3.a.solution() . Which three prefectures have relatively higher density than the others? Are they spread throughout the country, or all located in roughly the same geographical region? (If you&#39;re unfamiliar with Japanese geography, you might find this map useful to answer the questions.) . q_3.b.solution() . 4) Which high-density prefecture is prone to high-magnitude earthquakes? . Create a map to suggest one prefecture that might benefit from earthquake reinforcement. Your map should visualize both density and earthquake magnitude. . 지진 보강의 혜택을 받을 수 있는 한 현을 제안하는 지도를 만듭니다. 지도는 밀도와 지진 규모를 모두 시각화해야 합니다. . m_4 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) # Your code here: create a map def color_producer(magnitude): if magnitude &gt; 6.5: return &#39;red&#39; else: return &#39;green&#39; Choropleth( geo_data=prefectures[&#39;geometry&#39;].__geo_interface__, data=stats[&#39;density&#39;], key_on=&quot;feature.id&quot;, fill_color=&#39;BuPu&#39;, legend_name=&#39;Population density (per square kilometer)&#39;).add_to(m_4) for i in range(0,len(earthquakes)): folium.Circle( location=[earthquakes.iloc[i][&#39;Latitude&#39;], earthquakes.iloc[i][&#39;Longitude&#39;]], popup=(&quot;{} ({})&quot;).format( earthquakes.iloc[i][&#39;Magnitude&#39;], earthquakes.iloc[i][&#39;DateTime&#39;].year), radius=earthquakes.iloc[i][&#39;Magnitude&#39;]**5.5, color=color_producer(earthquakes.iloc[i][&#39;Magnitude&#39;])).add_to(m_4) # Uncomment to see a hint #q_4.a.hint() # View the map embed_map(m_4, &#39;q_4.html&#39;) . q_4.a.check() # Uncomment to see our solution (your code may look different!) #q_4.a.solution() . Which prefecture do you recommend for extra earthquake reinforcement? . q_4.b.solution() . Keep going . Learn how to convert names of places to geographic coordinates with geocoding. You&#39;ll also explore special ways to join information from multiple GeoDataFrames. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/05/24/exercise-interactive-maps.html",
            "relUrl": "/jupyter/python/2022/05/24/exercise-interactive-maps.html",
            "date": " • May 24, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Coordinate Refrence Systems",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . import pandas as pd import geopandas as gpd from shapely.geometry import LineString from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex2 import * . Exercises . 1) Load the data. . Run the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df. . 다음 코드 셀(변경 없이)을 실행하여 GPS 데이터를 pandas DataFrame birds_df에 로드합니다. . birds_df = pd.read_csv(&quot;../input/geospatial-learn-course-data/purple_martin.csv&quot;, parse_dates=[&#39;timestamp&#39;]) print(&quot;There are {} different birds in the dataset.&quot;.format(birds_df[&quot;tag-local-identifier&quot;].nunique())) birds_df.head() . There are 11 birds in the dataset, where each bird is identified by a unique value in the &quot;tag-local-identifier&quot; column. Each bird has several measurements, collected at different times of the year. . Use the next code cell to create a GeoDataFrame birds. . birds should have all of the columns from birds_df, along with a &quot;geometry&quot; column that contains Point objects with (longitude, latitude) locations. -birds에는 birds_df의 모든 열과 함께 (경도, 위도) 위치가 있는 Point 개체가 포함된 &quot;geometry&quot; 열이 있어야 합니다. | Set the CRS of birds to {&#39;init&#39;: &#39;epsg:4326&#39;}. | . birds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[&quot;location-long&quot;], birds_df[&quot;location-lat&quot;])) # Your code here: Set the CRS to {&#39;init&#39;: &#39;epsg:4326&#39;} birds.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # Check your answer q_1.check() . #q_1.hint() #q_1.solution() . 2) Plot the data. . Next, we load in the &#39;naturalearth_lowres&#39; dataset from GeoPandas, and set americas to a GeoDataFrame containing the boundaries of all countries in the Americas (both North and South America). Run the next code cell without changes. . 다음으로 GeoPandas에서 naturalearth_lowres 데이터 세트를 로드하고 americas를 미주(북미와 남미 모두)의 모든 국가 경계를 포함하는 GeoDataFrame으로 설정합니다. 변경 없이 다음 코드 셀을 실행합니다. . world = gpd.read_file(gpd.datasets.get_path(&#39;naturalearth_lowres&#39;)) americas = world.loc[world[&#39;continent&#39;].isin([&#39;North America&#39;, &#39;South America&#39;])] americas.head() . Use the next code cell to create a single plot that shows both: (1) the country boundaries in the americas GeoDataFrame, and (2) all of the points in the birds_gdf GeoDataFrame. . 다음 코드 셀을 사용하여 (1) americas GeoDataFrame의 국가 경계와 (2) birds_gdf GeoDataFrame의 모든 점을 모두 표시하는 단일 플롯을 만듭니다. . Don&#39;t worry about any special styling here; just create a preliminary plot, as a quick sanity check that all of the data was loaded properly. In particular, you don&#39;t have to worry about color-coding the points to differentiate between birds, and you don&#39;t have to differentiate starting points from ending points. We&#39;ll do that in the next part of the exercise. . ax = americas.plot(figsize=(8,8), color=&#39;red&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) americas.plot(markersize=1, ax=ax) # Uncomment to see a hint #q_2.hint() . q_2.check() # Uncomment to see our solution (your code may look different!) #q_2.solution() . 3) Where does each bird start and end its journey? (Part 1) . Now, we&#39;re ready to look more closely at each bird&#39;s path. Run the next code cell to create two GeoDataFrames: . path_gdf contains LineString objects that show the path of each bird. It uses the LineString() method to create a LineString object from a list of Point objects. | path_gdf에는 각 새의 경로를 표시하는 LineString 개체가 포함되어 있습니다. LineString() 메서드를 사용하여 Point 개체 목록에서 LineString 개체를 만듭니다. | start_gdf contains the starting points for each bird. | start_gdf는 각 새의 시작 지점을 포함합니다. | . path_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: LineString(x)).reset_index() path_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry) path_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # GeoDataFrame showing starting point for each bird start_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[0]).reset_index() start_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry) start_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # Show first five rows of GeoDataFrame start_gdf.head() . Use the next code cell to create a GeoDataFrame end_gdf containing the final location of each bird. . The format should be identical to that of start_gdf, with two columns (&quot;tag-local-identifier&quot; and &quot;geometry&quot;), where the &quot;geometry&quot; column contains Point objects. | 형식은 두 개의 열(&quot;tag-local-identifier&quot; 및 &quot;geometry&quot;)이 있는 `start_gdf&#39;의 형식과 동일해야 합니다. 여기서 &quot;geometry&quot; 열은 Point 개체를 포함합니다. | Set the CRS of end_gdf to {&#39;init&#39;: &#39;epsg:4326&#39;}. | end_gdf의 CRS를 {&#39;init&#39;: &#39;epsg:4326&#39;}으로 설정합니다. | . end_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[-1]).reset_index() end_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry) end_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # Check your answer q_3.check() . #q_3.hint() #q_3.solution() . 4) Where does each bird start and end its journey? (Part 2) . Use the GeoDataFrames from the question above (path_gdf, start_gdf, and end_gdf) to visualize the paths of all birds on a single map. You may also want to use the americas GeoDataFrame. . 위 질문의 GeoDataFrames(path_gdf, start_gdf, end_gdf)를 사용하여 단일 지도에서 모든 새의 경로를 시각화하세요. americas GeoDataFrame을 사용할 수도 있습니다. . ax = americas.plot(figsize=(10,10), color=&#39;none&#39;, edgecolor=&#39;gainsboro&#39;, zorder=3) # Add wild lands, campsites, and foot trails to the base map start_gdf.plot(color=&#39;lightgreen&#39;, ax=ax) path_gdf.plot(color=&#39;maroon&#39;, markersize=2, ax=ax) end_gdf.plot(color=&#39;black&#39;, markersize=1, ax=ax) # Uncomment to see a hint #q_4.hint() . q_4.check() # Uncomment to see our solution (your code may look different!) #q_4.solution() . 5) Where are the protected areas in South America? (Part 1) . It looks like all of the birds end up somewhere in South America. But are they going to protected areas? . In the next code cell, you&#39;ll create a GeoDataFrame protected_areas containing the locations of all of the protected areas in South America. The corresponding shapefile is located at filepath protected_filepath. . 다음 코드 셀에서는 남미의 모든 보호 지역 위치를 포함하는 GeoDataFrame protected_areas를 생성합니다. 해당 shapefile은 파일 경로 protected_filepath에 있습니다. . protected_filepath = &quot;../input/geospatial-learn-course-data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp&quot; # Your code here protected_areas = gpd.read_file(protected_filepath) # Check your answer q_5.check() . #q_5.hint() #q_5.solution() . 6) Where are the protected areas in South America? (Part 2) . Create a plot that uses the protected_areas GeoDataFrame to show the locations of the protected areas in South America. (You&#39;ll notice that some protected areas are on land, while others are in marine waters.) . &#39;protected_areas&#39; GeoDataFrame을 사용하여 남아메리카의 보호 지역 위치를 표시하는 플롯을 만듭니다. (일부 보호 구역은 육지에 있고 다른 보호 구역은 바다에 있음을 알 수 있습니다.) . south_america = americas.loc[americas[&#39;continent&#39;]==&#39;South America&#39;] # Your code here: plot protected areas in South America ax = south_america.plot(figsize=(8,8), color=&#39;whitesmoke&#39;, edgecolor=&#39;black&#39;) protected_areas.plot(markersize=1, ax=ax,alpha=0.4) # Uncomment to see a hint #q_6.hint() . q_6.check() # Uncomment to see our solution (your code may look different!) #q_6.solution() . 7) What percentage of South America is protected? . You&#39;re interested in determining what percentage of South America is protected, so that you know how much of South America is suitable for the birds. . As a first step, you calculate the total area of all protected lands in South America (not including marine area). To do this, you use the &quot;REP_AREA&quot; and &quot;REP_M_AREA&quot; columns, which contain the total area and total marine area, respectively, in square kilometers. . Run the code cell below without changes. . P_Area = sum(protected_areas[&#39;REP_AREA&#39;]-protected_areas[&#39;REP_M_AREA&#39;]) print(&quot;South America has {} square kilometers of protected areas.&quot;.format(P_Area)) . Then, to finish the calculation, you&#39;ll use the south_america GeoDataFrame. . south_america.head() . Calculate the total area of South America by following these steps: . Calculate the area of each country using the area attribute of each polygon (with EPSG 3035 as the CRS), and add up the results. The calculated area will be in units of square meters. | 각 폴리곤(CRS로 EPSG 3035 사용)의 &#39;area&#39; 속성을 사용하여 각 국가의 면적을 계산하고 결과를 합산합니다. 계산된 면적은 평방 미터 단위입니다. | Convert your answer to have units of square kilometeters. | 평방 킬로미터 단위가 되도록 답을 변환하십시오. | . totalArea = sum(south_america.geometry.to_crs(epsg=3035).area)/10**6 totalArea # Check your answer q_7.check() . #q_7.hint() #q_7.solution() . Run the code cell below to calculate the percentage of South America that is protected. . percentage_protected = P_Area/totalArea print(&#39;Approximately {}% of South America is protected.&#39;.format(round(percentage_protected*100, 2))) . 8) Where are the birds in South America? . So, are the birds in protected areas? . 그렇다면 새들은 보호 구역에 있습니까? . Create a plot that shows for all birds, all of the locations where they were discovered in South America. Also plot the locations of all protected areas in South America. . 모든 새, 남미에서 발견된 모든 위치를 보여주는 플롯을 만듭니다. 또한 남아메리카의 모든 보호 지역의 위치를 ​​표시합니다. . To exclude protected areas that are purely marine areas (with no land component), you can use the &quot;MARINE&quot; column (and plot only the rows in protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;], instead of every row in the protected_areas GeoDataFrame). . 순수한 해양 지역(토지 구성요소 없음)인 보호 지역을 제외하려면 &quot;MARINE&quot; 열을 사용할 수 있습니다(그리고 protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;] protected_areas GeoDataFrame의 모든 행). . ax = south_america.plot(figsize=(8,8), color=&#39;whitesmoke&#39;, edgecolor=&#39;black&#39;) protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;].plot(ax=ax, alpha=0.4, zorder=1) birds[birds.geometry.y &lt; 0].plot(ax=ax, color=&#39;red&#39;, alpha=0.6, markersize=10, zorder=2) # Uncomment to see a hint #q_8.hint() . q_8.check() # Uncomment to see our solution (your code may look different!) #q_8.solution() . Keep going . Create stunning interactive maps with your geospatial data. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/05/24/exercise-coordinate-reference-systems.html",
            "relUrl": "/jupyter/python/2022/05/24/exercise-coordinate-reference-systems.html",
            "date": " • May 24, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Visualization_with_seaborn_and_matplotlib",
            "content": "코드 출처 - 이제현님 블로그 . https://jehyunlee.github.io/2020/09/30/Python-DS-34-seaborn_matplotlib/ | . 원문: by Aurélien Geron (Link) | Translated by Chansung PARK (Link) | Object Oriented API Addition by Jehyun LEE (Link) | . &#52376;&#51020;&#51004;&#47196; &#44536;&#47140;&#48372;&#45716; &#44536;&#47000;&#54532; . 데이터 몇 개로 plot 함수를 호출한 다음, show 함수를 호출해주면 간단히 그래프를 그려볼 수 있습니다! . plot 함수에 단일 배열의 데이터가 주어진다면, 수직 축의 좌표로서 이를 사용하게 되며, 각 데이터의 배열상 색인(인덱스)을 수평 좌표로서 사용합니다. 두 개의 배열을 넣어줄 수도 있습니다: 그러면, 하나는 x 축에 대한것이며, 다른 하나는 y 축에 대한것이 됩니다 . import matplotlib %matplotlib inline import matplotlib.pyplot as plt . plt.plot([1, 3, 4, 9, 1, 3]) plt.show() #객체지향이 아닌 기본적인 pyplot의 ploting 방법 . 같은 그림을 object oriented API를 이용해 그려보겠습니다. . object oriented API는 그래프의 각 부분을 객체로 지정하고 그리는 것으로, 다음과 같은 패턴을 가지고 있습니다. . # 1. 도화지(Figure: fig)를 깔고 그래프를 그릴 구역(Axes: ax)을 정의합니다. fig, ax = plt.subplots() # 2. ax 위에 그래프를 그립니다. ax.plot([1, 3, 4, 9, 1, 3]) # 3. 그래프를 화면에 출력합니다. plt.show() . fig #이처럼 객체 지향은 fig라는 곳에 플롯을 저장해서 진행하는 스타일 . 이번에는 수학적인 함수를 그려보겠습니다. NumPy의 linespace 함수를 사용하여 -2 ~ 2 범위에 속하는 500개의 부동소수로 구성된 x 배열을 생성합니다. 그 다음 x의 각 값의 거듭제곱된 값을 포함하는 y 배열을 생성합니다 . 그래프가 약간은 삭막해 보입니다. 타이틀과 x 및 y축에 대한 라벨, 그리고 모눈자를 추가적으로 그려보겠습니다. . import numpy as np x = np.linspace(-2, 2, 500) y = x**2 plt.plot(x, y) plt.title(&quot;Square function&quot;) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y = x**2&quot;) plt.grid(True) plt.show() . object-oriented API는 축 이름과 같은 설정 명령어가 pyplot과 다소 다릅니다. | 대체로 축 이름(label), 범위(limits) 등을 지정하는 명령어는 set_대상(), 거꾸로 그래프에서 설정값을 가져오는 명령어는 get_대상()으로 통일되어 있습니다. | . fig, ax = plt.subplots() ax.plot(x, y) ax.set_title(&quot;Square function&quot;) ax.set_xlabel(&quot;x&quot;) ax.set_ylabel(&quot;y = x**2&quot;) ax.grid(True) plt.show() #pyplot과 유사한 방식이지만 set이 추가되어 있는 모습이다 . &#49440;&#51032; &#49828;&#53440;&#51068;&#44284; &#49353;&#49345; . 기본적으로 matplotlib은 바로 다음에 위치한(연이은) 데이터 사이에 선을 그립니다. . 세 번째 파라미터를 지정하면 선의 스타일과 색상을 바꿀 수 있습니다. 예를 들어서 &quot;g--&quot;는 &quot;초록색 파선&quot;을 의미합니다. . fig, ax = plt.subplots() ax.plot([0, 100, 100, 0, 0, 100, 50, 0, 100], [0, 0, 100, 100, 0, 100, 130, 100, 0]) ax.set_xlim(-10, 110) ax.set_ylim(-10, 140) # 그래프의 범위는 pyplot과 같이 ax.axis([-10, 110, -10, 140]) 으로 지정할 수 있습니다. # 하지만 위와 같이 set_xlim, set_ylim을 사용해서 명시하는 것이 더 체계적으로 느껴집니다. plt.show() . fig, ax = plt.subplots() ax.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], &quot;r-&quot;) #선의 색생과 선의 종류를 지정 ax.plot([0, 100, 50, 0, 100], [0, 100, 130, 100, 0], &quot;g--&quot;) ax.set_xlim(-10, 110) ax.set_ylim(-10, 140) plt.show() . 선 대신에 간단한 점을 그려보는 것도 가능합니다. 아래는 초록색 파선, 빨강 점선, 파랑 삼각형의 예를 보여줍니다. . fig, ax = plt.subplots() x = np.linspace(-1.4, 1.4, 30) ax.plot(x, x, &#39;g--&#39;) ax.plot(x, x**2, &#39;r:&#39;) ax.plot(x, x**3, &#39;b^&#39;) #플롯을 하나씩 나누어서 그려주어 합쳐서 한줄에 쓰는 것보다 더 직관적이다 plt.show() . x = np.linspace(-1.4, 1.4, 30) fig, ax = plt.subplots() line1 = ax.plot(x, x, &#39;g--&#39;, linewidth=3, dash_capstyle=&#39;round&#39;) #linewidth로 굵기 dash_capstyle로 대쉬의 모형 선택 line2 = ax.plot(x, x**2, &#39;r:&#39;) line3 = ax.plot(x, x**3, &#39;b^&#39;, alpha=0.2) #alpha로 투명도 조절 plt.show() . &#48512;&#48516; &#44536;&#47000;&#54532; (subplot) . matplotlib는 하나의 그림(figure)에 여러개의 부분 그래프를 포함할 수 있습니다. 이 부분 그래프는 격자 형식으로 관리됩니다. subplot 함수를 호출하여 부분 그래프를 생성할 수 있습니다. 이 때 격자의 행/열의 수 및 그래프를 그리고자 하는 부분 그래프의 색인을 파라미터로서 지정해줄 수 있습니다 (색인은 1부터 시작하며, 좌-&gt;우, 상단-&gt;하단의 방향입니다). . x = np.linspace(-1.4, 1.4, 30) fig, ax = plt.subplots(2, 2) # 행과 열의 개수를 지정 ax[0, 0].plot(x, x) ax[0, 1].plot(x, x**2) ax[1, 0].plot(x, x**3) ax[1, 1].plot(x, x**4) # ax[row, col] plt.show() . grid = plt.GridSpec(2, 2) # 2행 2열 크기의 격자를 준비합니다. ax1 = plt.subplot(grid[0, 0]) ax2 = plt.subplot(grid[0, 1]) ax3 = plt.subplot(grid[1, 0:]) # 2행의 전체열을 의미한다 ax1.plot(x, x) ax2.plot(x, x**2) ax3.plot(x, x**3) plt.show() . 보다 복잡한 부분 그래프의 위치 선정이 필요하다면, subplot2grid를 대신 사용할 수 있습니다. 격자의 행과 열의 번호 및 격자에서 해당 부분 그래프를 그릴 위치를 지정해줄 수 있습니다 (좌측상단 = (0,0). 또한 몇 개의 행/열로 확장되어야 하는지도 추가적으로 지정할 수 있습니다. . gridsize = (3, 3) # 3행 3열 크기의 격자를 준비합니다. ax1 = plt.subplot2grid(gridsize, (0,0), rowspan=2, colspan=2) #행과 열 방향으로 각각 2씩 늘린다 ax2 = plt.subplot2grid(gridsize, (0,2)) ax3 = plt.subplot2grid(gridsize, (1,2), rowspan=2) #행 방향으로 2만큼 늘린다 ax4 = plt.subplot2grid(gridsize, (2,0), colspan=2) #열 방향으로 2만큼 늘린다 ax1.plot(x, x**2) ax2.plot(x, x**3) ax3.plot(x, x**4) ax4.plot(x, x**5) plt.show() . &#50668;&#47084;&#44060;&#51032; &#44536;&#47548; (figure) . 여러개의 그림을 그리는것도 가능합니다. 각 그림은 하나 이상의 부분 그래프를 가질 수 있습니다. 기본적으로는 matplotlib이 자동으로 figure(1)을 생성합니다. 그림간 전환을 할 때, pyplot은 현재 활성화된 그림을 계속해서 추적합니다. 또한 활성화된 그림의 활성화된 부분 그래프가 현재 그래프가 그려질 부분 그래프가 됩니다. . x = np.linspace(-1.4, 1.4, 30) fig1, ax1 = plt.subplots(nrows=2, ncols=1) #2행 1열의 격자를 생성 ax1[0].plot(x, x**2) ax1[0].set_title(&quot;Square and Cube&quot;) ax1[1].plot(x, x**3) fig2, ax2 = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) #격자의 크기를 지정 ax2[0].plot(x, x**4) ax2[0].set_title(&quot;y = x**4&quot;) ax2[1].plot(x, x**5) ax2[1].set_title(&quot;y = x**5&quot;) ax1[1].plot(x, -x**3, &quot;r:&quot;) # fig1의 두번째 그림에 추가되어 표현이 된다 plt.show() . &#53581;&#49828;&#53944; &#44536;&#47532;&#44592; . text 함수를 호출하여 텍스트를 그래프의 원하는 위치에 추가할 수 있습니다. 출력을 원하는 텍스트와 수평 및 수직 좌표를 지정하고, 추가적으로 몇 가지 속성을 지정해 주기만 하면 됩니다. matplotlib의 모든 텍스트는 TeX 방정식 표현을 포함할 수 있습니다. . x = np.linspace(-1.5, 1.5, 30) #정해진 범위에 따라 숫자를 n개 만든다 px = 0.8 py = px**2 # fig, ax = plt.subplots() ax.plot(x, x**2, &quot;b-&quot;) ax.plot(px, py, &quot;ro&quot;) #red색상과 o모양으로 설정 ax.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;blue&#39;, horizontalalignment=&quot;center&quot;) #위치와 글자크기등을 설정해줌 ax.text(px - 0.08, py, &quot;Beautiful point&quot;, ha=&quot;right&quot;, weight=&quot;heavy&quot;) ax.text(px, py, &quot;x = %0.2f ny = %0.2f&quot;%(px, py), rotation=50, color=&#39;gray&#39;) #rotation으로 기울기도 설정 plt.show() . &#48276;&#47168; (Legends) . 범례를 추가하는 가장 간단한 방법은 모든 선에 라벨을 설정 해 주고, legend 함수를 호출하는 것입니다. . x = np.linspace(-1.4, 1.4, 50) fig, ax = plt.subplots() ax.plot(x, x**2, &quot;r--&quot;, label=&quot;Square function&quot;) #label을 이용해서 범례를 설정이 가능하다 ax.plot(x, x**3, &quot;g-&quot;, label=&quot;Cube function&quot;) ax.legend(loc=&quot;best&quot;) ax.grid(True) plt.show() . &#54001;&#44284; &#54001;&#52964; (Ticks and tickers) . 각 축에는 &quot;틱(ticks)&quot;이라는 작은 표시가 있습니다. 정확히 말하자면, &quot;틱&quot;은 표시(예. (-1, 0, 1))의 위치&quot;이며, 틱 선은 그 위치에 그려지는 작은 선입니다. 또한 &quot;틱 라벨&quot;은 틱 선 옆에 그려지는 라벨이며, &quot;틱커&quot;는 틱의 위치를 결정하는 객체 입니다. 기본적인 틱커는 ~5 에서 8 틱을 위치시키는데 꽤 잘 작동합니다. 즉, 틱 서로간에 적당한 거리를 표현합니다. . 하지만, 가끔은 좀 더 이를 제어할 필요가 있습니다 (예. 위의 로짓 그래프에서는 너무 많은 틱 라벨이 있습니다). 다행히도 matplotlib은 틱을 완전히 제어하는 방법을 제공합니다. 심지어 보조 눈금(minor tick)을 활성화 할 수도 있습니다. . x = np.linspace(-2, 2, 100) fig, ax = plt.subplots(ncols=3, figsize=(15, 10)) #3열로 플롯을 그릴수 있도록 ax[0].plot(x, x**3) ax[0].grid(True) ax[0].set_title(&quot;Default ticks&quot;) ax[1].plot(x, x**3) ax[1].grid(True) ax[1].set_xticks(np.arange(-2, 2, 1)) #xticks를 -2~1까지 1씩 증가하도록 ax[1].set_title(&quot;Manual ticks on the x-axis&quot;) ax[2].plot(x, x**3) ax[2].grid(True) ax[2].minorticks_on() ax[2].set_xticks([-2, 0, 1, 2], minor=False) #xticks를 지정 ax[2].set_yticks(np.arange(-5, 5, 1)) #yticks를 -5~4까지 1씩 증가하도록 ax[2].set_yticklabels([&quot;min&quot;, -4, -3, -2, -1, 0, 1, 2, 3, &quot;max&quot;]) #label를 지정해서 표현도 가능 ax[2].set_title(&quot;Manual ticks and tick labels n(plus minor ticks) on the y-axis&quot;) plt.show() . &#49328;&#51216;&#46020;(Scatter plot) . 단순히 각 점에 대한 x 및 y 좌표를 제공하면 산점도를 그릴 수 있습니다. . 부수적으로 각 점의 크기를 정할 수도 있습니다. . 마찬가지로 여러 속성을 설정할 수 있습니다. 가령 테두리 및 모양의 내부 색상, 그리고 투명도와 같은것의 설정이 가능합니다. . from numpy.random import rand x, y = rand(2, 100) #랜덤한 값들을 뽑아줌 fig, ax = plt.subplots() ax.scatter(x, y) #scatter함수로 스캐터 플롯으로 그릴수 있도록 해줌 plt.show() . fig, ax = plt.subplots() for color in [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]: n = 100 x, y = rand(2, n) scale = 500.0 * rand(n) ** 5 #포인트들의 크기를 조절 ax.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors=&#39;blue&#39;) #지정해준 컬러와 투명도 테두리의 색상을 지정이 가능 ax.grid(True) plt.show() . from numpy.random import randn # Axis를 인자로 전달하여 함수 연산과 시각화를 수행합니다. def plot_line(axis, slope, intercept, **kargs): xmin, xmax = axis.get_xlim() axis.plot([xmin, xmax], [xmin*slope+intercept, xmax*slope+intercept], **kargs) x = randn(1000) y = 0.5*x + 5 + randn(1000)*2 fig, ax = plt.subplots() ax.set_xlim(-2.5, 2.5) #x축의 범위지정 ax.set_ylim(-5, 15) #y축의 범위지정 ax.scatter(x, y, alpha=0.2) #alpha=0.2로 투명도 설정 ax.plot(1, 0, &quot;ro&quot;) #포인트를 red색상 o모양으로 표시 ax.vlines(1, -5, 0, color=&quot;red&quot;) #세로 라인 ax.hlines(0, -2.5, 1, color=&quot;red&quot;) #가로 라인을 표시 plot_line(axis=ax, slope=0.5, intercept=5, color=&quot;magenta&quot;) #기울기 0.5이고 beta_0가 5인 직선 ax.grid(True) plt.show() . &#55176;&#49828;&#53664;&#44536;&#47016; . data = [1, 1.1, 1.8, 2, 2.1, 3.2, 3, 3, 3, 3] fig, ax = plt.subplots(2, 1) ax[0].hist(data, bins = 5, rwidth=0.8) #bin - x축에서 막대그래프의 폭을 설정 ax[1].hist(data, bins = [1, 1.5, 2, 2.5, 3], rwidth=0.95) #rwidth로 row의 넓이를 설정 ax[1].set_xlabel(&quot;Value&quot;) ax[1].set_ylabel(&quot;Frequency&quot;) plt.show() . data1 = np.random.randn(400) data2 = np.random.randn(500) + 3 # x축에서 +3만큼 원점을 이동해서 플롯팅 data3 = np.random.randn(450) + 6 data4a = np.random.randn(200) + 9 data4b = np.random.randn(100) + 10 fig, ax = plt.subplots() ax.hist(data1, bins=5, color=&#39;g&#39;, alpha=0.75, label=&#39;bar hist&#39;) # 기본적인 histtype=&#39;bar&#39; ax.hist(data2, color=&#39;b&#39;, alpha=0.65, histtype=&#39;stepfilled&#39;, label=&#39;stepfilled hist&#39;) ax.hist(data3, color=&#39;r&#39;, histtype=&#39;step&#39;, label=&#39;step hist&#39;) ax.hist((data4a, data4b), color=(&#39;r&#39;,&#39;m&#39;), alpha=0.55, histtype=&#39;barstacked&#39;, label=(&#39;barstacked a&#39;, &#39;barstacked b&#39;)) ax.set_xlabel(&quot;Value&quot;) # x축의 이름 지정 ax.set_ylabel(&quot;Frequency&quot;) ax.legend() ax.grid(True) # 한번에 여러개의 플롯을 그리는 경우 plt.show() . Visualization with seaborn . seaborn은 python의 시각화 라이브러리인 matplolib를 기반으로 제작된 라이브러리입니다. . import seaborn as sns sns.set() sns.set(style=&quot;darkgrid&quot;) import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;) plt.rcParams[&#39;figure.figsize&#39;]=(5,5) #figure의 사이즈를 설정이 가능하다 . Loding dataset . data_BM = pd.read_csv(&#39;bigmart_data.csv&#39;) data_BM = data_BM.dropna(how=&quot;any&quot;) #NA값 제거 data_BM[&quot;Visibility_Scaled&quot;] = data_BM[&quot;Item_Visibility&quot;] * 100 #Visibility_Scaled 컬럼의 값들에 100 곱해줌 data_BM.head() . Item_Identifier Item_Weight Item_Fat_Content Item_Visibility Item_Type Item_MRP Outlet_Identifier Outlet_Establishment_Year Outlet_Size Outlet_Location_Type Outlet_Type Item_Outlet_Sales Visibility_Scaled . 0 FDA15 | 9.300 | Low Fat | 0.016047 | Dairy | 249.8092 | OUT049 | 1999 | Medium | Tier 1 | Supermarket Type1 | 3735.1380 | 1.604730 | . 1 DRC01 | 5.920 | Regular | 0.019278 | Soft Drinks | 48.2692 | OUT018 | 2009 | Medium | Tier 3 | Supermarket Type2 | 443.4228 | 1.927822 | . 2 FDN15 | 17.500 | Low Fat | 0.016760 | Meat | 141.6180 | OUT049 | 1999 | Medium | Tier 1 | Supermarket Type1 | 2097.2700 | 1.676007 | . 4 NCD19 | 8.930 | Low Fat | 0.000000 | Household | 53.8614 | OUT013 | 1987 | High | Tier 3 | Supermarket Type1 | 994.7052 | 0.000000 | . 5 FDP36 | 10.395 | Regular | 0.000000 | Baking Goods | 51.4008 | OUT018 | 2009 | Medium | Tier 3 | Supermarket Type2 | 556.6088 | 0.000000 | . data_BM.describe() #이상치가 있는지 확인 . Item_Weight Item_Visibility Item_MRP Outlet_Establishment_Year Item_Outlet_Sales Visibility_Scaled . count 4650.000000 | 4650.000000 | 4650.000000 | 4650.000000 | 4650.000000 | 4650.000000 | . mean 12.898675 | 0.060700 | 141.716328 | 1999.190538 | 2272.037489 | 6.070048 | . std 4.670973 | 0.044607 | 62.420534 | 7.388800 | 1497.964740 | 4.460652 | . min 4.555000 | 0.000000 | 31.490000 | 1987.000000 | 69.243200 | 0.000000 | . 25% 8.770000 | 0.025968 | 94.409400 | 1997.000000 | 1125.202000 | 2.596789 | . 50% 12.650000 | 0.049655 | 142.979900 | 1999.000000 | 1939.808300 | 4.965549 | . 75% 17.000000 | 0.088736 | 186.614150 | 2004.000000 | 3111.616300 | 8.873565 | . max 21.350000 | 0.188323 | 266.888400 | 2009.000000 | 10256.649000 | 18.832266 | . 1. Creating basic plots . matplotlib에서 여러 줄이 필요한 한 줄로 seaborn에서 몇 가지 기본 플롯을 만드는 방법을 살펴보겠습니다. . Line Chart . 일부 데이터 세트의 경우 한 변수의 변화를 시간의 함수로 이해하거나 이와 유사한 연속 변수를 이해하고자 할 수 있습니다. . | seaborn에서 이는 lineplot() 함수로 직접 또는 kind=&quot;line&quot;:을 설정하여 relplot()으로 수행할 수 있습니다. . | . sns.lineplot(x=&quot;Item_Weight&quot;, y=&quot;Item_MRP&quot;,data=data_BM[:50]); #처음부터 50번째 까지의 데이터만 사용한다 . Bar Chart . Seaborn에서는 barplot 기능을 사용하여 간단하게 막대 차트를 생성할 수 있습니다. | matplotlib에서 동일한 결과를 얻으려면 데이터 범주를 현명하게 그룹화하기 위해 추가 코드를 작성해야 했습니다. | 그리고 나서 플롯이 올바르게 나오도록 훨씬 더 많은 코드를 작성해야 했습니다. | . sns.barplot(x=&quot;Item_Type&quot;, y=&quot;Item_MRP&quot;, data=data_BM[:5]) . &lt;AxesSubplot:xlabel=&#39;Item_Type&#39;, ylabel=&#39;Item_MRP&#39;&gt; . Histogram . distplot()을 사용하여 seaborn에서 히스토그램을 만들 수 있습니다. 사용할 수 있는 여러 옵션이 있으며 노트북에서 더 자세히 살펴보겠습니다. | . sns.distplot(data_BM[&#39;Item_MRP&#39;]) . &lt;AxesSubplot:xlabel=&#39;Item_MRP&#39;, ylabel=&#39;Density&#39;&gt; . Box plots . Seaborn에서 boxplot을 생성하기 위해 boxplot()을 사용할 수 있습니다. | . sns.boxplot(data_BM[&#39;Item_Outlet_Sales&#39;], orient=&#39;vertical&#39;) . &lt;AxesSubplot:xlabel=&#39;Item_Outlet_Sales&#39;&gt; . Violin plot . 바이올린 플롯은 상자 및 수염 플롯과 유사한 역할을 합니다. | 이는 하나(또는 그 이상) 범주형 변수의 여러 수준에 걸친 정량적 데이터의 분포를 보여줌으로써 해당 분포를 비교할 수 있습니다. | 모든 플롯 구성 요소가 실제 데이터 포인트에 해당하는 상자 플롯과 달리 바이올린 플롯은 기본 분포의 커널 밀도 추정을 특징으로 합니다. | seaborn에서 violinplot()을 사용하여 바이올린 플롯을 만들 수 있습니다. | . sns.violinplot(data_BM[&#39;Item_Outlet_Sales&#39;], orient=&#39;vertical&#39;, color=&#39;skyblue&#39;) . &lt;AxesSubplot:xlabel=&#39;Item_Outlet_Sales&#39;&gt; . Scatter plot . 각 포인트는 데이터 세트의 관찰을 나타내는 포인트 클라우드를 사용하여 두 변수의 분포를 나타냅니다. | 이 묘사를 통해 눈은 그들 사이에 의미 있는 관계가 있는지 여부에 대한 상당한 양의 정보를 추론할 수 있습니다. | relplot()을 kind=scatter 옵션과 함께 사용하여 seaborn에서 산점도를 그릴 수 있습니다. | . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;scatter&quot;); #sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;line&quot;); #kind의 설정으로 표현 설정을 바꿈 . Hue semantic . 세 번째 변수에 따라 점을 색칠하여 플롯에 다른 차원을 추가할 수도 있습니다. Seaborn에서는 이것을 &quot;hue semantic&quot; 사용이라고 합니다. . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, hue=&quot;Item_Type&quot;,data=data_BM[:200]); #hue로 item_type에 대한 플롯을 그려줌 . sns.lineplot(x=&quot;Item_Weight&quot;, y=&quot;Item_MRP&quot;,hue=&#39;Outlet_Size&#39;,data=data_BM[:150]); . Bubble plot . hue semantic 활용하여 Item_Visibility별로 거품을 색칠함과 동시에 개별 거품의 크기로 사용합니다. | . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;scatter&quot;, size=&quot;Visibility_Scaled&quot;, hue=&quot;Visibility_Scaled&quot;); . Category wise sub plot . Seaborn에서 카테고리를 기반으로 플롯을 만들 수도 있습니다. | 각 Outlet_Size에 대한 산점도를 만들었습니다. | . sns.relplot(x=&quot;Item_Weight&quot;, y=&quot;Item_Visibility&quot;, hue=&#39;Outlet_Size&#39;,style=&#39;Outlet_Size&#39;, col=&#39;Outlet_Size&#39;,data=data_BM[:100]); . 2. Advance categorical plots in seaborn . 범주형 변수의 경우 seaborn에 세 가지 다른 패밀리가 있습니다. . catplot()에서 데이터의 기본 표현은 산점도를 사용합니다. . a. Categorical scatterplots . Strip plot . 하나의 변수가 범주형인 산점도를 그립니다. | catplot()에서 kind=strip을 전달하여 생성할 수 있습니다. | . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;, kind=&#39;strip&#39;,data=data_BM[:250]); . Swarm plot . 이 함수는 stripplot()과 유사하지만 점이 겹치지 않도록 조정됩니다(범주형 축을 따라만). | 이렇게 하면 값 분포를 더 잘 표현할 수 있지만 많은 수의 관측치에 대해서는 잘 확장되지 않습니다. 이러한 스타일의 플롯은 때때로 &quot;beeswarm&quot;이라고 불립니다. | catplot()에서 kind=swarm을 전달하여 이를 생성할 수 있습니다. | . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;, kind=&#39;swarm&#39;,data=data_BM[:250]); . b. Categorical distribution plots . Box Plots . 상자 그림은 극단값과 함께 분포의 3사분위수 값을 보여줍니다. | &quot;whiskers&quot;은 하위 및 상위 사분위수의 1.5 IQR 내에 있는 점으로 확장되고 이 범위를 벗어나는 관찰은 독립적으로 표시됩니다. | 즉, 상자 그림의 각 값은 데이터의 실제 관측값에 해당합니다. | . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;box&quot;,data=data_BM); . Violin Plots . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;violin&quot;,data=data_BM); . Point Plot . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;point&quot;,data=data_BM); #y축은 연속형 플랏 x축은 범주형 . Bar plots . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;bar&quot;,data=data_BM); . 3. Density Plots . 히스토그램 대신 Seaborn이 sn.kdeplot으로 수행하는 커널 밀도 추정을 사용하여 분포의 부드러운 추정치를 얻을 수 있습니다.: . plt.figure(figsize=(5,5)) sns.kdeplot(data_BM[&#39;Item_Visibility&#39;], shade=True); . Histogram and Density Plot . distplot을 사용하여 히스토그램과 KDE를 결합할 수 있습니다.: . plt.figure(figsize=(10,10)) sns.distplot(data_BM[&#39;Item_Outlet_Sales&#39;]); . 4. Pair plots . 조인트 플롯을 더 큰 차원의 데이터세트로 일반화하면 쌍 플롯으로 끝납니다. 이것은 모든 값 쌍을 서로에 대해 플롯하려는 경우 다차원 데이터 간의 상관 관계를 탐색하는 데 매우 유용합니다. . | 세 가지 붓꽃 종의 꽃잎과 꽃받침 측정값을 나열하는 잘 알려진 Iris 데이터 세트를 사용하여 이것을 시연할 것입니다. . | . iris = sns.load_dataset(&quot;iris&quot;) sns.pairplot(iris, hue=&#39;species&#39;, height=2.5); #둘다 연속형이면 scatter플랏으로 보여줌 . Seaborn and Matplotlib . 1.1 Load data . 예제로 사용할 펭귄 데이터를 불러옵니다. | seaborn에 내장되어 있습니다. | . import pandas as pd import matplotlib.pyplot as plt import seaborn as sns penguins = sns.load_dataset(&quot;penguins&quot;) penguins.head() . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | Male | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | Female | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | Female | . 3 Adelie | Torgersen | NaN | NaN | NaN | NaN | NaN | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | Female | . 1.2 Figure and Axes . matplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다. | 1 x 2 축공간을 구성합니다. | . fig, axes = plt.subplots(ncols=2, figsize=(8,4)) fig.tight_layout() . 1.3 plot with matplotlib . matplotlib 기능을 이용해서 산점도를 그립니다. . x축은 부리 길이 bill length | y축은 부리 위 아래 두께 bill depth | 색상은 종species로 합니다. | Adelie, Chinstrap, Gentoo이 있습니다. | . | 두 축공간 중 왼쪽에만 그립니다. . | . fig, axes = plt.subplots(ncols=2,figsize=(8,4)) species_u = penguins[&quot;species&quot;].unique() for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plt.show() fig.tight_layout() . 1.4 Plot with seaborn . 단 세 줄로 거의 동일한 그림이 나왔습니다. . scatter plot의 점 크기만 살짝 작습니다. | label의 투명도만 살짝 다릅니다. | . | seaborn 명령 scatterplot()을 그대로 사용했습니다. . | x축과 y축 label도 바꾸었습니다. ax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다. | matplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다. | . | . fig, axes = plt.subplots(ncols=2,figsize=(8,4)) species_u = penguins[&quot;species&quot;].unique() # plot 0 : matplotlib for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plot 1 : seaborn sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1]) axes[1].set_xlabel(&quot;Bill Length (mm)&quot;) axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) fig.tight_layout() . 1.5 matplotlib + seaborn &amp; seaborn + matplotlib . matplotlib과 seaborn이 자유롭게 섞일 수 있습니다. matplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고, | seaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다. | . | . fig, axes = plt.subplots(ncols=2, figsize=(8, 4)) species_u = penguins[&quot;species&quot;].unique() # plot 0 : matplotlib + seaborn for i, s in enumerate(species_u): # matplotlib 산점도 axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3 ) # seaborn 추세선 sns.regplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, data=penguins.loc[penguins[&quot;species&quot;]==s], scatter=False, ax=axes[0]) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plot 1 : seaborn + matplotlib # seaborn 산점도 sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1]) axes[1].set_xlabel(&quot;Bill Length (mm)&quot;) axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) for i, s in enumerate(species_u): # matplotlib 중심점 axes[1].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), c=f&quot;C{i}&quot;, alpha=1, marker=&quot;x&quot;, s=100 ) fig.tight_layout() . 1.6 seaborn + seaborn + matplotlib . seaborn scatterplot + seaborn kdeplot + matplotlib text입니다 | . fig, ax = plt.subplots(figsize=(6,5)) # plot 0: scatter plot sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, color=&quot;k&quot;, data=penguins, alpha=0.3, ax=ax, legend=False) # plot 1: kde plot sns.kdeplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.5, ax=ax, legend=False) # text: species_u = penguins[&quot;species&quot;].unique() for i, s in enumerate(species_u): ax.text(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), s = s, fontdict={&quot;fontsize&quot;:14, &quot;fontweight&quot;:&quot;bold&quot;,&quot;color&quot;:&quot;k&quot;} ) ax.set_xlabel(&quot;Bill Length (mm)&quot;) ax.set_ylabel(&quot;Bill Depth (mm)&quot;) fig.tight_layout() .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/05/07/matplotlib-and-seaborn.html",
            "relUrl": "/jupyter/python/2022/05/07/matplotlib-and-seaborn.html",
            "date": " • May 7, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Python Language Basics",
            "content": "Python language bascis . language semantics . Identation, not braces // &#46308;&#50668;&#50416;&#44592; . for x in array: if x &lt; pivot : less.append(x) else: greater.append(x) . a=5; b=6; c=7 . a=5; b=6; c=7 . c . 7 . Everything is an object // &#50724;&#48652;&#51229;&#53944;&#47484; &#44592;&#48376; &#54624;&#45817;&#54616;&#44256; &#46041;&#51089;&#54632; . Comments . result =[] for line in file_handle: . # keep the empty line for now # if len(line) == 0: # continue result.append(line.replace(&#39;foo&#39;,&#39;bar&#39;)) . print(&quot;Reach this line&quot;) #simple statis report . Function and object method calls . result = f(x,y,z) g() . obj.some_method(x,y,z) result = f(a,b,c,d=5,e=&#39;foo&#39;) . Variables and argument passing . a=[1,2,3] . b=a . a.append(4) #뒤에 붙여줌 b . [1, 2, 3, 4] . a=5 type(a) #int - integer - 수치형 . int . a=&#39;foo&#39; type(a) . str . &#39;5&#39;+5 . TypeError Traceback (most recent call last) &lt;ipython-input-13-e84694abbbbf&gt; in &lt;module&gt; -&gt; 1 &#39;5&#39;+5 TypeError: can only concatenate str (not &#34;int&#34;) to str . a=4.5 b=2 # String formatting, to be visted later print(&#39;a is {0}, b is {1}&#39;.format(type(a),type(b))) a/b . a is &lt;class &#39;float&#39;&gt;, b is &lt;class &#39;int&#39;&gt; . 2.25 . a=5 isinstance(a,int) . True . a=5; b=4.5 isinstance(a,(int,float)) isinstance(b,(int,float)) . True . a=&#39;foo&#39; #Tab 키를 누르면 다양한 옵션이 나오면서 대문자화 카운트등 여러옵션 존재 . a.&lt;Press Tab&gt; . Input In [7] a.&lt;Press Tab&gt; ^ SyntaxError: invalid syntax . a.upper() . &#39;FOO&#39; . getattr(a,&#39;split&#39;) #object에 속한 속성을 가지고 온다 . &lt;function str.split(sep=None, maxsplit=-1)&gt; . Duck typing . def isiterable(obj): try: iter(obj) return True except TypeError: # not literable return False . isiterable(&#39;a string&#39;) . True . isiterable([1,2,3]) . True . isiterable(5) . False . if not isinstance(x,list) and isiterable(x): x=list(x) . Imports . In Python a module is simply a file with the .py extension containing Python code. Suppose that we had the following module: . If you wanted to access the variables and functions defined in some_module.py, from another file in the same directory we could do: . import some_module #같은 디렉토리의 .py파일을 불러와서 사용가능 result = some_module.f(5) result . 7 . pi = some_module.PI pi . 3.14159 . Or equivalently: . from some_module import f,g,PI result = g(5,PI) result . 8.14159 . By using the as keyword you can give imports different variable names: . import some_module as sm from some_module import PI as pi, g as gf r1 = sm.f(pi) r2 = gf(6,pi) . r1 . 5.14159 . r2 . 9.14159 . Binary operators and comparisons . Most of the binary math operations and comparisons are as you might expect: . 5-7 12+21.5 5 &lt;= 2 . False . a = [1,2,3] b=a c=list() a is b a is not c . True . a == c . False . a = None a is None . True . Mutable and immutable objects . Most object in Python such as list, dict, NumPy arrays, and most user-defined types(classes), are mutable. . This means that the object or values that they contain can be modified . a_list = [&#39;foo&#39;,2,[4,5]] a_list[2] = (3,4) a_list . [&#39;foo&#39;, 2, (3, 4)] . Others, like strings and tuples, are immutable: . a_tuple = (3,5,(4,5)) a_tuple[1] = &#39;four&#39; #tuple은 변경이 불가능 하다 / list는 가능 . TypeError Traceback (most recent call last) Input In [16], in &lt;cell line: 2&gt;() 1 a_tuple = (3,5,(4,5)) -&gt; 2 a_tuple[1] = &#39;four&#39; TypeError: &#39;tuple&#39; object does not support item assignment . Scalar Types . Numeric types . The primary Python types for numbers are int and float. An int can store arbitrarily large numbers: . ival = 17239871 ival ** 6 . 26254519291092456596965462913230729701102721 . Floating-point numbers are represented with the Python float type. Under the hood each one is a double-precision(64-bit) value. They can also be expressed with scientific notation: . fval = 7.2343 fval2 = 6.78e-5 . 3/2 . 1.5 . type(3/2) . float . 3//2 #몫 . 1 . type(3//2) . int . Strings . Many people use Python for its powerful and flexible built-in string processing capabilities. . You can write string literals using either single quotes or double quotes: . a = &#39;one way of writing a string&#39; b = &#39;another way&#39; . For multiline strings with line breaks, you can use triple qutoes, either &#39;&#39;&#39; or &quot;&quot;&quot; . c = &quot;&quot;&quot; This is a longer string that spans multiple lines &quot;&quot;&quot; . c . &#39; nThis is a longer string that nspans multiple lines n&#39; . It may surprise you that this string c actually contains for lines of text; . the line breaks after &quot;&quot;&quot; and after lines are include in the string. . We can count the new line characters with the count method on c: . c.count(&#39; n&#39;) . 3 . Python strings are immutable; you cannot modify a string: . a = &#39;this is a string&#39; a[10] = &#39;f&#39; #변형불가 . TypeError Traceback (most recent call last) Input In [19], in &lt;cell line: 2&gt;() 1 a = &#39;this is a string&#39; -&gt; 2 a[10] = &#39;f&#39; TypeError: &#39;str&#39; object does not support item assignment . b = a.replace(&#39;string&#39;,&#39;longer string&#39;) b . &#39;this is a longer string&#39; . a #변형은 안되나 대체는 가능 . &#39;this is a string&#39; . Many Python object can be converted to a string using the str function . a = 5.6 s = str(a) print(s) . 5.6 . Strings are a sequnce of Unicode characters and therefore can be treated like other sequences, such as lists and tuples(which we will explore in more detail in the next chapter): . s=&#39;python&#39; list(s) . [&#39;p&#39;, &#39;y&#39;, &#39;t&#39;, &#39;h&#39;, &#39;o&#39;, &#39;n&#39;] . s[:3] . &#39;pyt&#39; . The syntax s[:3] is called slicing and is implemented for many kinds of Python sequences. This will be explained in moire detail later on, as it it used extensively in the book. . The backslash character is an escape character, meaning that it is used to specify special characters like newline n or Unicode characters. To write a string literal with backslashes, you need to escape them: . print(&#39;12 n34&#39;) # n is Enter . 12 34 . s = &#39;12 34&#39; #백슬래쉬를 문자열로 바꾼다 print(s) . 12 34 . Adding two strings together concatenates them and produces a new string: . a=&#39;this is the first half &#39; b=&#39;and this is the second half&#39; a+b . &#39;this is the first half and this is the second half&#39; . String templating or formatting is another important topic. . The number of ways tod do so has expanded with the advent of Python 3, . and here I will briefly describe the mechanics of one of the main interfaces. . String objects have a format method that can be used to substitute formatted arguments into the string, producting a new string: . template = &#39;{0:.2f} {1:s} are worth US${2:d}&#39; template . &#39;{0:.2f} {1:s} are worth US${2:d}&#39; . {0:.2f} means to format the first argument as a floating-point number with two decimal places. | {1:s} means to format the second argument as a string. | {2:d} means to format the third argument as an exact integer | . template.format(4.560, &#39;Argentine Pesos&#39;,1) . &#39;4.56 Argentine Pesos are worth US$1&#39; . template.format(1263.23,&#39;won&#39;,1) . &#39;1263.23 won are worth US$1&#39; . Booleans . The two boolean value in python are written as True as False. . Comprasions and other conditional expressions evaluate to either True or False. . Boolean values are combined with the and or keywords: . True and True . True . False or True . True . Type casting . The str, bool, int, and float types are also functions that can be used to cast values . s=&#39;3.14159&#39; fval=float(s) type(fval) . float . int(fval) . 3 . bool(fval) . True . bool(0) . False . None . None is the Python null value type. If a function does not explicitly return a value, it implicitly returns None: . a = None a is None . True . b = 5 b is not None . True . None is also a common default vlaue for function arguments: . def add_and_maybe_multiply(a,b,c=None): result = a+b if c is not None: result = result * c return result . add_and_maybe_multiply(5,3) . 8 . add_and_maybe_multiply(5,3,10) . 80 . While a technical point, it&#39;s worth bearing in mind that None is not only a reserved keyword but also a unique instance of NoneType: . type(None) . NoneType . Dates and times . The built-in Python datetime module provides datetime, date, and time types. . The datetime type, as you may imagine, combines the information stored in date and time and is the most commonly used: . from datetime import datetime, date, time dt = datetime(2011,10,29,20,30,21) #year,month,day,hour,minute,second dt . datetime.datetime(2011, 10, 29, 20, 30, 21) . dt.day . 29 . dt.minute . 30 . Given a datetime instance, you can extract the equivalent date and time objects by calling methods on the datetime of the same name: . dt.date() . datetime.date(2011, 10, 29) . dt.time() . datetime.time(20, 30, 21) . The strfime method formats a datetime as a string: . dt.strftime(&#39;%m/%d/%Y %H:%M&#39;) . &#39;10/29/2011 20:30&#39; . dt.strftime(&#39;%Y/%m/%d %H:%M&#39;) . &#39;2011/10/29 20:30&#39; . String can be converted (parsed) into datetime objects with the strptime function: . datetime.strptime(&#39;20091031&#39;,&quot;%Y%m%d&quot;) . datetime.datetime(2009, 10, 31, 0, 0) . When you are aggregating or otherwise grouping time series data, it will occasionally be useful to replace time fields of a series of datetimes-for example,replacing the minute and second fields with zero: . dt.replace(minute=0,second=0) . datetime.datetime(2011, 10, 29, 20, 0) . dt2 = datetime(2011,11,15,22,30) delta =dt2 - dt #dt = datetime(2011,10,29,20,30,21) delta . datetime.timedelta(days=17, seconds=7179) . type(delta) . datetime.timedelta . dt dt + delta . datetime.datetime(2011, 11, 15, 22, 30) . Control Flow . Python has several built-in keywords for conditonal logic, loops, and other standard control flow concepts found in other porgramming languages. . if, elif, and else . The if statement is one of the most well-known control flow statement types. . It checks a conditon that, if True, evaluates the code in the block that follows: . x = -5 if x &lt; 0: print(&#39;It is negative&#39;) . It is negative . An if statement can be optionally followed by one or mor elif blocks and a catch all else block if all of the conditions are False . x = -5 if x &lt; 0 : print(&#39;It is negative&#39;) elif x == 0 : print(&#39;Equal to zero&#39;) elif 0 &lt; x &lt; 5 : print(&#39;Postive but smaller than 5&#39;) else : print(&#39;Postive and larger than or equal to 5&#39;) . It is negative . If any of the conditions is True, no futher elif or else blocks will be reached. . With a compound condition using and or or, conditions are evaluated left to right and will short-circuit: . a = 5; b = 7 c = 8; d = 4 if a &lt; b or c &gt; d : print(&#39;Made it&#39;) . Made it . In this examplme, the comparison c &gt; d never get evaluated because the first compar- ison was True. It is also possible to chain comparisons: . 4 &gt; 3 &gt; 2 &gt; 1 . True . 3 &gt; 5 or 2 &gt; 1 . True . 3&gt;5&gt;2&gt;1 . False . for loops . for loops are fir iterating over a collection (like a list or tuple) or an iterater. They standard syntax for a for loop is: . for value in collection: . # do something with value . You can advance a for loop to the next iteration, skipping the remainder of the block, using the continue keyword. . Consider this code, which sums up integers in a list and skips None values . sequnce = [1,2,None,4,None,5] total = 0 for value in sequnce: total += value . TypeError Traceback (most recent call last) Input In [69], in &lt;cell line: 4&gt;() 2 total = 0 4 for value in sequnce: -&gt; 5 total += value TypeError: unsupported operand type(s) for +=: &#39;int&#39; and &#39;NoneType&#39; . sequnce = [1,2,None,4,None,5] total = 0 for value in sequnce: if value is None: continue total += value . total . 12 . A for loop cna be exited altogether with the break keyword. This code sums ele- ments of the list until a 5 is reached: . sequnce = [1,2,0,4,6,5,2,1] total_until_5 = 0 for value in sequnce: if value == 5: break total_until_5 += value . total_until_5 #값이 5가 되면 멈추는 조건으로 1+2+0+4+6까지 계산후 5가 오기에 for문이 종료된다 . 13 . The break keyword only terminates the innermost for loop; any outer for loops will continue to run: . for i in range(4): for j in range(4): if j&gt;i: break print((i,j)) . (0, 0) (1, 0) (1, 1) (2, 0) (2, 1) (2, 2) (3, 0) (3, 1) (3, 2) (3, 3) . As we will see in more detail, if the elements in the collection or iterator are sequences (tuples or list, say), they can be conveniently unpacked into variables in the for loop statement: . for a,b,c in iterator: . # do something . for a,b,c in [[1,2,3],[4,5,6],[7,8,9]]: print(a,b,c) . 1 2 3 4 5 6 7 8 9 . While loops . A while looops specifies a conditon and a block o f code that is to be excused until the condition evaluates to False or the loops is explicitly ended with break: . x = 256 total = 0 while x &gt; 0: if total &gt; 500 : break total += x x = x //2 . total #504 . 504 . x . 4 . 256+128+64+32+16+8 . 504 . pass . pass is the &quot;no-op&quot;(No Operation) statement in Python. It can be used in block where no action is to be taken (or as a placeholder for code not yer implemented); it is only required becauese Python uese whitespace to delimit blocks: . x = -1 if x &lt; 0: print(&#39;negative&#39;) elif x == 0: # TODO: put something smart here pass else: print(&#39;Postive!&#39;) . negative . range . The range function returns an iterator that yields a sequnce of evenly spaced intergers: . range(10) . range(0, 10) . list(range(10)) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . Both a start,end,and step(Which may be negative) can be given: . list(range(0,20,2)) #등차수열 . [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] . list(range(5,0,-1)) #리버스 인덱스 . [5, 4, 3, 2, 1] . As you can see, range prodices integers up to but not including the endpoint. . A common use of range is for iterating through sequcnes by index: . seq = [1,2,3,4] for i in range(len(seq)): val = seq[i] . val . 4 . While you can use fuctions loke list to store all the integers generated by range in some other data structure, often the default iterator form will be what you want. This snippet sums all numbers form 0 to 99,999 that are multiples of 3 or 5: . sum = 0 for i in range(10000) : # % is the modulo operator if i % 3 == 0 or i % 5 == 0: sum += 1 . Ternary expressions . value = true - expr if conditon else false - expr . Here, true-expr and false-expr cna be any Python expressions. It has the identical effect as the more verbose: . if conditon: value = true-expr else: value = false-expr . x = 5 &#39;Non-negative&#39; if x &gt;= 0 else &#39;Negative&#39; . &#39;Non-negative&#39; . x = 5 a = 100 if x&gt;= 0 else -100 a . 100 .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/03/18/Python-Language-Basics.html",
            "relUrl": "/jupyter/python/2022/03/18/Python-Language-Basics.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Markdown Cheat Sheet",
            "content": "Markdown Cheat Sheet . Thanks for visiting The Markdown Guide! . This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax. . Basic Syntax . These are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements. . Heading . H1 . H2 . H3 . Bold . bold text . Italic . italicized text . Blockquote . blockquote . Ordered List . First item | Second item | Third item | Unordered List . First item | Second item | Third item | . Code . code . Horizontal Rule . . Link . Markdown Guide . Image . . Extended Syntax . These elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements. . Table . Syntax Description . Header | Title | . Paragraph | Text | . Fenced Code Block . { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;age&quot;: 25 } . Footnote . Here’s a sentence with a footnote. 1 . Heading ID . My Great Heading . Definition List . term definition Strikethrough . The world is flat. . Task List . Write the press release | Update the website | Contact the media | . Emoji . That is so funny! :joy: . (See also Copying and Pasting Emoji) . Highlight . I need to highlight these ==very important words==. . Subscript . H~2~O . Superscript . X^2^ . This is the footnote. &#8617; . |",
            "url": "https://hyunsookim0813.github.io/blog/markdown/2022/03/11/markdown-cheat-sheet.html",
            "relUrl": "/markdown/2022/03/11/markdown-cheat-sheet.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Numpy 기본",
            "content": "도구 - 넘파이(NumPy) . *넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.&quot; . 구글 코랩에서 실행하기 | &#48176;&#50676; &#49373;&#49457; . numpy를 임포트해 보죠. 대부분의 사람들이 np로 알리아싱하여 임포트합니다: . import numpy as np . np.zeros . zeros 함수는 0으로 채워진 배열을 만듭니다: . np.zeros(5) . array([0., 0., 0., 0., 0.]) . 2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 $3 times 4$ 크기의 행렬입니다: . np.zeros((3,4)) . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . &#50857;&#50612; . 넘파이에서 각 차원을 축(axis) 이라고 합니다 | 축의 개수를 랭크(rank) 라고 합니다. 예를 들어, 위의 $3 times 4$ 행렬은 랭크 2인 배열입니다(즉 2차원입니다). | 첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다. | . | 배열의 축 길이를 배열의 크기(shape)라고 합니다. 예를 들어, 위 행렬의 크기는 (3, 4)입니다. | 랭크는 크기의 길이와 같습니다. | . | 배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, $3 times 4=12$). | . a = np.zeros((3,4)) a . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . a.shape . (3, 4) . a.ndim # len(a.shape)와 같습니다 . 2 . a.size . 12 . N-&#52264;&#50896; &#48176;&#50676; . 임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다: . np.zeros((2,2,5)) . array([[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]]) . &#48176;&#50676; &#53440;&#51077; . 넘파이 배열의 타입은 ndarray입니다: . type(np.zeros((3,4))) . numpy.ndarray . np.ones . ndarray를 만들 수 있는 넘파이 함수가 많습니다. . 다음은 1로 채워진 $3 times 4$ 크기의 행렬입니다: . np.ones((3,4)) . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . np.full . 주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 $3 times 4$ 크기의 행렬입니다. . np.full((3,4), np.pi) . array([[3.14159265, 3.14159265, 3.14159265, 3.14159265], [3.14159265, 3.14159265, 3.14159265, 3.14159265], [3.14159265, 3.14159265, 3.14159265, 3.14159265]]) . np.empty . 초기화되지 않은 $2 times 3$ 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다): . np.empty((2,3)) . array([[9.6677106e-317, 0.0000000e+000, 0.0000000e+000], [0.0000000e+000, 0.0000000e+000, 0.0000000e+000]]) . np.array . array 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다: . np.array([[1,2,3,4], [10, 20, 30, 40]]) . array([[ 1, 2, 3, 4], [10, 20, 30, 40]]) . np.arange . 파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다: . np.arange(1, 5) . array([1, 2, 3, 4]) . 부동 소수도 가능합니다: . np.arange(1.0, 5.0) . array([1., 2., 3., 4.]) . 파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다: . np.arange(1, 5, 0.5) . array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5]) . 부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다: . print(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다. print(np.arange(0, 5/3, 0.333333333)) print(np.arange(0, 5/3, 0.333333334)) . [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [0. 0.33333333 0.66666667 1. 1.33333334] . np.linspace . 이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다): . print(np.linspace(0, 5/3, 6)) . [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] . np.rand&#50752; np.randn . 넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 $3 times 4$ 행렬을 초기화합니다: . np.random.rand(3,4) . array([[0.37892456, 0.17966937, 0.38206837, 0.34922123], [0.80462136, 0.9845914 , 0.9416127 , 0.28305275], [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]]) . 다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 $3 times 4$ 행렬입니다: . np.random.randn(3,4) . array([[ 0.83811287, -0.57131751, -0.4381827 , 1.1485899 ], [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ], [ 1.01003549, 1.04381736, -0.93060038, 2.39043293]]) . 이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요): . %matplotlib inline import matplotlib.pyplot as plt . plt.hist(np.random.rand(100000), density=True, bins=100, histtype=&quot;step&quot;, color=&quot;blue&quot;, label=&quot;rand&quot;) plt.hist(np.random.randn(100000), density=True, bins=100, histtype=&quot;step&quot;, color=&quot;red&quot;, label=&quot;randn&quot;) plt.axis([-2.5, 2.5, 0, 1.1]) plt.legend(loc = &quot;upper left&quot;) plt.title(&quot;Random distributions&quot;) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Density&quot;) plt.show() . np.fromfunction . 함수를 사용하여 ndarray를 초기화할 수도 있습니다: . def my_function(z, y, x): return x + 10 * y + 100 * z np.fromfunction(my_function, (3, 2, 10)) . array([[[ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], [ 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]], [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.], [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]], [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.], [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]]) . 넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다: . [[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] [[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] [ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]] . 위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다. . &#48176;&#50676; &#45936;&#51060;&#53552; . dtype . 넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다: . c = np.arange(1, 5) print(c.dtype, c) . int64 [1 2 3 4] . c = np.arange(1.0, 5.0) print(c.dtype, c) . float64 [1. 2. 3. 4.] . 넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다: . d = np.arange(1, 5, dtype=np.complex64) print(d.dtype, d) . complex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j] . 가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요. . itemsize . itemsize 속성은 각 아이템의 크기(바이트)를 반환합니다: . e = np.arange(1, 5, dtype=np.complex64) e.itemsize . 8 . data &#48260;&#54140; . 배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요). . f = np.array([[1,2],[1000, 2000]], dtype=np.int32) f.data . &lt;memory at 0x7f97929dd790&gt; . 파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다. . if (hasattr(f.data, &quot;tobytes&quot;)): data_bytes = f.data.tobytes() # python 3 else: data_bytes = memoryview(f.data).tobytes() # python 2 data_bytes . b&#39; x01 x00 x00 x00 x02 x00 x00 x00 xe8 x03 x00 x00 xd0 x07 x00 x00&#39; . 여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다. . &#48176;&#50676; &#53356;&#44592; &#48320;&#44221; . &#51088;&#49888;&#51012; &#48320;&#44221; . ndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다. . g = np.arange(24) print(g) print(&quot;랭크:&quot;, g.ndim) . [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] 랭크: 1 . g.shape = (6, 4) print(g) print(&quot;랭크:&quot;, g.ndim) . [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15] [16 17 18 19] [20 21 22 23]] 랭크: 2 . g.shape = (2, 3, 4) print(g) print(&quot;랭크:&quot;, g.ndim) . [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 랭크: 3 . reshape . reshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다. . g2 = g.reshape(4,6) print(g2) print(&quot;랭크:&quot;, g2.ndim) . [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 랭크: 2 . 행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요). . g2[1, 2] = 999 g2 . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 999, 9, 10, 11], [ 12, 13, 14, 15, 16, 17], [ 18, 19, 20, 21, 22, 23]]) . 이에 상응하는 g의 원소도 수정됩니다. . g . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [999, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]]) . ravel . 마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다: . g.ravel() . array([ 0, 1, 2, 3, 4, 5, 6, 7, 999, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . &#49328;&#49696; &#50672;&#49328; . 일반적인 산술 연산자(+, -, *, /, //, ** 등)는 모두 ndarray와 사용할 수 있습니다. 이 연산자는 원소별로 적용됩니다: . a = np.array([14, 23, 32, 41]) b = np.array([5, 4, 3, 2]) print(&quot;a + b =&quot;, a + b) print(&quot;a - b =&quot;, a - b) print(&quot;a * b =&quot;, a * b) print(&quot;a / b =&quot;, a / b) print(&quot;a // b =&quot;, a // b) print(&quot;a % b =&quot;, a % b) print(&quot;a ** b =&quot;, a ** b) . a + b = [19 27 35 43] a - b = [ 9 19 29 39] a * b = [70 92 96 82] a / b = [ 2.8 5.75 10.66666667 20.5 ] a // b = [ 2 5 10 20] a % b = [4 3 2 1] a ** b = [537824 279841 32768 1681] . 여기 곱셈은 행렬 곱셈이 아닙니다. 행렬 연산은 아래에서 설명합니다. . 배열의 크기는 같아야 합니다. 그렇지 않으면 넘파이가 브로드캐스팅 규칙을 적용합니다. . &#48652;&#47196;&#46300;&#52880;&#49828;&#54021; . 일반적으로 넘파이는 동일한 크기의 배열을 기대합니다. 그렇지 않은 상황에는 브로드캐시틍 규칙을 적용합니다: . &#44508;&#52825; 1 . 배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다. . h = np.arange(5).reshape(1, 1, 5) h . array([[[0, 1, 2, 3, 4]]]) . 여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다! . h + [10, 20, 30, 40, 50] # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]] . array([[[10, 21, 32, 43, 54]]]) . &#44508;&#52825; 2 . 특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다. . k = np.arange(6).reshape(2, 3) k . array([[0, 1, 2], [3, 4, 5]]) . (2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다: . k + [[100], [200]] # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]] . array([[100, 101, 102], [203, 204, 205]]) . 규칙 1과 2를 합치면 다음과 같이 동작합니다: . k + [100, 200, 300] # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]] . array([[100, 201, 302], [103, 204, 305]]) . 또 매우 간단히 다음 처럼 해도 됩니다: . k + 1000 # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]] . array([[1000, 1001, 1002], [1003, 1004, 1005]]) . &#44508;&#52825; 3 . 규칙 1 &amp; 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다. . try: k + [33, 44] except ValueError as e: print(e) . operands could not be broadcast together with shapes (2,3) (2,) . 브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요. . &#50629;&#52880;&#49828;&#54021; . dtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다. . k1 = np.arange(0, 5, dtype=np.uint8) print(k1.dtype, k1) . uint8 [0 1 2 3 4] . k2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8) print(k2.dtype, k2) . int16 [ 5 7 9 11 13] . 모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다. . k3 = k1 + 1.5 print(k3.dtype, k3) . float64 [1.5 2.5 3.5 4.5 5.5] . &#51312;&#44148; &#50672;&#49328;&#51088; . 조건 연산자도 원소별로 적용됩니다: . m = np.array([20, -5, 30, 40]) m &lt; [15, 16, 35, 36] . array([False, True, True, False]) . 브로드캐스팅을 사용합니다: . m &lt; 25 # m &lt; [25, 25, 25, 25] 와 동일 . array([ True, True, False, False]) . 불리언 인덱싱과 함께 사용하면 아주 유용합니다(아래에서 설명하겠습니다). . m[m &lt; 25] . array([20, -5]) . &#49688;&#54617; &#54632;&#49688;&#50752; &#53685;&#44228; &#54632;&#49688; . ndarray에서 사용할 수 있는 수학 함수와 통계 함수가 많습니다. . ndarray &#47700;&#49436;&#46300; . 일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) print(a) print(&quot;평균 =&quot;, a.mean()) . [[-2.5 3.1 7. ] [10. 11. 12. ]] 평균 = 6.766666666666667 . 이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다. . 다음은 유용한 ndarray 메서드입니다: . for func in (a.min, a.max, a.sum, a.prod, a.std, a.var): print(func.__name__, &quot;=&quot;, func()) . min = -2.5 max = 12.0 sum = 40.6 prod = -71610.0 std = 5.084835843520964 var = 25.855555555555554 . 이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면: . c=np.arange(24).reshape(2,3,4) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . c.sum(axis=0) # 첫 번째 축을 따라 더함, 결과는 3x4 배열 . array([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]) . c.sum(axis=1) # 두 번째 축을 따라 더함, 결과는 2x4 배열 . array([[12, 15, 18, 21], [48, 51, 54, 57]]) . 여러 축에 대해서 더할 수도 있습니다: . c.sum(axis=(0,2)) # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열 . array([ 60, 92, 124]) . 0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23 . (60, 92, 124) . &#51068;&#48152; &#54632;&#49688; . 넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) np.square(a) . array([[ 6.25, 9.61, 49. ], [100. , 121. , 144. ]]) . 다음은 유용한 단항 일반 함수들입니다: . print(&quot;원본 ndarray&quot;) print(a) for func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos): print(&quot; n&quot;, func.__name__) print(func(a)) . 원본 ndarray [[-2.5 3.1 7. ] [10. 11. 12. ]] absolute [[ 2.5 3.1 7. ] [10. 11. 12. ]] sqrt [[ nan 1.76068169 2.64575131] [3.16227766 3.31662479 3.46410162]] exp [[8.20849986e-02 2.21979513e+01 1.09663316e+03] [2.20264658e+04 5.98741417e+04 1.62754791e+05]] log [[ nan 1.13140211 1.94591015] [2.30258509 2.39789527 2.48490665]] sign [[-1. 1. 1.] [ 1. 1. 1.]] ceil [[-2. 4. 7.] [10. 11. 12.]] modf (array([[-0.5, 0.1, 0. ], [ 0. , 0. , 0. ]]), array([[-2., 3., 7.], [10., 11., 12.]])) isnan [[False False False] [False False False]] cos [[-0.80114362 -0.99913515 0.75390225] [-0.83907153 0.0044257 0.84385396]] . &lt;ipython-input-59-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in sqrt print(func(a)) &lt;ipython-input-59-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in log print(func(a)) . &#51060;&#54637; &#51068;&#48152; &#54632;&#49688; . 두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다: . a = np.array([1, -2, 3, 4]) b = np.array([2, 8, -1, 7]) np.add(a, b) # a + b 와 동일 . array([ 3, 6, 2, 11]) . np.greater(a, b) # a &gt; b 와 동일 . array([False, False, True, False]) . np.maximum(a, b) . array([2, 8, 3, 7]) . np.copysign(a, b) . array([ 1., 2., -3., 4.]) . &#48176;&#50676; &#51064;&#45937;&#49905; . 1&#52264;&#50896; &#48176;&#50676; . 1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다: . a = np.array([1, 5, 3, 19, 13, 7, 3]) a[3] . 19 . a[2:5] . array([ 3, 19, 13]) . a[2:-1] . array([ 3, 19, 13, 7]) . a[:2] . array([1, 5]) . a[2::2] . array([ 3, 13, 3]) . a[::-1] . array([ 3, 7, 13, 19, 3, 5, 1]) . 물론 원소를 수정할 수 있죠: . a[3]=999 a . array([ 1, 5, 3, 999, 13, 7, 3]) . 슬라이싱을 사용해 ndarray를 수정할 수 있습니다: . a[2:5] = [997, 998, 999] a . array([ 1, 5, 997, 998, 999, 7, 3]) . &#48372;&#53685;&#51032; &#54028;&#51060;&#50028; &#48176;&#50676;&#44284; &#52264;&#51060;&#51216; . 보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다. . a[2:5] = -1 a . array([ 1, 5, -1, -1, -1, 7, 3]) . 또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다: . try: a[2:5] = [1,2,3,4,5,6] # 너무 길어요 except ValueError as e: print(e) . cannot copy sequence with size 6 to array axis with dimension 3 . 원소를 삭제할 수도 없습니다: . try: del a[2:5] except ValueError as e: print(e) . cannot delete array elements . 중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다! . a_slice = a[2:6] a_slice[1] = 1000 a # 원본 배열이 수정됩니다! . array([ 1, 5, -1, 1000, -1, 7, 3]) . a[3] = 2000 a_slice # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다! . array([ -1, 2000, -1, 7]) . 데이터를 복사하려면 copy 메서드를 사용해야 합니다: . another_slice = a[2:6].copy() another_slice[1] = 3000 a # 원본 배열이 수정되지 않습니다 . array([ 1, 5, -1, 2000, -1, 7, 3]) . a[3] = 4000 another_slice # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다 . array([ -1, 3000, -1, 7]) . &#45796;&#52264;&#50896; &#48176;&#50676; . 다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다: . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . b[1, 2] # 행 1, 열 2 . 14 . b[1, :] # 행 1, 모든 열 . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[:, 1] # 모든 행, 열 1 . array([ 1, 13, 25, 37]) . 주의: 다음 두 표현에는 미묘한 차이가 있습니다: . b[1, :] . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[1:2, :] . array([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]) . 첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다. . &#54060;&#49884; &#51064;&#45937;&#49905;(Fancy indexing) . 관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다. . b[(0,2), 2:5] # 행 0과 2, 열 2에서 4(5-1)까지 . array([[ 2, 3, 4], [26, 27, 28]]) . b[:, (-1, 2, -1)] # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로) . array([[11, 2, 11], [23, 14, 23], [35, 26, 35], [47, 38, 47]]) . 여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다. . b[(-1, 2, -1, 2), (5, 9, 1, 9)] # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again) . array([41, 33, 37, 33]) . &#44256;&#52264;&#50896; . 고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다: . c = b.reshape(4,2,6) c . array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]], [[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], [[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]], [[36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47]]]) . c[2, 1, 4] # 행렬 2, 행 1, 열 4 . 34 . c[2, :, 3] # 행렬 2, 모든 행, 열 3 . array([27, 33]) . 어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다: . c[2, 1] # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다. . array([30, 31, 32, 33, 34, 35]) . &#49373;&#47029; &#48512;&#54840; (...) . 생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다. . c[2, ...] # 행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일 . array([[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]) . c[2, 1, ...] # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일 . array([30, 31, 32, 33, 34, 35]) . c[2, ..., 3] # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일 . array([27, 33]) . c[..., 3] # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일 . array([[ 3, 9], [15, 21], [27, 33], [39, 45]]) . &#48520;&#47532;&#50616; &#51064;&#45937;&#49905; . 불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다. . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . rows_on = np.array([True, False, True, False]) b[rows_on, :] # 행 0과 2, 모든 열. b[(0, 2), :]와 동일 . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]]) . cols_on = np.array([False, True, False] * 4) b[:, cols_on] # 모든 행, 열 1, 4, 7, 10 . array([[ 1, 4, 7, 10], [13, 16, 19, 22], [25, 28, 31, 34], [37, 40, 43, 46]]) . np.ix_ . 여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다: . b[np.ix_(rows_on, cols_on)] . array([[ 1, 4, 7, 10], [25, 28, 31, 34]]) . np.ix_(rows_on, cols_on) . (array([[0], [2]]), array([[ 1, 4, 7, 10]])) . ndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다: . b[b % 3 == 1] . array([ 1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46]) . &#48152;&#48373; . ndarray를 반복하는 것은 일반적인 파이썬 배열을 반복한는 것과 매우 유사합니다. 다차원 배열을 반복하면 첫 번째 축에 대해서 수행됩니다. . c = np.arange(24).reshape(2, 3, 4) # 3D 배열 (두 개의 3x4 행렬로 구성됨) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . for m in c: print(&quot;아이템:&quot;) print(m) . 아이템: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 아이템: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . for i in range(len(c)): # len(c) == c.shape[0] print(&quot;아이템:&quot;) print(c[i]) . 아이템: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 아이템: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . ndarray에 있는 모든 원소를 반복하려면 flat 속성을 사용합니다: . for i in c.flat: print(&quot;아이템:&quot;, i) . 아이템: 0 아이템: 1 아이템: 2 아이템: 3 아이템: 4 아이템: 5 아이템: 6 아이템: 7 아이템: 8 아이템: 9 아이템: 10 아이템: 11 아이템: 12 아이템: 13 아이템: 14 아이템: 15 아이템: 16 아이템: 17 아이템: 18 아이템: 19 아이템: 20 아이템: 21 아이템: 22 아이템: 23 . &#48176;&#50676; &#49939;&#44592; . 종종 다른 배열을 쌓아야 할 때가 있습니다. 넘파이는 이를 위해 몇 개의 함수를 제공합니다. 먼저 배열 몇 개를 만들어 보죠. . q1 = np.full((3,4), 1.0) q1 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . q2 = np.full((4,4), 2.0) q2 . array([[2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.]]) . q3 = np.full((3,4), 3.0) q3 . array([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . vstack . vstack 함수를 사용하여 수직으로 쌓아보죠: . q4 = np.vstack((q1, q2, q3)) q4 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . q4.shape . (10, 4) . q1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다). . hstack . hstack을 사용해 수평으로도 쌓을 수 있습니다: . q5 = np.hstack((q1, q3)) q5 . array([[1., 1., 1., 1., 3., 3., 3., 3.], [1., 1., 1., 1., 3., 3., 3., 3.], [1., 1., 1., 1., 3., 3., 3., 3.]]) . q5.shape . (3, 8) . q1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다: . try: q5 = np.hstack((q1, q2, q3)) except ValueError as e: print(e) . all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4 . concatenate . concatenate 함수는 지정한 축으로도 배열을 쌓습니다. . q7 = np.concatenate((q1, q2, q3), axis=0) # vstack과 동일 q7 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . q7.shape . (10, 4) . 예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다. . stack . stack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다. . q8 = np.stack((q1, q3)) q8 . array([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], [[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]]) . q8.shape . (2, 3, 4) . &#48176;&#50676; &#48516;&#54624; . 분할은 쌓기의 반대입니다. 예를 들어 vsplit 함수는 행렬을 수직으로 분할합니다. . 먼저 6x4 행렬을 만들어 보죠: . r = np.arange(24).reshape(6,4) r . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]) . 수직으로 동일한 크기로 나누어 보겠습니다: . r1, r2, r3 = np.vsplit(r, 3) r1 . array([[0, 1, 2, 3], [4, 5, 6, 7]]) . r2 . array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) . r3 . array([[16, 17, 18, 19], [20, 21, 22, 23]]) . split 함수는 주어진 축을 따라 배열을 분할합니다. vsplit는 axis=0으로 split를 호출하는 것과 같습니다. hsplit 함수는 axis=1로 split를 호출하는 것과 같습니다: . r4, r5 = np.hsplit(r, 2) r4 . array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13], [16, 17], [20, 21]]) . r5 . array([[ 2, 3], [ 6, 7], [10, 11], [14, 15], [18, 19], [22, 23]]) . &#48176;&#50676; &#51204;&#52824; . transpose 메서드는 주어진 순서대로 축을 뒤바꾸어 ndarray 데이터에 대한 새로운 뷰를 만듭니다. . 예를 위해 3D 배열을 만들어 보죠: . t = np.arange(24).reshape(4,2,3) t . array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23]]]) . 0, 1, 2(깊이, 높이, 너비) 축을 1, 2, 0 (깊이→너비, 높이→깊이, 너비→높이) 순서로 바꾼 ndarray를 만들어 보겠습니다: . t1 = t.transpose((1,2,0)) t1 . array([[[ 0, 6, 12, 18], [ 1, 7, 13, 19], [ 2, 8, 14, 20]], [[ 3, 9, 15, 21], [ 4, 10, 16, 22], [ 5, 11, 17, 23]]]) . t1.shape . (2, 3, 4) . transpose 기본값은 차원의 순서를 역전시킵니다: . t2 = t.transpose() # t.transpose((2, 1, 0))와 동일 t2 . array([[[ 0, 6, 12, 18], [ 3, 9, 15, 21]], [[ 1, 7, 13, 19], [ 4, 10, 16, 22]], [[ 2, 8, 14, 20], [ 5, 11, 17, 23]]]) . t2.shape . (3, 2, 4) . 넘파이는 두 축을 바꾸는 swapaxes 함수를 제공합니다. 예를 들어 깊이와 높이를 뒤바꾸어 t의 새로운 뷰를 만들어 보죠: . t3 = t.swapaxes(0,1) # t.transpose((1, 0, 2))와 동일 t3 . array([[[ 0, 1, 2], [ 6, 7, 8], [12, 13, 14], [18, 19, 20]], [[ 3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]]]) . t3.shape . (2, 4, 3) . &#49440;&#54805; &#45824;&#49688;&#54617; . 넘파이 2D 배열을 사용하면 파이썬에서 행렬을 효율적으로 표현할 수 있습니다. 주요 행렬 연산을 간단히 둘러 보겠습니다. 선형 대수학, 벡터와 행렬에 관한 자세한 내용은 Linear Algebra tutorial를 참고하세요. . &#54665;&#47148; &#51204;&#52824; . T 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다: . m1 = np.arange(10).reshape(2,5) m1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . m1.T . array([[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]) . T 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다: . m2 = np.arange(5) m2 . array([0, 1, 2, 3, 4]) . m2.T . array([0, 1, 2, 3, 4]) . 먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다: . m2r = m2.reshape(1,5) m2r . array([[0, 1, 2, 3, 4]]) . m2r.T . array([[0], [1], [2], [3], [4]]) . &#54665;&#47148; &#44273;&#49480; . 두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠. . n1 = np.arange(10).reshape(2, 5) n1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . n2 = np.arange(15).reshape(5,3) n2 . array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) . n1.dot(n2) . array([[ 90, 100, 110], [240, 275, 310]]) . 주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다. . &#50669;&#54665;&#47148;&#44284; &#50976;&#49324; &#50669;&#54665;&#47148; . numpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다: . import numpy.linalg as linalg m3 = np.array([[1,2,3],[5,7,11],[21,29,31]]) m3 . array([[ 1, 2, 3], [ 5, 7, 11], [21, 29, 31]]) . linalg.inv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . pinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다: . linalg.pinv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . &#45800;&#50948; &#54665;&#47148; . 행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다): . m3.dot(linalg.inv(m3)) . array([[ 1.00000000e+00, -1.66533454e-16, 0.00000000e+00], [ 6.31439345e-16, 1.00000000e+00, -1.38777878e-16], [ 5.21110932e-15, -2.38697950e-15, 1.00000000e+00]]) . eye 함수는 NxN 크기의 단위 행렬을 만듭니다: . np.eye(3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . QR &#48516;&#54644; . qr 함수는 행렬을 QR 분해합니다: . q, r = linalg.qr(m3) q . array([[-0.04627448, 0.98786672, 0.14824986], [-0.23137241, 0.13377362, -0.96362411], [-0.97176411, -0.07889213, 0.22237479]]) . r . array([[-21.61018278, -29.89331494, -32.80860727], [ 0. , 0.62427688, 1.9894538 ], [ 0. , 0. , -3.26149699]]) . q.dot(r) # q.r는 m3와 같습니다 . array([[ 1., 2., 3.], [ 5., 7., 11.], [21., 29., 31.]]) . &#54665;&#47148;&#49885; . det 함수는 행렬식을 계산합니다: . linalg.det(m3) # 행렬식 계산 . 43.99999999999997 . &#44256;&#50995;&#44050;&#44284; &#44256;&#50976;&#48289;&#53552; . eig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다: . eigenvalues, eigenvectors = linalg.eig(m3) eigenvalues # λ . array([42.26600592, -0.35798416, -2.90802176]) . eigenvectors # v . array([[-0.08381182, -0.76283526, -0.18913107], [-0.3075286 , 0.64133975, -0.6853186 ], [-0.94784057, -0.08225377, 0.70325518]]) . m3.dot(eigenvectors) - eigenvalues * eigenvectors # m3.v - λ*v = 0 . array([[ 8.88178420e-15, 2.22044605e-16, -3.10862447e-15], [ 3.55271368e-15, 2.02615702e-15, -1.11022302e-15], [ 3.55271368e-14, 3.33413852e-15, -8.43769499e-15]]) . &#53945;&#51079;&#44050; &#48516;&#54644; . svd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다: . m4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]]) m4 . array([[1, 0, 0, 0, 2], [0, 0, 3, 0, 0], [0, 0, 0, 0, 0], [0, 2, 0, 0, 0]]) . U, S_diag, V = linalg.svd(m4) U . array([[ 0., 1., 0., 0.], [ 1., 0., 0., 0.], [ 0., 0., 0., -1.], [ 0., 0., 1., 0.]]) . S_diag . array([3. , 2.23606798, 2. , 0. ]) . svd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다: . S = np.zeros((4, 5)) S[np.diag_indices(4)] = S_diag S # Σ . array([[3. , 0. , 0. , 0. , 0. ], [0. , 2.23606798, 0. , 0. , 0. ], [0. , 0. , 2. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. ]]) . V . array([[-0. , 0. , 1. , -0. , 0. ], [ 0.4472136 , 0. , 0. , 0. , 0.89442719], [-0. , 1. , 0. , -0. , 0. ], [ 0. , 0. , 0. , 1. , 0. ], [-0.89442719, 0. , 0. , 0. , 0.4472136 ]]) . U.dot(S).dot(V) # U.Σ.V == m4 . array([[1., 0., 0., 0., 2.], [0., 0., 3., 0., 0.], [0., 0., 0., 0., 0.], [0., 2., 0., 0., 0.]]) . &#45824;&#44033;&#50896;&#49548;&#50752; &#45824;&#44033;&#54633; . np.diag(m3) # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래) . array([ 1, 7, 31]) . np.trace(m3) # np.diag(m3).sum()와 같습니다 . 39 . &#49440;&#54805; &#48169;&#51221;&#49885; &#54400;&#44592; . solve 함수는 다음과 같은 선형 방정식을 풉니다: . $2x + 6y = 6$ | $5x + 3y = -9$ | . coeffs = np.array([[2, 6], [5, 3]]) depvars = np.array([6, -9]) solution = linalg.solve(coeffs, depvars) solution . array([-3., 2.]) . solution을 확인해 보죠: . coeffs.dot(solution), depvars # 네 같네요 . (array([ 6., -9.]), array([ 6, -9])) . 좋습니다! 다른 방식으로도 solution을 확인해 보죠: . np.allclose(coeffs.dot(solution), depvars) . True . &#48289;&#53552;&#54868; . 한 번에 하나씩 개별 배열 원소에 대해 연산을 실행하는 대신 배열 연산을 사용하면 훨씬 효율적인 코드를 만들 수 있습니다. 이를 벡터화라고 합니다. 이를 사용하여 넘파이의 최적화된 성능을 활용할 수 있습니다. . 예를 들어, $sin(xy/40.5)$ 식을 기반으로 768x1024 크기 배열을 생성하려고 합니다. 중첩 반복문 안에 파이썬의 math 함수를 사용하는 것은 나쁜 방법입니다: . import math data = np.empty((768, 1024)) for y in range(768): for x in range(1024): data[y, x] = math.sin(x*y/40.5) # 매우 비효율적입니다! . 작동은 하지만 순수한 파이썬 코드로 반복문이 진행되기 때문에 아주 비효율적입니다. 이 알고리즘을 벡터화해 보죠. 먼저 넘파이 meshgrid 함수로 좌표 벡터를 사용해 행렬을 만듭니다. . x_coords = np.arange(0, 1024) # [0, 1, 2, ..., 1023] y_coords = np.arange(0, 768) # [0, 1, 2, ..., 767] X, Y = np.meshgrid(x_coords, y_coords) X . array([[ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], ..., [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023]]) . Y . array([[ 0, 0, 0, ..., 0, 0, 0], [ 1, 1, 1, ..., 1, 1, 1], [ 2, 2, 2, ..., 2, 2, 2], ..., [765, 765, 765, ..., 765, 765, 765], [766, 766, 766, ..., 766, 766, 766], [767, 767, 767, ..., 767, 767, 767]]) . 여기서 볼 수 있듯이 X와 Y 모두 768x1024 배열입니다. X에 있는 모든 값은 수평 좌표에 해당합니다. Y에 있는 모든 값은 수직 좌표에 해당합니다. . 이제 간단히 배열 연산을 사용해 계산할 수 있습니다: . data = np.sin(X*Y/40.5) . 맷플롯립의 imshow 함수를 사용해 이 데이터를 그려보죠(matplotlib tutorial을 참조하세요). . import matplotlib.pyplot as plt import matplotlib.cm as cm fig = plt.figure(1, figsize=(7, 6)) plt.imshow(data, cmap=cm.hot) plt.show() . &#51200;&#51109;&#44284; &#47196;&#46377; . 넘파이는 ndarray를 바이너리 또는 텍스트 포맷으로 손쉽게 저장하고 로드할 수 있습니다. . &#48148;&#51060;&#45320;&#47532; .npy &#54252;&#47607; . 랜덤 배열을 만들고 저장해 보죠. . a = np.random.rand(2,3) a . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . np.save(&quot;my_array&quot;, a) . 끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다: . with open(&quot;my_array.npy&quot;, &quot;rb&quot;) as f: content = f.read() content . b&#34; x93NUMPY x01 x00v x00{&#39;descr&#39;: &#39;&lt;f8&#39;, &#39;fortran_order&#39;: False, &#39;shape&#39;: (2, 3), } nY xc1 xfc xd0 x1ee xe1? xde{3 t? xb9 xed? x80V x08 xef xa5p x8f? x96I} xe0J x9b xda? xe0U xfaav xed? xd8 xe50 xc59 xa4 xe1?&#34; . 이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다: . a_loaded = np.load(&quot;my_array.npy&quot;) a_loaded . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#53581;&#49828;&#53944; &#54252;&#47607; . 배열을 텍스트 포맷으로 저장해 보죠: . np.savetxt(&quot;my_array.csv&quot;, a) . 파일 내용을 확인해 보겠습니다: . with open(&quot;my_array.csv&quot;, &quot;rt&quot;) as f: print(f.read()) . 5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02 4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01 . 이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다: . np.savetxt(&quot;my_array.csv&quot;, a, delimiter=&quot;,&quot;) . 이 파일을 로드하려면 loadtxt 함수를 사용합니다: . a_loaded = np.loadtxt(&quot;my_array.csv&quot;, delimiter=&quot;,&quot;) a_loaded . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#50517;&#52629;&#46108; .npz &#54252;&#47607; . 여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다: . b = np.arange(24, dtype=np.uint8).reshape(2, 3, 4) b . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=uint8) . np.savez(&quot;my_arrays&quot;, my_a=a, my_b=b) . 파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다. . with open(&quot;my_arrays.npz&quot;, &quot;rb&quot;) as f: content = f.read() repr(content)[:180] + &quot;[...]&quot; . &#39;b&#34;PK x03 x04 x14 x00 x00 x00 x00 x00 x00 x00! x00 x063 xcf xb9 xb0 x00 x00 x00 xb0 x00 x00 x00 x08 x00 x14 x00my_a.npy x01 x00 x10 x00 xb0 x00 x00 x00 x00 x00 x00 x00 xb0 x00 x00 x[...]&#39; . 다음과 같이 이 파일을 로드할 수 있습니다: . my_arrays = np.load(&quot;my_arrays.npz&quot;) my_arrays . &lt;numpy.lib.npyio.NpzFile at 0x7f9791c73d60&gt; . 게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다: . my_arrays.keys() . KeysView(&lt;numpy.lib.npyio.NpzFile object at 0x7f9791c73d60&gt;) . my_arrays[&quot;my_a&quot;] . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#44536; &#45796;&#51020;&#51008;? . 넘파이 기본 요소를 모두 배웠지만 훨씬 더 많은 기능이 있습니다. 이를 배우는 가장 좋은 방법은 넘파이를 직접 실습해 보고 훌륭한 넘파이 문서에서 필요한 함수와 기능을 찾아 보세요. .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/python/2022/03/04/numpy.html",
            "relUrl": "/jupyter/python/2022/03/04/numpy.html",
            "date": " • Mar 4, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hyunsookim0813.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hyunsookim0813.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About me . 관심분야 . 사용가능한 Tools . R | Python | SAS | SQL | . 데이터 분석 project . 참여 공모전 .",
          "url": "https://hyunsookim0813.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hyunsookim0813.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}